This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-09-09 15:26:55

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
LICENSES
  AGPL-3.0-or-later.txt
forest_ensys
  .ruff_cache
    0.6.7
      14794575240109317716
    .gitignore
    CACHEDIR.TAG
  api
    endpoints
      __init__.py
      authentication.py
      emissions_data.py
      grid_data.py
      model.py
      process_data.py
      process_electricity_data.py
      process_heat_data.py
      users.py
      footprint_data.py
      flexibility.py
      simulation_input_data.py
      price_data.py
      result_data.py
    __init__.py
    api.py
    deps.py
  core
    __init__.py
    calliope_model.py
    config.py
    security.py
    crawlers.py
    optimization.py
    aas_helper.py
    timeseries_helpers.py
  crud
    __init__.py
    data_parc.py
    emissions.py
    footprint.py
    grid.py
    model.py
    process_electricity.py
    process_heat.py
    user.py
    weather.py
    flexible_power.py
    prices.py
    simulation_input_data.py
    optimization_results.py
    base.py
  database
    __init__.py
    base_class.py
    init_db.py
    session.py
  model
    __init__.py
    data_parc.py
    emissions.py
    footprint.py
    grid.py
    model.py
    process_electricity.py
    process_heat.py
    user.py
    weather.py
    prices.py
    simulation_input_data.py
    optimization_results.py
    flexible_power.py
  schemas
    __init__.py
    co2.py
    data_parc.py
    emissions.py
    footprint.py
    grid.py
    model.py
    process_electricity.py
    process_heat.py
    token.py
    user.py
    weather.py
    flexible_power.py
    optimization_results.py
    prices.py
  __init__.py
  __main__.py
  app.py
  repomix-output.md
.gitignore
Dockerfile
README.md
requirements.txt
grafana
  dashboard-provisioning.yml
  datasource-example.yml
  dashboard_smard.json
  dashboard_heating_optimization.json
compose.yml
```

# Repository Files


## LICENSES/AGPL-3.0-or-later.txt

```text
GNU AFFERO GENERAL PUBLIC LICENSE
Version 3, 19 November 2007

Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>

Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.

                            Preamble

The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.

The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.

When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.

Developers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.

A secondary benefit of defending all users' freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate.  Many developers of free software are heartened and encouraged by the resulting cooperation.  However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.

The GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community.  It requires the operator of a network server to provide the source code of the modified version running there to the users of that server.  Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.

An older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals.  This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.

The precise terms and conditions for copying, distribution and modification follow.

                       TERMS AND CONDITIONS

0. Definitions.

"This License" refers to version 3 of the GNU Affero General Public License.

"Copyright" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.

"The Program" refers to any copyrightable work licensed under this License.  Each licensee is addressed as "you".  "Licensees" and "recipients" may be individuals or organizations.

To "modify" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a "modified version" of the earlier work or a work "based on" the earlier work.

A "covered work" means either the unmodified Program or a work based on the Program.

To "propagate" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.

To "convey" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.

An interactive user interface displays "Appropriate Legal Notices" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.

1. Source Code.
The "source code" for a work means the preferred form of the work for making modifications to it.  "Object code" means any non-source form of a work.

A "Standard Interface" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.

The "System Libraries" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A "Major Component", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.

The "Corresponding Source" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those
subprograms and other parts of the work.

The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.

The Corresponding Source for a work in source code form is that same work.

2. Basic Permissions.
All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.

You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.

Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.

3. Protecting Users' Legal Rights From Anti-Circumvention Law.
No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.

When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.

4. Conveying Verbatim Copies.
You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.

You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.

5. Conveying Modified Source Versions.
You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7.  This requirement modifies the requirement in section 4 to "keep intact all notices".

    c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy.  This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged.  This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.

A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an "aggregate" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.

6. Conveying Non-Source Forms.
You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:

    a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source.  This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.

    d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge.  You need not require recipients to copy the Corresponding Source along with the object code.  If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source.  Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.

A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.

A "User Product" is either (1) a "consumer product", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, "normally used" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.

"Installation Information" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.

If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).

The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.

Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.

7. Additional Terms.
"Additional permissions" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.

When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.

Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or authors of the material; or

    e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.

All other non-permissive additional terms are considered "further restrictions" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.

If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.

Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.

8. Termination.

You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).

However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.

Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.

Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.

9. Acceptance Not Required for Having Copies.

You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.

10. Automatic Licensing of Downstream Recipients.

Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.

An "entity transaction" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.

You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.

11. Patents.

A "contributor" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's "contributor version".

A contributor's "essential patent claims" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, "control" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.

Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.

In the following three paragraphs, a "patent license" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To "grant" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.

If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.

If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.

A patent license is "discriminatory" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.

Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.

12. No Surrender of Others' Freedom.

If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.

13. Remote Network Interaction; Use with the GNU General Public License.

Notwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software.  This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.

Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.

14. Revised Versions of this License.

The Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.

Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU Affero General Public License "or any later version" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.

If the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.

Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.

15. Disclaimer of Warranty.

THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

16. Limitation of Liability.

IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

17. Interpretation of Sections 15 and 16.

If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.

END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.

To do so, attach the following notices to the program.  It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the "copyright" line and a pointer to where the full notice is found.

     <one line to give the program's name and a brief idea of what it does.>
     Copyright (C) <year>  <name of author>

     This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

     This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Affero General Public License for more details.

     You should have received a copy of the GNU Affero General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

If your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source.  For example, if your program is a web application, its interface could display a "Source" link that leads users to an archive of the code.  There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.

You should also get your employer (if you work as a programmer) or school, if any, to sign a "copyright disclaimer" for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see <http://www.gnu.org/licenses/>.
```

## forest_ensys/api/endpoints/__init__.py

```python

```

## forest_ensys/api/endpoints/authentication.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from datetime import timedelta

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session

from forest_ensys import schemas, crud, model
from forest_ensys.api import deps
from forest_ensys.core import security, settings

router = APIRouter()


@router.post("/login", response_model=schemas.Token)
def login(
    db: Session = Depends(deps.get_db), form_data: OAuth2PasswordRequestForm = Depends()
) -> schemas.Token:
    """
    OAuth2 compatible token login, get an access token for future requests
    """
    user = crud.user.authenticate(
        db, username=form_data.username, password=form_data.password
    )
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password!",
        )
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    token = security.create_access_token(user.id, expires_delta=access_token_expires)
    return schemas.Token(access_token=token, token_type="bearer")


@router.post(
    "/register",
    response_model=schemas.User,
    responses={409: {"description": "User with same name already exists."}},
)
def register(
    request: schemas.UserCreate, db: Session = Depends(deps.get_db)
) -> schemas.User:
    """
    Create a new user
    """
    user = crud.user.get_by_username(db, username=request.username)
    if user:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="User with that username already exists!",
        )

    user = crud.user.create(db, obj_in=request)

    return user


@router.get("/test-token", response_model=schemas.User)
def test_token(
    current_user: model.User = Depends(deps.get_current_user),
) -> schemas.User:
    """
    Test access token
    """
    return current_user
```

## forest_ensys/api/endpoints/emissions_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text, Optional

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core import crawlers

router = APIRouter()


@router.get("/", response_model=List[schemas.Emissions])
def get_all_emissions_data(
    db: Session = Depends(deps.get_db),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Emissions]:
    """
    Retrieve all emissions data
    """
    emissions_data = crud.emissions.get_multi(db=db, skip=skip, limit=limit)
    return emissions_data


@router.get(
    "/get-specific-emissions-factor", response_model=Optional[schemas.Emissions]
)
def get_emissions_data(
    db: Session = Depends(deps.get_db),
    zone_key: str = "DE",
    emission_type: str = "direct",
    production_mode: str = "gas",
) -> schemas.Emissions:
    """
    Retrieve emissions data for a specific zone and emission type and production mode
    """
    emissions_data = crud.emissions.get_specific_emissions(
        db=db,
        zone_key=zone_key,
        emission_type=emission_type,
        production_mode=production_mode,
    )
    return emissions_data


@router.post(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Emissions data updated successfully"}
                }
            },
        },
        502: {
            "description": "Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Could not retrieve emissions data. Server probably offline"
                    }
                }
            },
        },
        409: {
            "description": "Conflict Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Emissions data already exists. Please delete first if you want to update."
                    }
                }
            },
        },
    },
)
def update_emissions_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Retrieve the most recent emissions data
    """
    try:
        timeseries_data = crawlers.crawl_emissions_data()
    except Exception:
        raise HTTPException(
            status_code=502,
            detail="Could not retrieve emissions data. Server probably offline",
        )
    emission_objects = []
    for index, row in timeseries_data.iterrows():
        db_obj = model.Emissions(
            timestamp=row["datetime"],
            zone_key=row["zone_key"],
            emission_factor_type=row["emission_factor_type"],
            production_mode=row["production_mode"],
            value=row["value"],
            source=row["source"],
        )
        emission_objects.append(db_obj)

    try:
        crud.emissions.create_multi(db=db, obj_in=emission_objects)
        raise HTTPException(
            status_code=200, detail="Emissions data updated successfully"
        )
    except Exception:
        raise HTTPException(
            status_code=409,
            detail="Emissions data already exists. Please delete first if you want to update.",
        )


@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Emissions data table deleted successfully"}
                }
            },
        }
    },
)
def delete_emissions_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all emissions data
    """
    crud.emissions.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Emissions data table deleted successfully"
    )


@router.post("/get_specific_emissions_factor", response_model=Optional[schemas.Emissions])
def get_specific_emissions_factor(
    db: Session = Depends(deps.get_db),
    zone_key: str = "DE",
    emission_type: str = "direct",
    production_mode: str = "gas",
) -> schemas.Emissions:
    """
    Retrieve emissions data for a specific zone and emission type and production mode
    """
    return crud.emissions.get_specific_emissions(
        db=db,
        zone_key=zone_key,
        emission_type=emission_type,
        production_mode=production_mode,
    )
```

## forest_ensys/api/endpoints/grid_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.api.endpoints import footprint_data, emissions_data

from forest_ensys.core import crawlers
import pandas as pd
from datetime import timedelta

router = APIRouter()

keys = {
    # 411: 'Prognostizierter Stromverbrauch',
    # 410: 'Realisierter Stromverbrauch',
    4169: "Preis",
    4066: "Biomasse",
    1226: "Wasserkraft",
    1225: "Wind Offshore",
    4067: "Wind Onshore",
    4068: "Photovoltaik",
    1228: "Sonstige Erneuerbare",
    1223: "Braunkohle",
    4071: "Erdgas",
    4070: "Pumpspeicher",
    1227: "Sonstige Konventionelle",
    4069: "Steinkohle",
    # 5097: 'Prognostizierte Erzeugung PV und Wind Day-Ahead'
}

grid_to_factors = {
    "Biomasse": "biomass",
    "Wasserkraft": "hydro",
    "Wind Offshore": "wind",
    "Wind Onshore": "wind",
    "Photovoltaik": "solar",
    "Braunkohle": "coal",
    "Steinkohle": "coal",
    "Erdgas": "gas",
    "Sonstige Konventionelle": "gas",
    "Sonstige Erneuerbare": "solar",
    "Pumpspeicher": "hydro",
}

@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Grid data table deleted successfully"}
                }
            },
        }
    },
)
def delete_grid_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all grid data
    """
    crud.grid.delete(db=db)
    crud.prices.delete(db=db)
    crud.footprint.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Grid data table deleted successfully"
    )

@router.get("/", response_model=List[schemas.Grid])
def get_all_grid_data(
    db: Session = Depends(deps.get_db),
    current: model.Grid = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Grid]:
    """
    Retrieve all grid data
    """
    grid_data = crud.grid.get_multi(db=db, skip=skip, limit=limit)
    return grid_data

@router.get("/update", responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Grid data updated successfully"}
                }
            },
        },
        502: {
            "description": "Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Could not retrieve emissions data. Server probably offline"
                    }
                }
            }
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Website did not return any data."
                    }
                }
            }
        }   
    }
)
def update_recent_grid_data(db: Session = Depends(deps.get_db)):
    """
    Retrieve the most recent grid data
    """
    while True:
        grid_data = []
        prices = []
        start_date = "12-31-2023 22:00:00"
        try:
            latest_emissions_factors = get_latest_emissions_factors(db=db)
        except Exception:
            raise HTTPException(
                status_code=502,
                detail="Could not retrieve emissions data. Server probably offline",
            )
        for commodity_id, commodity_name in keys.items():
            if commodity_id != 4169:
                latest = crud.grid.get_latest_for_commodity(db=db, commodity_id=commodity_id)
            else:
                latest = crud.prices.get_latest(db=db)
            latest_in_db = None
            try:
                latest = pd.to_datetime(latest.timestamp)
                latest_in_db = latest.tz_localize("UTC")
                print(f"The latest date in the database is {latest}")
                if latest.weekday() != 6 or (latest.hour < 21 and latest.minute == 45):
                    last_sunday = latest - timedelta(days=(latest.weekday() + 1) % 7)
                    print(
                        f"the latest date in the database is not a sunday after 22:00, taking last week sunday 22:00 as start date to fill the missing data: {latest} -> {last_sunday}"
                    )
                    latest = last_sunday
                if latest.hour == 21 and latest.minute == 45:
                    print(
                        "the latest date in the database is a sunday 21:45, taking this sunday 22:00 as start date"
                    )
                    latest = latest.replace(hour=22, minute=0, second=0, microsecond=0)
                    latest2 = latest.replace(hour=23, minute=0, second=0, microsecond=0)
                else:
                    print(
                        "the latest date in the database is a sunday 22:45, taking this sunday 23:00 as start date"
                    )
                    latest = latest.replace(hour=23, minute=0, second=0, microsecond=0)
                    latest2 = latest.replace(hour=22, minute=00, second=0, microsecond=0)
            except Exception as e:
                print(f"Using the default start date for commodity {commodity_id, e}")
                latest = pd.to_datetime(start_date)
                latest2 = latest.replace(hour=23, minute=0, second=0, microsecond=0)

            start_date_unix = int(latest.timestamp() * 1000)
            second_start_date_unix = int(latest2.timestamp() * 1000)
            data_for_commodity = crawlers.get_data_per_commodity(
                commodity_id, commodity_name, start_date_unix, second_start_date_unix
            )
            if data_for_commodity.empty:
                raise HTTPException(
                    status_code=404, detail="Could not get data for commodity {commodity_id}"
                )
            # if data_for_commodity is None:
            #    return HTTPException(status_code=404, detail="Could not reach website for crawling data")
            # delete timezone duplicate
            # https://stackoverflow.com/a/34297689
            data_for_commodity = data_for_commodity[
                ~data_for_commodity.index.duplicated(keep="first")
            ]
            if latest_in_db is not None:
                data_for_commodity = data_for_commodity[
                    data_for_commodity["timestamp"] > latest_in_db
                ]
            if commodity_id == 4169:
                for index, row in data_for_commodity.iterrows():
                    db_obj = model.Prices(
                        timestamp=row["timestamp"],
                        price=row["mwh"],
                        source="smard"
                    )
                    prices.append(db_obj)
            else:
                for index, row in data_for_commodity.iterrows():
                    db_obj = model.Grid(
                        timestamp=row["timestamp"],
                        commodity_id=row["commodity_id"],
                        commodity_name=row["commodity_name"],
                        mwh=row["mwh"],
                        co2=row["mwh"] * latest_emissions_factors[commodity_name] * 1000,
                    )
                    grid_data.append(db_obj)
        crud.grid.create_multi(db=db, obj_in=grid_data)
        crud.prices.create_multi(db=db, obj_in=prices)
        footprint_data.update_footprint_data(db)
    raise HTTPException(
            status_code=200, detail="Grid data updated successfully"
        )

def get_latest_emissions_factors(db: Session) -> dict:
    emissions = {}
    if not crud.emissions.get_multi(db=db):
        print("Emissions data seems empty, trying to crawl")
        emissions_data.update_emissions_data()

    for commodity_id, commodity_name in keys.items():
        if commodity_id == 4169:
            continue
        print(
            f"getting latest emission factor for {commodity_name} and {grid_to_factors[commodity_name]}"
        )
        specific_emission = crud.emissions.get_specific_emissions(
            db=db,
            zone_key="DE",
            emission_type="direct",
            production_mode=grid_to_factors[commodity_name],
        )
        print(specific_emission)
        emissions[commodity_name] = specific_emission.value
    return emissions
```

## forest_ensys/api/endpoints/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core.calliope_model import generate_calliope_model
import pandas as pd
import json

router = APIRouter()


@router.get("/", response_model=List[schemas.Model])
def get_all_model_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Model]:
    """
    Retrieve all contraints data
    """
    model_data = crud.model.get_multi(db=db, user_id=current.id, skip=skip, limit=limit)
    return model_data


@router.post("/", response_model=schemas.Model)
def add_model_data(
    request: schemas.ModelCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Create a new model
    """
    return crud.model.create(db=db, obj_in=request, user_id=current.id)


@router.get("/{model_id}/optimize")
def optimize_model_by_id(
    model_id: int,
    start_date: datetime = None,
    end_date: datetime = None,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> str:
    """
    Optimize the model by id
    """
    model = crud.model.get(db=db, id=model_id)
    electricity_data = crud.process_electricity.get_multi_by_date_range(
        db=db, user_id=current.id, start_date=start_date, end_date=end_date
    )
    heat_data = crud.process_heat.get_multi_by_date_range(
        db=db, user_id=current.id, start_date=start_date, end_date=end_date
    )
    if electricity_data is None or heat_data is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Electricity data or heat data not found for model {model_id}!",
        )   
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail=f"Model {model_id} not found!"
        )
    calliope_model = generate_calliope_model(
        model.model, electricity_data, heat_data
    )
    calliope_model.build()
    calliope_model.solve()
    # results to html
    calliope_model.to_csv("results.csv")
    return json.dumps(calliope_model.results["cost"][0].to_dict())


@router.get("/{model_name}/optimize_by_name")
def optimize_model_by_name(
    model_name: str,
    start_date: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> str:
    """
    Optimize the model by name
    """
    model = crud.model.get_by_name(db=db, name=model_name)
    electricity_data = crud.process_electricity.get_from_start_date(
        db=db, user_id=current.id, start_date=start_date
    )
    electricity_data_df = pd.read_sql(
        electricity_data.statement, electricity_data.session.connection()
    )
    heat_data = crud.process_heat.get_from_start_date(
        db=db, user_id=current.id, start_date=start_date
    )
    heat_data_df = pd.read_sql(heat_data.statement, heat_data.session.connection())
    if electricity_data_df.empty or heat_data_df.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Electricity data or heat data not found for model {model_name}!",
        )
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Model {model_name} not found!",
        )
    calliope_model = generate_calliope_model(
        model.model, electricity_data_df, heat_data_df
    )
    calliope_model.run()
    return json.dumps(calliope_model.results["cost"][0].to_dict(), default=str)


@router.delete("/{model_name}", response_model=schemas.Model)
def delete_model_by_name(
    model_name: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Delete a model by name
    """
    model = crud.model.get_by_name(db=db, name=model_name)
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Model {model_name} not found!",
        )
    return crud.model.remove(db=db, id=model.id)
```

## forest_ensys/api/endpoints/process_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()

@router.post("/", response_model=schemas.DataParc)
def add_process_data(
    request: schemas.DataParcCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Create a new process data
    """
    return crud.data_parc.create(db=db, obj_in=request, user_id=current.id)

@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Process data table deleted successfully"}
                }
            }
        },
        502: {
            "description": "Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Could not retrieve emissions data. Server probably offline"
                    }
                }
            }
        }
    },
)
def delete_process_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all process data
    """
    crud.data_parc.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Process data table deleted successfully"
    )
```

## forest_ensys/api/endpoints/process_electricity_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()


@router.get("/", response_model=List[schemas.ProcessElectricity])
def get_all_process_electricity_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.ProcessElectricity]:
    """
    Retrieve all electricity data
    """
    process_electricity_data = crud.process_electricity.get_multi(
        db=db, user_id=current.id, skip=skip, limit=limit
    )
    return process_electricity_data


@router.post("/", response_model=schemas.ProcessElectricity)
def add_process_electricity_data(
    request: schemas.ProcessElectricityCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Create a new process_electricity
    """
    return crud.process_electricity.create(db=db, obj_in=request, user_id=current.id)
```

## forest_ensys/api/endpoints/process_heat_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()


@router.get("/", response_model=List[schemas.ProcessHeat])
def get_all_process_heat_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[model.ProcessHeat]:
    """
    Retrieve all heat data
    """
    process_heat_data = crud.process_heat.get_multi(
        db=db, user_id=current.id, skip=skip, limit=limit
    )
    return process_heat_data


@router.post("/", response_model=schemas.ProcessHeat)
def add_process_heat_data(
    request: schemas.ProcessHeatCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> model.ProcessHeat:
    """
    Create a new process heat data
    """
    return crud.process_heat.create(db=db, obj_in=request, user_id=current.id)
```

## forest_ensys/api/endpoints/users.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()


@router.get("/", response_model=List[schemas.User])
def get_all_users(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.User]:
    """
    Retrieve all users.
    """
    users = crud.user.get_multi(db=db, skip=skip, limit=limit)
    return users
```

## forest_ensys/api/endpoints/footprint_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from datetime import datetime
from pyomo.environ import ConcreteModel, Var, Objective, SolverFactory, NonNegativeReals, minimize, Constraint

router = APIRouter()

grid_to_factors = {
    "Biomasse": "biomass",
    "Wasserkraft": "hydro",
    "Wind Offshore": "wind",
    "Wind Onshore": "wind",
    "Photovoltaik": "solar",
    "Braunkohle": "coal",
    "Steinkohle": "coal",
    "Erdgas": "gas",
    "Sonstige Konventionelle": "gas",
    "Sonstige Erneuerbare": "solar",
    "Pumpspeicher": "hydro",
}
@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Footprint data table deleted successfully"
                    }
                }
            },
        }
    },
)
def delete_footprint_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all footprint data
    """
    crud.footprint.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Footprint data table deleted successfully"
    )

def update_footprint_data(db: Session = Depends(deps.get_db)):
    grid = crud.grid.get_average_co2_by_commodity(db=db)
    footprint_data = []
    for row in grid:
        # this means we have missing data for this 15 mins and the co2 is zero
        if row[1] == 0:
            continue
        footprint_data.append(
            model.Footprint(
                timestamp=row[0],
                co2=row[1] / (row[2] * 1000),
            )
        )
    crud.footprint.create_multi(db=db, obj_in=footprint_data)

@router.get("/", response_model=List[schemas.Footprint])
def get_all_footprint_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Footprint]:
    """
    Retrieve all footprint data
    """
    return crud.footprint.get_multi(db=db, skip=skip, limit=limit)

@router.get("/latest", response_model=schemas.Footprint)
def get_latest_footprint_data(db: Session = Depends(deps.get_db)) -> schemas.Footprint:
    """
    Retrieve the latest footprint data
    """
    return crud.footprint.get_latest(db=db)
```

## forest_ensys/api/endpoints/flexibility.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Text
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core.timeseries_hepers import (
    check_granularity_and_merge,
    calculate_dynamic_network_fee,
)
from forest_ensys.core.optimization import optimize_dryers as optimize
from forest_ensys.core.aas_helper import get_data_from_aas
from datetime import datetime

import pandas as pd
import numpy as np

router = APIRouter()


@router.delete(
    "/",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "message": "Flexibility data table deleted successfully",
                    }
                }
            }
        }
    },
)
def delete_flexibility_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> Text:
    """
    Delete all flexibility data
    """
    crud.flexible_power.delete(db=db, user_id=current.id)
    crud.optimization_results.delete(db=db, user_id=current.id)
    raise HTTPException(
        status_code=200, detail="Flexibility data table deleted successfully"
    )


@router.delete(
    "/{optimization_case_name}",
    responses={200: {"description": "Optimization case deleted successfully"}},
)
def delete_optimization_case(
    optimization_case_name: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> Text:
    """
    Delete a specific optimization case
    """
    crud.flexible_power.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name=optimization_case_name
    )
    crud.optimization_results.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name=optimization_case_name
    )
    raise HTTPException(
        status_code=200, detail="Optimization case deleted successfully"
    )


@router.post(
    "/optimize_flexibility",
    response_model=schemas.OptimizationResult,
    responses={404: {"description": "No data found for the given date range"}},
)
def optimize_flexibility(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    start_date: datetime = "2024-10-01",
    end_date: datetime = "2024-10-31",
    flexible_power: float = 8000,  # kW
    electricity_network_fee: float = 10,  # cost per MWh
    gas_emissions_factor: float = 204,  # g/kWh
    cost_per_mwh_gas: float = 60,  # cost per MWh
    co2_price: float = 55,  # cost per ton of CO2
) -> schemas.OptimizationResult:
    """
    Binary decision problem to optimize the use of electric heating
    """
    crud.flexible_power.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    crud.optimization_results.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    # Retrieve data from database
    footprint_data = crud.footprint.get_multi_by_date_range(
        db=db, start_date=start_date, end_date=end_date
    )
    heat_demand = crud.data_parc.get_multi_by_date_range(
        db=db, user_id=current.id, start_date=start_date, end_date=end_date
    )
    price_data = crud.prices.get_multi_by_date_range_and_source(
        db=db,
        start_date=start_date,
        end_date=end_date,
        source="smard",
    )

    # Merge dataframes and check granularity
    merged_data = check_granularity_and_merge(
        footprint_data, heat_demand[["timestamp", "value"]], method="sum"
    )
    merged_data = check_granularity_and_merge(
        merged_data,
        price_data[["timestamp", "price"]].rename(
            columns={"price": "electricity_price"}
        ),
    )
    merged_data["electricity_price"] = (
        merged_data["electricity_price"] + electricity_network_fee
    )

    time_interval_hours = merged_data["timestamp"].diff().min().total_seconds() / 3600

    # Decision based on electrical power (kW) at each timestamp
    use_flexible_power = merged_data["co2"] < gas_emissions_factor
    total_energy_demand = merged_data["value"].sum()
    # Integrate flexible power used and gas usage over time (kWh)
    electricity_used = np.where(
        use_flexible_power, flexible_power * time_interval_hours, 0
    ).sum()
    gas_usage = total_energy_demand - electricity_used

    # Flexible power time series (kWh per interval)
    flexible_power_time_series = pd.DataFrame(
        {
            "timestamp": merged_data["timestamp"],
            "electricity_used": np.where(use_flexible_power, flexible_power, 0),
        }
    )

    emissions_gas_only = merged_data["value"].sum() * gas_emissions_factor * 1e-6

    emissions_savings = (
        np.where(
            use_flexible_power,
            (gas_emissions_factor - merged_data["co2"])
            * flexible_power
            * time_interval_hours,
            0,
        ).sum()
    ) * 1e-6  # convert to tonnes

    emissions_with_electric_heating = emissions_gas_only - emissions_savings

    cost_gas_only = (
        total_energy_demand * 1e-3 * cost_per_mwh_gas
        + emissions_gas_only * 1e-6 * co2_price
    )

    cost_savings = emissions_savings * co2_price + (
        np.where(
            use_flexible_power,
            (cost_per_mwh_gas - merged_data["electricity_price"])
            * flexible_power
            * time_interval_hours
            * 1e-3,
            0,
        ).sum()
    )

    cost_with_electric_heating = cost_gas_only - cost_savings

    flexible_power = pd.DataFrame(
        {
            "timestamp": flexible_power_time_series["timestamp"],
            "electricity_used": flexible_power_time_series["electricity_used"],
            "low_price_window": 0,
            "optimization_case_name": "binary_decision_problem",
            "ref_created_by": current.id,
        }
    )

    crud.flexible_power.create_multi(
        db=db, obj_in=flexible_power.to_dict(orient="records")
    )

    optimization_results = {
        "name": "binary_decision_problem",
        "ref_created_by": current.id,
        "time_from": start_date,
        "time_to": end_date,
        "network_fee_type": "static",
        "network_fee": 0.0,
        "total_energy_demand": round(total_energy_demand, 2),
        "electricity_used": round(electricity_used, 2),
        "gas_usage": round(gas_usage, 2),
        "cost_savings": round(cost_savings, 2),
        "emissions_savings": round(emissions_savings, 2),
        "cost_gas_only": round(cost_gas_only, 2),
        "cost_with_electric_heating": round(cost_with_electric_heating, 2),
        "emissions_gas_only": round(emissions_gas_only, 2),
        "emissions_with_electric_heating": round(emissions_with_electric_heating, 2),
        "full_load_hours": 0,
        "full_load_hours_after_optimization": 0,
        "mean_electricity_price_when_heating": (
            merged_data["electricity_price"][use_flexible_power].mean()
        ),
        "electric_heating_in_low_price_windows_ratio": 0,
    }

    return crud.optimization_results.create(db=db, obj_in=optimization_results)


@router.post(
    "/optimize_flexibility_aas",
    response_model=schemas.OptimizationResult,
    responses={404: {"description": "No data found for the given date range"}},
)
def optimize_flexibility_aas_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> schemas.OptimizationResult:
    """
    Binary decision problem to optimize the use of electric heating
    """
    parameters = get_data_from_aas()
    crud.flexible_power.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    flexible_power = int(parameters["powerMax"])
    electricity_network_fee = int(parameters["electricityNetworkFee"])
    gas_emissions_factor = 204
    cost_per_mwh_gas = int(parameters["gasPrice"])
    co2_price = int(parameters["co2Price"])
    crud.optimization_results.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    # Retrieve data from database
    footprint_data = crud.footprint.get_multi_by_date_range(
        db=db, start_date=parameters["from"], end_date=parameters["until"]
    )
    heat_demand = crud.data_parc.get_multi_by_date_range(
        db=db,
        user_id=current.id,
        start_date=parameters["from"],
        end_date=parameters["until"],
    )
    price_data = crud.prices.get_multi_by_date_range_and_source(
        db=db,
        start_date=parameters["from"],
        end_date=parameters["until"],
        source="smard",
    )

    # Merge dataframes and check granularity
    merged_data = check_granularity_and_merge(
        footprint_data, heat_demand[["timestamp", "value"]], method="sum"
    )
    merged_data = check_granularity_and_merge(
        merged_data,
        price_data[["timestamp", "price"]].rename(
            columns={"price": "electricity_price"}
        ),
    )
    merged_data["electricity_price"] = (
        merged_data["electricity_price"] + electricity_network_fee
    )

    time_interval_hours = merged_data["timestamp"].diff().min().total_seconds() / 3600

    # Decision based on electrical power (kW) at each timestamp
    use_flexible_power = merged_data["co2"] < gas_emissions_factor
    total_energy_demand = merged_data["value"].sum()
    # Integrate flexible power used and gas usage over time (kWh)
    electricity_used = np.where(
        use_flexible_power, flexible_power * time_interval_hours, 0
    ).sum()
    gas_usage = total_energy_demand - electricity_used

    # Flexible power time series (kWh per interval)
    flexible_power_time_series = pd.DataFrame(
        {
            "timestamp": merged_data["timestamp"],
            "electricity_used": np.where(use_flexible_power, flexible_power, 0),
        }
    )

    emissions_gas_only = merged_data["value"].sum() * gas_emissions_factor * 1e-6

    emissions_savings = (
        np.where(
            use_flexible_power,
            (gas_emissions_factor - merged_data["co2"])
            * flexible_power
            * time_interval_hours,
            0,
        ).sum()
    ) * 1e-6  # convert to tonnes

    emissions_with_electric_heating = emissions_gas_only - emissions_savings

    cost_gas_only = (
        total_energy_demand * 1e-3 * cost_per_mwh_gas
        + emissions_gas_only * 1e-6 * co2_price
    )

    cost_savings = emissions_savings * co2_price + (
        np.where(
            use_flexible_power,
            (cost_per_mwh_gas - merged_data["electricity_price"])
            * flexible_power
            * time_interval_hours
            * 1e-3,
            0,
        ).sum()
    )

    cost_with_electric_heating = cost_gas_only - cost_savings

    flexible_power = pd.DataFrame(
        {
            "timestamp": flexible_power_time_series["timestamp"],
            "electricity_used": flexible_power_time_series["electricity_used"],
            "low_price_window": 0,
            "optimization_case_name": "binary_decision_problem",
            "ref_created_by": current.id,
        }
    )

    crud.flexible_power.create_multi(
        db=db, obj_in=flexible_power.to_dict(orient="records")
    )

    optimization_results = {
        "name": "binary_decision_problem",
        "ref_created_by": current.id,
        "time_from": parameters["from"],
        "time_to": parameters["until"],
        "network_fee_type": "static",
        "network_fee": 0.0,
        "total_energy_demand": round(total_energy_demand, 2),
        "electricity_used": round(electricity_used, 2),
        "gas_usage": round(gas_usage, 2),
        "cost_savings": round(cost_savings, 2),
        "emissions_savings": round(emissions_savings, 2),
        "cost_gas_only": round(cost_gas_only, 2),
        "cost_with_electric_heating": round(cost_with_electric_heating, 2),
        "emissions_gas_only": round(emissions_gas_only, 2),
        "emissions_with_electric_heating": round(emissions_with_electric_heating, 2),
        "full_load_hours": 0,
        "full_load_hours_after_optimization": 0,
        "mean_electricity_price_when_heating": (
            merged_data["electricity_price"][use_flexible_power].mean()
        ),
        "electric_heating_in_low_price_windows_ratio": 0,
    }

    return crud.optimization_results.create(db=db, obj_in=optimization_results)


@router.post(
    "/optimize_dryers",
    response_model=schemas.OptimizationResult,
    responses={
        404: {"description": "No data found for the given date range"},
        400: {"description": "Invalid Network Type"},
        501: {"description": "Not implemented yet"},
        500: {"description": "Internal Server Error"},
        401: {"description": "Unauthorized"},
    },
)
def optimize_dryers(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    start_date: datetime = Query(
        "2024-01-01", description="Start date for the optimization period."
    ),
    end_date: datetime = Query(
        "2024-12-31", description="End date for the optimization period."
    ),
    optimization_case_name: str = Query(
        "optimization_case_1",
        description="Name of the optimization case. Used to store the results in the database.",
    ),
    electricity_price_data_source: str = Query(
        "smard",
        description="Data source for electricity prices. Smard are the market prices. You can upload PFCs or other forecasts under prices.",
    ),
    gas_price_data_source: str = Query(
        "gas_spot",
        description="Data source for gas prices. Constant uses a constant gas price for the whole period. You can upload PFCs or other forecasts under prices.",
    ),
    flexible_power: int = Query(
        8000, description="The power of the flexible load in kW."
    ),
    gas_emissions_factor: int = Query(
        204,
        description="The emissions factor for gas in g/kWh. It is around 200g/kWh for natural gas based on the Gross Calorific Value.",
    ),
    cost_per_mwh_gas: int = Query(
        60,
        description="The cost of gas in /MWh. 60/MWh was the average price for gas in Germany in 2024.",
    ),
    co2_price: int = Query(
        55,
        description="The price of CO2 in /Tonne CO2. 55/Tonne is the price for CO2 in Germany in 2025.",
    ),
    ramp_up_rate: int = Query(
        6000, description="The ramp up rate of the flexible load in kW/h."
    ),
    ramp_down_rate: int = Query(
        6000, description="The ramp down rate of the flexible load in kW/h."
    ),
    minimum_runtime: int = Query(
        2, description="The minimum runtime of the flexible load in quarter hours."
    ),
    network_fee: str = Query(
        "static",
        enum=["static", "dynamic"],
        description="Method for calculating network fees. Static adds a fixed value to the market electricity price, while dynamic calculates the network fee based on BK4-22-089",
    ),
    network_fee_value: float = Query(
        20.0,
        description="The network fee value in /MWh.",
    ),
    relative_network_fee_reduction: float = Query(
        0.8,
        description="The relative reduction of the network fee for flexible loads.",
    ),
    gas_network_fee: float = Query(
        4.0,
        description="The gas network fee in /MWh.",
    ),
    relative_network_fee_surcharge: float = Query(
        0.1,
        description="The relative surcharge of the network fee for inflexible loads.",
    ),
    window_size: int = Query(
        2,
        description="The window size for the dynamic network fee calculation in hours.",
    ),
) -> schemas.OptimizationResult:
    """
    Optimizes the use of electric heating using Pyomo.
    """
    if start_date > end_date:
        raise HTTPException(
            status_code=400, detail="Start date must be before end date"
        )
    if any(
        value < 0 or not isinstance(value, (int, float)) or value is None
        for value in [
            flexible_power,
            gas_emissions_factor,
            cost_per_mwh_gas,
            co2_price,
            ramp_up_rate,
            ramp_down_rate,
            minimum_runtime,
            network_fee_value,
        ]
    ):
        raise HTTPException(
            status_code=400, detail="All values must be positive numbers"
        )
    if (
        crud.optimization_results.get(
            db=db, user_id=current.id, optimization_case_name=optimization_case_name
        )
        is not None
    ):
        raise HTTPException(
            status_code=400,
            detail="Optimization case already exists. Please delete it first or change the name.",
        )
    footprint_data = crud.footprint.get_multi_by_date_range(
        db=db, start_date=start_date, end_date=end_date
    )
    if footprint_data is None:
        raise HTTPException(
            status_code=404, detail="No footprint data found for the given date range"
        )
    heat_demand = crud.simulation_input_data.get_multi_by_date_range_and_name(
        db=db,
        user_id=current.id,
        start_date=start_date,
        end_date=end_date,
        name="flexible_device_demand",
    )
    if heat_demand is None:
        raise HTTPException(
            status_code=404, detail="No heat demand data found for the given date range"
        )
    heat_demand.rename(columns={"value": heat_demand["name"][0]}, inplace=True)
    heat_demand.drop(columns=["name"], inplace=True)
    total_electricity_demand = (
        crud.simulation_input_data.get_multi_by_date_range_and_name(
            db=db,
            user_id=current.id,
            start_date=start_date,
            end_date=end_date,
            name="total_electricity_demand",
        )
    )
    if total_electricity_demand is None:
        raise HTTPException(
            status_code=404,
            detail="No total electricity demand data found for the given date range",
        )
    total_electricity_demand.rename(
        columns={"value": total_electricity_demand["name"][0]}, inplace=True
    )
    total_electricity_demand.drop(columns=["name"], inplace=True)
    electricity_price_data = crud.prices.get_multi_by_date_range_and_source(
        db=db,
        start_date=start_date,
        end_date=end_date,
        source=electricity_price_data_source,
    )
    if electricity_price_data is None:
        raise HTTPException(
            status_code=404,
            detail="No price data found for the given date range and source",
        )
    if gas_price_data_source != "constant":
        gas_price_data = crud.prices.get_multi_by_date_range_and_source(
            db=db,
            start_date=start_date,
            end_date=end_date,
            source=gas_price_data_source,
        )
        if gas_price_data is None:
            raise HTTPException(
                status_code=404,
                detail="No gas price data found for the given date range and source",
            )
        # as gas price data is only daily data, we interpolate it to 15 minutes
        gas_price_data = (
            gas_price_data.set_index("timestamp")
            .resample("15min")
            .ffill()
            .reset_index()
        )
    else:
        gas_price_data = pd.DataFrame(
            {
                "timestamp": pd.date_range(
                    start=start_date, end=end_date, freq="15min"
                ),
                "price": cost_per_mwh_gas,
                "source": "constant",
            }
        )

    # Merge dataframes and check granularity
    merged_data = check_granularity_and_merge(footprint_data, heat_demand, method="sum")
    merged_data = check_granularity_and_merge(
        merged_data, total_electricity_demand, method="sum"
    )
    merged_data = check_granularity_and_merge(
        merged_data,
        electricity_price_data[["timestamp", "price"]].rename(
            columns={"price": "electricity_price"}
        ),
    )  # TODO make this better
    merged_data = check_granularity_and_merge(
        merged_data,
        gas_price_data[["timestamp", "price"]].rename(columns={"price": "gas_price"}),
    )
    merged_data["gas_data_source"] = gas_price_data["source"]
    merged_data["electricity_data_source"] = electricity_price_data["source"]

    if merged_data.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No data found for the given date range",
        )

    if merged_data["timestamp"][0] != pd.to_datetime(start_date).tz_localize(
        "UTC"
    ) or merged_data["timestamp"].iloc[-1] != pd.to_datetime(end_date).tz_localize(
        "UTC"
    ):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Data does not cover the entire date range",
        )
    # we can also make full load hours a parameter. Then we would not need the time series anymore.
    granularity_factor = 1 / (
        merged_data["timestamp"].diff().min().total_seconds() / 3600
    )
    full_load_hours = merged_data["total_electricity_demand"].sum() / (
        merged_data["total_electricity_demand"].max() * granularity_factor
    )

    if network_fee == "static":
        merged_data["electricity_price"] = merged_data[
            "electricity_price"
        ] + network_fee_value * (1 - relative_network_fee_reduction)
        merged_data["window_type"] = 0
    elif network_fee == "dynamic":
        merged_data = calculate_dynamic_network_fee(
            merged_data,
            network_fee_value,
            relative_network_fee_reduction,
            relative_network_fee_surcharge,
            window_size,
        )
    else:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid network fee type",
        )
    merged_data["gas_price"] = merged_data["gas_price"] + gas_network_fee

    # Parameters
    heat_demand_dict = merged_data[
        "flexible_device_demand"
    ].to_dict()  # Heat demand per interval (kW)
    co2_electricity_dict = merged_data[
        "co2"
    ].to_dict()  # Electricity CO2 intensity (g/kWh)
    price_electricity_dict = merged_data[
        "electricity_price"
    ].to_dict()  # Electricity price (/MWh)
    price_gas_dict = merged_data["gas_price"].to_dict()  # Gas price (/MWh)
    electricity_demand = merged_data["total_electricity_demand"].to_dict()
    window_type = merged_data["window_type"].to_dict()

    time_interval_hours = merged_data["timestamp"].diff().min().total_seconds() / 3600

    optimization_results = optimize(
        co2_data=co2_electricity_dict,
        heat_demand_data=heat_demand_dict,
        electricity_price_data=price_electricity_dict,
        electricity_demand=electricity_demand,
        window_type=window_type,
        electric_heating=flexible_power,
        gas_emissions_factor=gas_emissions_factor,
        gas_price_data=price_gas_dict,
        co2_price=co2_price,
        ramp_up_rate=ramp_up_rate,
        ramp_down_rate=ramp_down_rate,
        minimum_runtime=minimum_runtime,
        time_interval_hours=time_interval_hours,
    )

    merged_data["total_electricity_demand_with_flexible_power"] = np.where(
        merged_data["window_type"] != 1,
        merged_data["total_electricity_demand"]
        + (
            optimization_results["optimized_results_df"]["electric_power_used_kW"]
            * time_interval_hours
        ),
        merged_data["total_electricity_demand"],
    )
    full_load_hours_after_optimization = merged_data[
        "total_electricity_demand_with_flexible_power"
    ].sum() / (
        merged_data["total_electricity_demand_with_flexible_power"].max()
        * granularity_factor
    )
    print("Full Load Hours After Optimization: ", full_load_hours_after_optimization)

    electric_heating_in_low_price_windows = merged_data[
        merged_data["window_type"] == 1
    ]["total_electricity_demand_with_flexible_power"].sum()
    electric_heating_in_low_price_windows_ratio = (
        electric_heating_in_low_price_windows
        / (merged_data["total_electricity_demand_with_flexible_power"].sum())
    )
    mean_electricity_price_when_heating = np.nanmean(
        np.where(
            optimization_results["optimized_results_df"]["electric_power_used_kW"] > 0,
            merged_data["electricity_price"],
            np.nan,
        )
    )
    # Store optimized electric power usage in DB if needed:
    optimization_results["optimized_results_df"]["timestamp"] = merged_data["timestamp"]
    flexible_power = pd.DataFrame(
        {
            "timestamp": merged_data["timestamp"],
            "electricity_used": optimization_results["optimized_results_df"][
                "electric_power_used_kW"
            ],
            "low_price_window": merged_data["window_type"],
            "optimization_case_name": optimization_case_name,
            "ref_created_by": current.id,
        }
    )

    optimization_results["name"] = optimization_case_name
    optimization_results["time_from"] = start_date
    optimization_results["time_to"] = end_date
    optimization_results["network_fee_type"] = network_fee
    optimization_results["network_fee"] = network_fee_value
    optimization_results["full_load_hours"] = full_load_hours
    optimization_results["full_load_hours_after_optimization"] = (
        full_load_hours_after_optimization
    )
    optimization_results["mean_electricity_price_when_heating"] = round(
        mean_electricity_price_when_heating, 2
    )
    optimization_results["electric_heating_in_low_price_windows_ratio"] = round(
        electric_heating_in_low_price_windows_ratio, 2
    )
    try:
        crud.flexible_power.create_multi(
            db=db, obj_in=flexible_power.to_dict(orient="records")
        )
        return crud.optimization_results.create(
            db=db, obj_in=optimization_results, user_id=current.id
        )
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# @router.post("/optimize_flexible_power_size", response_model=schemas.OptimizationResult)
# def optimize_flexible_power_size(
#     db: Session = Depends(deps.get_db),
#     current: model.User = Depends(deps.get_current_user),
#     start_date: datetime = "2024-09-10",
#     end_date: datetime = "2024-09-28",
#     max_flexible_power: float = 100000,
#     flexible_power_cost: float = 0.00001,
#     gas_emissions_factor: float = 800,
#     gas_price: float = 0.1,
#     w_cost: float = 0.5,
#     w_emissions: float = 0.5,
# ) -> schemas.OptimizationResult:
#     # Validate weights
#     if not (0 <= w_cost <= 1) or not (0 <= w_emissions <= 1):
#         raise HTTPException(status_code=400, detail="Weights must be between 0 and 1.")

#     if abs(w_cost + w_emissions - 1) > 1e-6:
#         raise HTTPException(
#             status_code=400, detail="The sum of w_cost and w_emissions must equal 1."
#         )

#     # Retrieve data from database
#     footprint_data = crud.footprint.get_multi_by_date_range(
#         db=db, start_date=start_date, end_date=end_date
#     )
#     heat_demand = crud.data_parc.get_multi_by_date_range(
#         db=db, user_id=current.id, start_date=start_date, end_date=end_date
#     )
#     price_data = crud.prices.get_multi_by_date_range(
#         db=db, start_date=start_date, end_date=end_date
#     )

#     # Merge dataframes and check granularity
#     merged_data = check_granularity_and_merge(footprint_data, heat_demand)
#     merged_data = check_granularity_and_merge(merged_data, price_data)

#     # Extract relevant data from the dataframes
#     co2_emissions = merged_data["co2"].values.tolist()
#     heat_demand_values = merged_data["value"].values.tolist()
#     timestamps = merged_data["timestamp"].values

#     el_price = merged_data["price"].tolist()

#     # Create Pyomo model
#     model = ConcreteModel()

#     # Define sets (time steps)
#     model.T = RangeSet(0, len(timestamps) - 1)

#     # Parameters
#     model.co2_emissions = Param(
#         model.T, initialize={i: co2_emissions[i] for i in range(len(co2_emissions))}
#     )
#     model.heat_demand = Param(
#         model.T,
#         initialize={i: heat_demand_values[i] for i in range(len(heat_demand_values))},
#     )
#     model.el_price = Param(
#         model.T, initialize={i: el_price[i] for i in range(len(el_price))}
#     )

#     # Decision variable: Single flexible power variable for the entire time period
#     model.flexible_power = Var(within=NonNegativeReals, bounds=(0, max_flexible_power))
#     model.gas_power = Var(model.T, within=NonNegativeReals)

#     # Objective function with normalization
#     def objective_rule(model):
#         max_cost = max(el_price) * max_flexible_power * len(
#             heat_demand_values
#         ) + gas_price * sum(heat_demand_values)
#         max_emission = gas_emissions_factor * sum(heat_demand_values)

#         normalized_cost = sum(
#             (model.el_price[t] * model.flexible_power + gas_price * model.gas_power[t])
#             / max_cost
#             for t in model.T
#         )

#         normalized_emission = sum(
#             (
#                 gas_emissions_factor * model.gas_power[t]
#                 + model.co2_emissions[t] * model.flexible_power
#             )
#             / max_emission
#             for t in model.T
#         )

#         flexible_cost = flexible_power_cost * model.flexible_power

#         return (
#             w_cost * normalized_cost + w_emissions * normalized_emission + flexible_cost
#         )

#     model.objective = Objective(rule=objective_rule, sense=minimize)

#     # Constraints: Ensure heat demand is met at each time step
#     def heat_demand_rule(model, t):
#         return model.gas_power[t] + model.flexible_power >= model.heat_demand[t] * 0.99

#     model.heat_demand_constraint = Constraint(model.T, rule=heat_demand_rule)

#     # Solve the optimization problem
#     solver = SolverFactory("glpk")
#     results = solver.solve(model)

#     # Check solver status
#     if results.solver.termination_condition != TerminationCondition.optimal:
#         raise HTTPException(
#             status_code=500,
#             detail=f"Optimization failed with termination condition {results.solver.termination_condition}",
#         )

#     # Extract the optimal flexible power value
#     try:
#         optimal_flexible_power = value(model.flexible_power)

#         return schemas.OptimizationResult(
#             optimal_flexible_power=optimal_flexible_power,
#             detail="Optimization completed successfully.",
#         )

#     except Exception as e:
#         raise HTTPException(
#             status_code=500,
#             detail=f"Error extracting results from optimization: {str(e)}",
#         )
```

## forest_ensys/api/endpoints/simulation_input_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from fastapi import APIRouter, Depends, File, UploadFile, HTTPException, status, Query
from fastapi.responses import JSONResponse
import pandas as pd
from sqlalchemy.orm import Session
import sqlalchemy

from forest_ensys import crud, model
from forest_ensys.api import deps
from forest_ensys.core.timeseries_hepers import ensure_consistent_granularity

router = APIRouter()


# we want a method that receives a excel file and uploads it to the database
@router.post(
    "/upload-heat-demand",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "message": "Data uploaded successfully",
                    }
                }
            }
        },
        400: {"description": "Invalid file format"},
        409: {"description": "Data already exists"},
        422: {"description": "Validation error"},
        500: {"description": "Internal server error"},
    },
)
async def upload_simulation_input_data(
    file: UploadFile = File(...),
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    name: str = Query(
        "flexibility_timeseries",
        enum=["flexible_device_demand", "total_electricity_demand"],
        description="The name of the simulation input data. 'flexible_device_demand' is the energy demand of the flexible device. 'total_electricity_demand' is the total electricity demand of the building.",
    ),
    delimiter: str = ";",
    skiprows: int = 3,
    DateTimeColumn: str = "DateTime",
    ValueColumn: str = "Value",
    unit: str = Query(
        "m/h",
        enum=["m/h", "kWh", "kW"],
        description="The unit of the heat demand. Can be 'm/h', 'kWh' or 'kW'",
    ),
    heating_value: float = 10.0,
    conversion_factor: float = 0.8,
):
    if unit.lower() not in ["m/h", "kw", "kwh", "m3/h"]:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Invalid unit. Use 'm/h', 'kWh' or 'kW'",
        )
    if (
        not file
        or not delimiter
        or skiprows is None
        or not DateTimeColumn
        or not ValueColumn
        or not unit
        or not heating_value
        or not conversion_factor
        or not name
    ):
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Missing required parameters",
        )

    try:
        df = pd.read_csv(
            file.file,
            skiprows=skiprows,
            delimiter=delimiter,
            usecols=[DateTimeColumn, ValueColumn],
        )
    except pd.errors.EmptyDataError:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "Empty/invalid CSV file")
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Processing failed: {str(e)}",
        )

    # crud.simulation_input_data.delete(db)

    df.rename(columns={DateTimeColumn: "timestamp", ValueColumn: "value"}, inplace=True)
    df, granularity = ensure_consistent_granularity(df)
    df["name"] = name
    if unit.lower() == "m/h" or unit.lower() == "m3/h":
        df["value"] = df["value"] * heating_value * conversion_factor
    elif unit.lower() == "kw":
        df["value"] = df["value"] * granularity
    crud.simulation_input_data.create_multi(db, obj_in=df.to_dict(orient="records"), ref_created_by=current.id)
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "success", "message": "Data uploaded successfully"},
    )
    
@router.delete(
    "/simulation_input_data",
    responses={
        200: {"description": "Data deleted successfully"},
        400: {"description": "Bad request"},
    },
)
def delete_simulation_input_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    name: str = Query(
        "flexibility_timeseries",
        enum=["flexible_device_demand", "total_electricity_demand"],
        description="The name of the simulation input data. 'flexible_device_demand' is the energy demand of the flexible device. 'total_electricity_demand' is the total electricity demand of the building.",
    ) 
):
    """
    Delete all simulation input data.
    """
    crud.simulation_input_data.delete_by_user_and_name(db, user_id=current.id, name=name)
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "success", "message": "Data deleted successfully"},
    )
```

## forest_ensys/api/endpoints/price_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends, File, UploadFile, HTTPException, status, Query
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core.timeseries_hepers import ensure_consistent_granularity
import pandas as pd

router = APIRouter()


@router.get("/", response_model=List[schemas.Prices])
def get_all_price_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Prices]:
    """
    Retrieve all price data
    """
    price_data = crud.prices.get_multi(db=db, skip=skip, limit=limit)
    return price_data

@router.get("/{source}", response_model=List[schemas.Prices])
def get_price_data_by_source(
    source: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Prices]:
    """
    Retrieve price data by source
    """
    try:
        price_data = crud.prices.get_by_source(db=db, source=source, skip=skip, limit=limit)
        return price_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post(
    "/upload-price-data",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "message": "Data uploaded successfully",
                    }
                }
            }
        },
        400: {"description": "Invalid file format"},
        422: {"description": "Validation error"},
        500: {"description": "Internal server error"},
    },
)
async def upload_price_data(
    file: UploadFile = File(...),
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    delimiter: str = ";",
    skiprows: int = 3,
    DateTimeColumn: str = "timestamp",
    ValueColumn: str = "price",
    source: str = "greenPFC",
):
    if (
        not file
        or not delimiter
        or skiprows is None
        or not DateTimeColumn
        or not ValueColumn
        or not source
    ):
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Missing required parameters",
        )
    # check if file is excel or csv
    if not file.filename.endswith((".csv", ".xls", ".xlsx")):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid file format. Only CSV and Excel files are allowed.",
        )
    try:
        if file.filename.endswith(".csv"):
            df = pd.read_csv(
                file.file,
                skiprows=skiprows,
                delimiter=delimiter,
                usecols=[DateTimeColumn, ValueColumn],
            )
        else:
            df = pd.read_excel(
                file.file, skiprows=skiprows, usecols=[DateTimeColumn, ValueColumn]
            )
    except pd.errors.EmptyDataError:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "Empty/invalid CSV file")
    except Exception as e:
        raise HTTPException(
            status.HTTP_500_INTERNAL_SERVER_ERROR, f"Error reading CSV file: {str(e)}"
        )
    df.rename(columns={DateTimeColumn: "timestamp", ValueColumn: "price"}, inplace=True)
    df, granularity = ensure_consistent_granularity(df, ignore_timezone=True)# TODO
    df["source"] = source
    crud.prices.create_multi(db, obj_in=df.to_dict(orient="records"))
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "success", "message": "Data uploaded successfully"},
    )


@router.delete(
    "/prices",
    responses={
        200: {"description": "Data deleted successfully"},
        404: {"description": "Error deleting prices"},
    },
)
def delete_prices(
    db: Session = Depends(deps.get_db),
    source: str = Query(..., description="Source of the prices to delete"),
):
    try:
        crud.prices.delete(db, source=source)
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"status": "success", "message": "Data deleted successfully"},
        )
    except Exception as e:
        raise HTTPException(
            status.HTTP_404_NOT_FOUND, f"Error deleting prices: {str(e)}"
        )
```

## forest_ensys/api/endpoints/result_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from fastapi import APIRouter, Depends, HTTPException, status, Query
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session

from forest_ensys import crud, model
from forest_ensys.api import deps

router = APIRouter()


@router.delete(
    "/",
    responses={
        200: {"description": "Data deleted successfully"},
        400: {"description": "Bad request"},
    },
)
def delete_result_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    optimization_case_name: str = Query(
        ..., description="The name of the optimization case."
    ),
):
    try:
        crud.optimization_results.delete_by_user_id_and_optimization_case_name(
            db, user_id=current.id, optimization_case_name=optimization_case_name
        )
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"status": "success", "message": "Data deleted successfully"},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"An error occurred: {str(e)}",
        )
```

## forest_ensys/api/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains the routes exposed as REST-API.
"""

from .api import api_router
```

## forest_ensys/api/api.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from fastapi import APIRouter

from .endpoints import (
    authentication,
    users,
    grid_data,
    process_electricity_data,
    process_heat_data,
    emissions_data,
    model,
    footprint_data,
    process_data,
    flexibility,
    simulation_input_data,
    price_data,
    result_data,
)

api_router = APIRouter()
api_router.include_router(
    authentication.router, prefix="/auth", tags=["Authentication"]
)
api_router.include_router(users.router, prefix="/users", tags=["Users"])
api_router.include_router(grid_data.router, prefix="/grid-data", tags=["Grid Data"])
api_router.include_router(
    emissions_data.router, prefix="/emissions-data", tags=["Emissions Data"]
)
api_router.include_router(
    footprint_data.router, prefix="/footprint-data", tags=["Footprint Data"]
)
api_router.include_router(
    flexibility.router, prefix="/flexibility", tags=["Flexibility"]
)
api_router.include_router(
    process_electricity_data.router,
    prefix="/electricity-data",
    tags=["Process Electricity Data"],
)
api_router.include_router(
    process_heat_data.router, prefix="/heat-data", tags=["Process Heat Data"]
)
api_router.include_router(model.router, prefix="/model", tags=["Model"])
api_router.include_router(process_data.router, prefix="/process-data", tags=["Process Data"])
api_router.include_router(simulation_input_data.router, prefix="/simulation-input-data", tags=["Simulation Input Data"])
api_router.include_router(price_data.router, prefix="/prices", tags=["Prices"])
api_router.include_router(result_data.router, prefix="/results", tags=["Results"])
```

## forest_ensys/api/deps.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Generator

from fastapi import Depends, status, HTTPException
from fastapi.security import OAuth2PasswordBearer
from jose import jwt
from pydantic import ValidationError
from sqlalchemy.orm import Session

from forest_ensys import model, schemas, crud
from forest_ensys.core import settings, security
from forest_ensys.database.session import SessionLocal


def get_db() -> Generator:
    db = None
    try:
        db = SessionLocal()
        yield db
    finally:
        if db is not None:
            db.close()


oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/auth/login")


def get_current_user(
    db: Session = Depends(get_db), token: str = Depends(oauth2_scheme)
) -> model.User:
    try:
        payload = jwt.decode(
            token, settings.SECRET_KEY, algorithms=[security.ALGORITHM]
        )
        token_data = schemas.TokenPayload(**payload)
    except (jwt.JWTError, ValidationError):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Could not validate credentials",
        )
    user = crud.user.get(db, id=token_data.sub)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="User not found."
        )
    return user
```

## forest_ensys/core/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
All functions for everything.
"""

from .config import settings
```

## forest_ensys/core/calliope_model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import calliope
import pandas as pd
import yaml


def generate_calliope_model(
    model_dict: dict, electricity_data: pd.DataFrame, heat_data: pd.DataFrame
) -> calliope.Model:
    """
    Generates a calliope model from a dictionary.
    """
    model_def = calliope.AttrDict.from_yaml_string(str(model_dict))
    electricity_data.drop(columns=["id","ref_created_by"], inplace=True)    
    heat_data.drop(columns=["id","ref_created_by"], inplace=True)
    electricity_data.rename(columns={"timestamp": "timesteps"}, inplace=True)
    heat_data.rename(columns={"timestamp": "timesteps"}, inplace=True)
    electricity_data = electricity_data.set_index("timesteps")
    heat_data = heat_data.set_index("timesteps")
    location_names = list(model_dict.get("nodes").keys())
    # rename columns
    electricity_data.columns = pd.MultiIndex.from_tuples(
        [(node, "demand_electricity") for node in location_names]
    )
    heat_data.columns = pd.MultiIndex.from_tuples(
        [(node, "demand_heat") for node in location_names]
    )
    electricity_data.columns.names = ["nodes", "techs"]
    heat_data.columns.names = ["nodes", "techs"]
    df = pd.concat([electricity_data, heat_data], axis=1)
    df.index = df.index.strftime("%Y-%m-%d %H:%M:%S")
    return calliope.Model(
    model_def,
    data_source_dfs={"demand_data": df}
)
```

## forest_ensys/core/config.py

```python
import pathlib
import secrets
from typing import Optional, Dict, Any

from pydantic import validator, PostgresDsn
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """
    Settings for this application.

    You can override the variables with a .env file.
    You can override the variables (and .env file) by environment variables.
    """

    SERVER_NAME: str = "FOREST Energy System Model"

    # Secret key for hashing passwords
    SECRET_KEY: str = secrets.token_urlsafe(32)

    # Expire duration for access tokens
    # 60 minutes * 24 hours * 8 days = 8 days
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 8

    # Database access
    POSTGRES_SERVER: Optional[str] = None
    POSTGRES_USER: Optional[str] = None
    POSTGRES_PASSWORD: Optional[str] = None
    POSTGRES_DB: Optional[str] = None
    SQLALCHEMY_DATABASE_URI: Optional[str] = None

    @validator("SQLALCHEMY_DATABASE_URI", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
        if all(
            isinstance(values[key], str)
            for key in ("POSTGRES_USER", "POSTGRES_PASSWORD", "POSTGRES_SERVER")
        ):
            return PostgresDsn.build(
                scheme="postgresql",
                user=values.get("POSTGRES_USER"),
                password=values.get("POSTGRES_PASSWORD"),
                host=values.get("POSTGRES_SERVER"),
                path=f"/{values.get('POSTGRES_DB') or ''}",
            )

        config_folder = pathlib.Path(__file__).parent.resolve()
        return f"sqlite:///{config_folder}/../local.db"

    class Config:
        case_sensitive = True
        env_file = ".env"


settings = Settings()
```

## forest_ensys/core/security.py

```python
from datetime import datetime, timedelta
from typing import Any, Union

from jose import jwt
from passlib.context import CryptContext

from forest_ensys.core import settings

ALGORITHM = "HS256"

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def create_access_token(
    subject: Union[str, Any], expires_delta: timedelta = None
) -> str:
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(
            minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES
        )
    to_encode = {"exp": expire, "sub": str(subject)}
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt


def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)
```

## forest_ensys/core/crawlers.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import requests
import pandas as pd
import json
from io import StringIO


def crawl_emissions_data() -> pd.DataFrame:
    """
    Crawl the emissions data from the Electricity Maps database.
    """
    spreadsheet_id = "1ukTAD_oQKZfq-FgLpbLo_bGOv-UPTaoM_WS316xlDcE"
    url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv"
    response = requests.get(url)
    try:
        response.raise_for_status()
    except Exception as e:
        print(f"Coult not crawl emissions data: {e}")
    df = pd.read_csv(StringIO(response.text))
    df["datetime"] = pd.to_datetime(df["datetime"])
    df = df.dropna()
    return df


def get_data_per_commodity(
    commodity_id, commodity_name, start_date_unix, second_start_date_unix=None
) -> pd.DataFrame:
    url = f"https://www.smard.de/app/chart_data/{commodity_id}/DE/{commodity_id}_DE_quarterhour_{start_date_unix}.json"
    print(url)
    response = requests.get(url)
    try:
        response.raise_for_status()
    except requests.exceptions.HTTPError as e:
        print(f"Could not get data for commodity: {commodity_id} {e} trying different start date")
        return get_data_per_commodity(commodity_id, commodity_name, second_start_date_unix)
    data = json.loads(response.text)
    timeseries = pd.DataFrame.from_dict(data["series"])
    if timeseries.empty:
        print(f"Received empty data for commodity: {commodity_id}")
    timeseries[0] = pd.to_datetime(timeseries[0], unit="ms", utc=True)
    timeseries.columns = ["timestamp", "mwh"]
    timeseries["commodity_id"] = commodity_id
    timeseries["commodity_name"] = commodity_name
    timeseries = timeseries.dropna(subset="mwh")

    return timeseries
```

## forest_ensys/core/optimization.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pyomo.environ import (
    ConcreteModel,
    Var,
    Objective,
    SolverFactory,
    NonNegativeReals,
    minimize,
    Constraint,
    RangeSet,
    Param,
    value,
    Binary,
)
import pandas as pd
import random


def optimize_dryers(
    co2_data: dict,
    heat_demand_data: dict,
    electricity_price_data: dict,
    electricity_demand: dict,
    window_type: dict,
    electric_heating: float,
    gas_emissions_factor: float,
    gas_price_data: dict,
    co2_price: float,
    ramp_up_rate: int = 1,
    ramp_down_rate: int = 1,
    minimum_runtime: int = 1,
    time_interval_hours: float = 1,
) -> dict:
    """
    Optimizes the use of flexible power for electric heating in a dryer system.
    Parameters:
    - co2_data: DataFrame containing the electricity footprint data.
    - heat_demand_data: DataFrame containing the heat demand data.
    - electricity_price_data: DataFrame containing the electricity price data.
    - electricity_demand: DataFrame containing the electricity demand data.
    - window_type: DataFrame containing the window type data.
    - electric_heating: Maximum flexible power available for electric heating (kW).
    - gas_emissions_factor: Emissions factor for gas heating (g/kWh).
    - gas_price_data: DataFrame containing the gas price data.
    - cost_per_mwh_gas: Cost of gas heating per MWh (/MWh).
    - co2_price: Price of CO2 emissions (/ton CO2).
    - ramp_up_rate: Maximum ramp-up rate for electric heating (kW/h).
    - ramp_down_rate: Maximum ramp-down rate for electric heating (kW/h).
    - minimum_runtime: Minimum runtime for electric heating (h).
    - time_interval_hours: Time interval for the optimization (h).
    Returns:
    - A dictionary containing the optimized results.
    """

    # check if heat_demand, co2_data and price_data have the same length
    if (
        len(heat_demand_data) != len(co2_data)
        or len(heat_demand_data) != len(electricity_price_data)
        or len(heat_demand_data) != len(electricity_demand)
    ):
        raise ValueError(
            "The length of heat_demand, co2_data and price_data must be the same."
        )

    # Initialize model
    model = ConcreteModel()

    num_periods = len(heat_demand_data)
    print(f"Number of periods: {num_periods}")
    print(f"Time interval: {time_interval_hours} hours")
    model.T = RangeSet(0, num_periods - 1, doc="Time periods (integer indices)")

    model.heat_demand = Param(model.T, initialize=heat_demand_data)
    model.co2_electricity = Param(model.T, initialize=co2_data)
    model.price_electricity = Param(model.T, initialize=electricity_price_data)
    model.price_gas = Param(model.T, initialize=gas_price_data)
    model.electricity_demand = Param(model.T, initialize=electricity_demand)
    model.window_type = Param(model.T, initialize=window_type)
    model.max_total_demand = Var(within=NonNegativeReals)

    model.flexible_power_max = electric_heating  # kW
    model.gas_emissions_factor = gas_emissions_factor  # g/kWh
    model.co2_price = co2_price  # /ton CO2
    model.ramp_up_rate = ramp_up_rate
    model.ramp_down_rate = ramp_down_rate

    # Decision variables: Electric heating power used at each interval (kW)
    model.electric_power_used = Var(model.T, domain=NonNegativeReals)

    # Binary variable to track if the heater is running
    model.heater_on = Var(model.T, domain=Binary)

    # Auxiliary binary variable to track when the heater starts
    model.heater_start = Var(model.T, domain=Binary)

    # Constraints

    def ramp_up_rule(m, t):
        if t > 0:
            return (
                m.electric_power_used[t] - m.electric_power_used[t - 1]
                <= m.ramp_up_rate * time_interval_hours
            )
        return Constraint.Skip

    def ramp_down_rule(m, t):
        if t > 0:
            return (
                m.electric_power_used[t - 1] - m.electric_power_used[t]
                <= m.ramp_down_rate * time_interval_hours
            )
        return Constraint.Skip

    model.ramp_up = Constraint(model.T, rule=ramp_up_rule)
    model.ramp_down = Constraint(model.T, rule=ramp_down_rule)

    @model.Constraint(model.T)
    def heat_balance(m, t):
        return m.electric_power_used[t] * time_interval_hours <= m.heat_demand[t]

    @model.Constraint(model.T)
    def minimum_runtime_constraint(m, t):
        required_intervals = int(minimum_runtime / time_interval_hours)
        if t <= num_periods - required_intervals:
            # If heater turns on at t, it must stay on for required_intervals
            return sum(
                m.heater_on[t + i] for i in range(required_intervals)
            ) >= required_intervals * (
                m.heater_on[t] - (m.heater_on[t - 1] if t > 0 else 0)
            )
        else:
            # Near end of schedule: if heater turns on, it must stay on
            return sum(m.heater_on[t + i] for i in range(num_periods - t)) >= (
                num_periods - t
            ) * (m.heater_on[t] - (m.heater_on[t - 1] if t > 0 else 0))

    def max_electric_heating_rule(m, t):
        return m.electric_power_used[t] <= m.flexible_power_max * m.heater_on[t]

    model.max_electric_heating_constraint = Constraint(
        model.T, rule=max_electric_heating_rule
    )

    def electric_power_zero_when_off_rule(m, t):
        return m.electric_power_used[t] >= 0 * m.heater_on[t]

    model.electric_power_zero_when_off_constraint = Constraint(
        model.T, rule=electric_power_zero_when_off_rule
    )

    def electric_power_minimum_when_on_rule(m, t):
        return m.electric_power_used[t] >= 100 * m.heater_on[t]

    model.electric_power_minimum_when_on_constraint = Constraint(
        model.T, rule=electric_power_minimum_when_on_rule
    )

    def effective_total_demand(m, t):
        if m.window_type[t] != 1:
            return m.electricity_demand[t] + (
                m.electric_power_used[t] * time_interval_hours
            )
        else:
            return m.electricity_demand[t]

    def max_total_demand_rule(m, t):
        return m.max_total_demand >= effective_total_demand(m, t)

    model.max_total_demand_constraint = Constraint(model.T, rule=max_total_demand_rule)

    def full_load_hours_constraint_rule(m):
        total_demand = sum(effective_total_demand(m, t) for t in m.T)
        return total_demand >= (0.8*time_interval_hours*num_periods) * m.max_total_demand * (1 / time_interval_hours) # 0.8 is the minimum full load hours which is 7000 for one year

    model.full_load_hours_constraint = Constraint(rule=full_load_hours_constraint_rule)

    def total_cost_rule(m):
        electricity_cost = sum(
            m.electric_power_used[t]
            * m.price_electricity[t]
            * time_interval_hours
            * 1e-3
            for t in m.T
        )

        gas_cost = sum(
            (m.heat_demand[t] - (m.electric_power_used[t] * time_interval_hours))
            * m.price_gas[t]
            * 1e-3
            for t in m.T
        )

        emissions_gas = sum(
            (m.heat_demand[t] - (m.electric_power_used[t] * time_interval_hours))
            * m.gas_emissions_factor
            for t in m.T
        )
        emissions_electricity = sum(
            m.electric_power_used[t] * m.co2_electricity[t] * time_interval_hours
            for t in m.T
        )
        co2_cost = (emissions_gas + emissions_electricity) * 1e-6 * m.co2_price

        return electricity_cost + gas_cost + co2_cost

    model.total_cost = Objective(rule=total_cost_rule, sense=minimize)

    solver = SolverFactory("gurobi")
    solver.options["MIPGap"] = 0.001
    solver.options["TimeLimit"] = 120
    solver.options["OutputFlag"] = 1
    solver.options["Threads"] = 4
    
    results = solver.solve(model)

    print(results.solver.termination_condition)
    print(value(model.total_cost))

    optimized_results_df = pd.DataFrame(
        {
            "heat_demand_kwh": heat_demand_data.values(),
            "electric_power_used_kW": [
                value(model.electric_power_used[t]) for t in model.T
            ],
            "co2_electricity": co2_data.values(),
            "gas_price": gas_price_data.values(),
        }
    )
    optimized_results_df["gas_power_used_kwh"] = optimized_results_df[
        "heat_demand_kwh"
    ] - (optimized_results_df["electric_power_used_kW"] * time_interval_hours)

    optimized_results_df["emissions_gas"] = (
        optimized_results_df["gas_power_used_kwh"] * gas_emissions_factor
    )

    optimized_results_df["emissions_electricity"] = (
        optimized_results_df["electric_power_used_kW"]
        * optimized_results_df["co2_electricity"]
        * time_interval_hours
    )

    optimized_results_df["heater_on"] = [value(model.heater_on[t]) for t in model.T]

    total_energy_demand_kWh = optimized_results_df["heat_demand_kwh"].sum()
    electricity_used_kWh = (
        optimized_results_df["electric_power_used_kW"].sum() * time_interval_hours
    )
    gas_usage_kWh = total_energy_demand_kWh - electricity_used_kWh

    emissions_gas_only_tonnes = total_energy_demand_kWh * gas_emissions_factor * 1e-6

    cost_optimized_euro = value(model.total_cost)

    emissions_optimized_tonnes = (
        optimized_results_df["emissions_gas"].sum()
        + optimized_results_df["emissions_electricity"].sum()
    ) * 1e-6  # Gramm zu Tonnen

    emissions_savings_tonnes = emissions_gas_only_tonnes - emissions_optimized_tonnes

    # Cost savings calculation
    cost_gas_only = (
        (optimized_results_df["heat_demand_kwh"] / 1000)
        * optimized_results_df["gas_price"]
    ).sum() + (emissions_gas_only_tonnes * co2_price)
    cost_savings_euro = cost_gas_only - cost_optimized_euro

    return {
        "optimized_results_df": round(optimized_results_df, 2),
        "total_energy_demand": round(total_energy_demand_kWh, 2),
        "electricity_used": round(electricity_used_kWh, 2),
        "gas_usage": round(gas_usage_kWh, 2),
        "cost_savings": round(cost_savings_euro, 2),
        "cost_gas_only": round(cost_gas_only, 2),
        "cost_with_electric_heating": round(cost_optimized_euro, 2),
        "emissions_savings": round(emissions_savings_tonnes, 2),
        "emissions_gas_only": round(emissions_gas_only_tonnes, 2),
        "emissions_with_electric_heating": round(emissions_optimized_tonnes, 2),
    }


def test_optimize_dryers():
    """
    Test the optimize_dryers function with synthetic data to ensure constraints work as expected.
    """

    # Synthetic data
    time_intervals = 12  # 24 hours (1-hour intervals)
    time_interval_hours = 0.25  # Time interval in hours, e.g., 0.25 for 15 minutes,
    co2_data = {
        t: random.randint(0, 500)
        for t in range(int(time_intervals / time_interval_hours))
    }
    heat_demand_data = {
        t: random.randint(1000, 5000)
        for t in range(int(time_intervals / time_interval_hours))
    }
    price_data = {
        t: random.randint(0, 100)
        for t in range(int(time_intervals / time_interval_hours))
    }

    electric_heating = 200  # Max capacity of electric heating (kW)
    gas_emissions_factor = 250  # Gas emissions factor (g/kWh)
    cost_per_mwh_gas = 45  # Cost of gas heating (/MWh)
    co2_price = 50  # Price of CO2 emissions (/ton)
    ramp_up_rate = 100  # Max ramp-up rate (kW/h)
    ramp_down_rate = 100  # Max ramp-down rate (kW/h)
    minimum_runtime = 3  # Minimum runtime in hours

    # Run the optimization
    results = optimize_dryers(
        co2_data=co2_data,
        heat_demand_data=heat_demand_data,
        electricity_price_data=price_data,
        electric_heating=electric_heating,
        gas_emissions_factor=gas_emissions_factor,
        gas_price_data=cost_per_mwh_gas,
        co2_price=co2_price,
        ramp_up_rate=ramp_up_rate,
        ramp_down_rate=ramp_down_rate,
        minimum_runtime=minimum_runtime,
        time_interval_hours=time_interval_hours,
    )

    optimized_results_df = results["optimized_results_df"]

    print("Optimization completed successfully.")

    # Validation checks
    violations = []

    # Check if electric power used exceeds max capacity or heat demand
    for t, row in optimized_results_df.iterrows():
        if row["electric_power_used_kW"] > electric_heating:
            violations.append(f"Time {t}: Electric power used exceeds max capacity.")
        if row["electric_power_used_kW"] > row["heat_demand_kwh"]:
            violations.append(f"Time {t}: Electric power used exceeds heat demand.")

    # Check if ramp-up and ramp-down rates are respected
    for t in range(1, len(optimized_results_df)):
        ramp_up_diff = (
            optimized_results_df.loc[t, "electric_power_used_kW"]
            - optimized_results_df.loc[t - 1, "electric_power_used_kW"]
        )
        if ramp_up_diff > ramp_up_rate:
            violations.append(f"Time {t}: Ramp-up rate exceeded ({ramp_up_diff} kW).")

        ramp_down_diff = (
            optimized_results_df.loc[t - 1, "electric_power_used_kW"]
            - optimized_results_df.loc[t, "electric_power_used_kW"]
        )
        if ramp_down_diff > ramp_down_rate:
            violations.append(
                f"Time {t}: Ramp-down rate exceeded ({ramp_down_diff} kW)."
            )

    # Check if minimum consecutive runtime is respected
    heater_on_series = optimized_results_df["heater_on"]

    consecutive_runtime_violations = []

    current_run_length = 0
    for t, heater_on in enumerate(heater_on_series):
        if heater_on == 1:
            current_run_length += time_interval_hours
        else:
            if current_run_length > 0 and current_run_length < minimum_runtime:
                consecutive_runtime_violations.append(
                    f"Heater turned off after only {current_run_length} hours at time {t}."
                )
            current_run_length = 0

    if current_run_length > 0 and current_run_length < minimum_runtime:
        consecutive_runtime_violations.append(
            f"Heater turned off after only {current_run_length} hours at end of schedule."
        )

    violations.extend(consecutive_runtime_violations)

    # add a violation if heater on is 1 and electric power used is 0
    for t, row in optimized_results_df.iterrows():
        if row["heater_on"] == 1 and row["electric_power_used_kW"] == 0:
            violations.append(f"Time {t}: Heater is on but electric power used is 0.")
        if row["heater_on"] == 0 and row["electric_power_used_kW"] > 0:
            violations.append(
                f"Time {t}: Heater is off but electric power used is greater than 0."
            )

    #    print(optimized_results_df["heater_start"])
    print(optimized_results_df["heater_on"])
    print(optimized_results_df["electric_power_used_kW"])

    # Report results
    if violations:
        print("The following constraints were violated:")
        for violation in violations:
            print(f"- {violation}")
        print("\nTest failed due to constraint violations.")
    else:
        print("All constraints are satisfied. Test passed.")


# Run the test
# test_optimize_dryers()
```

## forest_ensys/core/aas_helper.py

```python
import requests
import json
from typing import Dict, Any, Optional, Generator, List

from aas_core3 import jsonization
from aas_core3.types import (
    SubmodelElementCollection,
    Property,
    Range,
    SubmodelElement,
)

needed_properties = [
    "powerMax",
    "regenerationDuration",
    "from",
    "until",
    "activationGradient",
    "deactivationGradient",
    "electricityNetworkFee",
    "co2Price",
    "gasPrice",
]

class ServerEasyv3:
    @staticmethod
    def submodels_server_url() -> str:
        return "https://forest.nowum.fh-aachen.de/aas-env/submodels"

    def send_request_helper(self, url: str) -> Optional[requests.Response]:
        try:
            response = requests.get(url, timeout=3)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            return None

    def get_submodel(self, submodel_id: str) -> Optional[Any]:
        url = f"{self.submodels_server_url()}/{submodel_id}"
        response = self.send_request_helper(url)
        if response is None:
            return None

        try:
            json_data = response.json()
            submodel = jsonization.submodel_from_jsonable(json_data)
            print("Deserialization successful:")
            print(submodel)
            return submodel
        except json.JSONDecodeError as e:
            print(f"JSON decode failed: {e}")
            return None
        except jsonization.DeserializationException as ex:
            print(f"Deserialization failed: {ex}")
            return None

def traverse_elements(element: SubmodelElement) -> Generator[Dict[str, Any], None, None]:
    if isinstance(element, SubmodelElementCollection):
        if hasattr(element, 'value'):
            for sub_element in element.value:
                yield from traverse_elements(sub_element)
    elif isinstance(element, (Property, Range)):
        if element.id_short in needed_properties:
            value = getattr(element, "value", None)
            min_value = getattr(element, "min", None)
            max_value = getattr(element, "max", None)
            kind_value = getattr(getattr(element, "kind", None), "value", None) if hasattr(element, "kind") else None

            qualifier_values = [q.value for q in element.qualifiers] if hasattr(element, "qualifiers") and element.qualifiers else []
            qualifier_types = [q.type for q in element.qualifiers] if hasattr(element, "qualifiers") and element.qualifiers else []
            if value is None and min_value is None and max_value is None:
                return
            yield {
                "idShort": element.id_short,
                "value": value if value is not None else max_value,
                "qualifier_values": qualifier_values,
                "qualifier_types": qualifier_types,
            }

def get_data_from_aas() -> Dict[str, Any]:
    server = ServerEasyv3()
    submodel_id = "aHR0cHM6Ly9hZG1pbi1zaGVsbC5pby9pZHRhL0VuZXJneUZsZXhpYmlsaXR5RGF0YU1vZGVsLzEvMC9FbmVyZ3lGbGV4aWJpbGl0eURhdGFNb2RlbA"
    submodel = server.get_submodel(submodel_id)

    return_dict = {}
    if submodel and hasattr(submodel, "submodel_elements") and submodel.submodel_elements:
        print("Submodel elements found.")
        for element in submodel.submodel_elements:
            for elem_data in traverse_elements(element):
                return_dict[elem_data["idShort"]] = elem_data["value"]
    return return_dict

if __name__ == "__main__":
    print(get_data_from_aas())
```

## forest_ensys/core/timeseries_helpers.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import pandas as pd
from datetime import timedelta
import numpy as np


def ensure_consistent_granularity(
    df, method="mean", ignore_timezone=False
) -> (pd.DataFrame, float):
    """
    Ensures that the DataFrame has a consistent granularity.

    Parameters:
    ----------
    df : pd.DataFrame
        The DataFrame to check and resample.
    method : str, optional
        The method to use for resampling (default is "mean").

    Returns:
    -------
    a float representing the granularity of the dataframe
    """
    if ignore_timezone:
        try:
            df["timestamp"] = (
                df["timestamp"]
                .str.replace(r"([+-]\d{2}:?\d{0,2})$", "", regex=True)
                .str.strip()
            )
        except AttributeError:
            pass
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True)
    granularity = df["timestamp"].diff().min()
    df_resampled = (
        df.set_index("timestamp").resample(granularity).agg(method).reset_index()
    )
    return df_resampled, granularity.total_seconds() / 3600


def check_granularity_and_merge(df1, df2, method="mean"):
    """
    Checks the granularity of the two DataFrames and merges them.

    Parameters:
    ----------
    df1 : pd.DataFrame
        The first DataFrame.
    df2 : pd.DataFrame
        The second DataFrame.

    Returns:
    -------
    pd.DataFrame
        The merged DataFrame.
    """
    df1["timestamp"] = pd.to_datetime(df1["timestamp"], utc=True)
    df2["timestamp"] = pd.to_datetime(df2["timestamp"], utc=True)
    df1_granularity = df1["timestamp"].diff().min()
    df2_granularity = df2["timestamp"].diff().min()
    if df1_granularity < df2_granularity:
        df1_resampled = (
            df1.set_index("timestamp")
            .resample(df2_granularity)
            .agg(method)
            .reset_index()
        )
        df2_resampled = df2
    else:
        df2_resampled = (
            df2.set_index("timestamp")
            .resample(df1_granularity)
            .agg(method)
            .reset_index()
        )
        df1_resampled = df1
    return pd.merge(df1_resampled, df2_resampled, on="timestamp", how="inner")


def get_reference_day(date):
    weekday = date.weekday()
    if weekday < 5:  # Mon-Fri
        days_back = 1 if weekday > 0 else 3
        ref_date = date - timedelta(days=days_back)
    else:
        ref_date = date - timedelta(days=7)
    return ref_date.date()


def select_peaks_no_overlap(day_df, window_size, price_col, kind="min"):
    """
    Select up to two non-overlapping peak timestamps (min or max) between 6:00 and 21:59.
    Each window is [peak_time - 2h, peak_time + 2h), i.e. 16 quarters.
    Windows must not overlap.
    Returns a list of peak timestamps.
    """
    # Only consider 6:00 to 21:59 (so window fits in day)
    mask = (day_df["timestamp"].dt.hour >= 6) & (day_df["timestamp"].dt.hour <= 21)
    df = day_df[mask].copy()
    if df.empty:
        return []
    # Sort by price
    if kind == "min":
        sorted_df = df.sort_values(price_col, ascending=True)
    else:
        sorted_df = df.sort_values(price_col, ascending=False)
    used_intervals = set()
    peaks = []
    for _, row in sorted_df.iterrows():
        peak_time = row["timestamp"]
        # Build set of all 15-min timestamps in the 4h window
        window_start = peak_time - timedelta(hours=window_size, minutes=15)
        window_end = peak_time + timedelta(hours=window_size)
        window_intervals = set(pd.date_range(window_start, window_end, freq="15min"))
        # Check for overlap with already selected windows
        if not window_intervals & used_intervals:
            peaks.append(peak_time)
            used_intervals.update(window_intervals)
        if len(peaks) == 2:
            break
    return sorted(peaks)


def calculate_dynamic_network_fee(
    merged_data,
    network_fee_value,
    relative_network_fee_reduction,
    relative_network_fee_surcharge,
    window_size
):
    merged_data = merged_data.copy()
    merged_data["timestamp"] = pd.to_datetime(merged_data["timestamp"])
    merged_data = merged_data.sort_values("timestamp").reset_index(drop=True)
    merged_data["date"] = merged_data["timestamp"].dt.date

    # Step 1: For each day, find non-overlapping min/max peaks
    peak_info = {}
    for date, group in merged_data.groupby("date"):
        min_peaks = select_peaks_no_overlap(
            group, window_size, price_col="electricity_price", kind="min"
        )
        max_peaks = select_peaks_no_overlap(
            group, window_size, price_col="electricity_price", kind="max"
        )
        peak_info[date] = {"min_peaks": min_peaks, "max_peaks": max_peaks}

    # Step 2: For each day, apply the reference day's windows
    merged_data["in_low_window"] = False
    merged_data["in_high_window"] = False
    for date in merged_data["date"].unique():
        ref_date = get_reference_day(pd.Timestamp(date))
        if ref_date not in peak_info:
            continue
        min_peaks = peak_info[ref_date]["min_peaks"]
        max_peaks = peak_info[ref_date]["max_peaks"]
        # Low price windows
        for peak_time in min_peaks:
            window_start = peak_time - timedelta(hours=window_size, minutes=15)
            window_end = peak_time + timedelta(hours=window_size)
            mask = (
                (merged_data["date"] == date)
                & (merged_data["timestamp"].dt.time >= window_start.time())
                & (merged_data["timestamp"].dt.time < window_end.time())
            )
            merged_data.loc[mask, "in_low_window"] = True

        # High price windows
        for peak_time in max_peaks:
            window_start = peak_time - timedelta(hours=window_size, minutes=15)
            window_end = peak_time + timedelta(hours=window_size)
            mask = (
                (merged_data["date"] == date)
                & (merged_data["timestamp"].dt.time >= window_start.time())
                & (merged_data["timestamp"].dt.time < window_end.time())
            )
            merged_data.loc[mask, "in_high_window"] = True

    # Step 3: High price window takes precedence
    # merged_data['window_type'] = np.where(
    #     merged_data['in_high_window'], 'high',
    #     np.where(merged_data['in_low_window'], 'low', 'normal')
    # )
    # Step 3: Low price window takes precedence
    merged_data["window_type"] = np.where(
        merged_data["in_low_window"],
        1,
        np.where(merged_data["in_high_window"], 2, 0),
    )

    # Step 4: Calculate dynamic price
    merged_data["electricity_price"] = merged_data.apply(
        lambda row: row["electricity_price"]
        + (
            network_fee_value * (1 - relative_network_fee_reduction)
            if row["window_type"] == 1
            else network_fee_value * (1 + relative_network_fee_surcharge)
            if row["window_type"] == 2
            else network_fee_value
        ),
        axis=1,
    )

    return merged_data


# df = pd.DataFrame({
#     'timestamp': pd.date_range("2025-03-01 00:00", periods=4800, freq="15min"),
#     'electricity_price': np.random.uniform(50, 200, 4800)
# })

# result = calculate_dynamic_network_fee(
#     df,
#     network_fee_value=20,
#     relative_network_fee_reduction=0.8,  # 80% Reduktion im Niedrigpreisfenster
#     relative_network_fee_surcharge=0.5   # 50% Zuschlag im Hochpreisfenster
# )

# print(result[['timestamp', 'electricity_price', 'dynamic_price', 'window_type']])
# result.to_csv('dynamic_prices.csv', index=False)
```

## forest_ensys/crud/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains the CRUD operations (CREATE, READ, UPDATE, DELETE) for each repository/table in database.
"""

from .user import user
from .grid import grid
from .process_electricity import process_electricity
from .process_heat import process_heat
from .emissions import emissions
from .model import model
from .footprint import footprint
from .data_parc import data_parc
from .flexible_power import flexible_power
from .optimization_results import optimization_results
from .prices import prices
from .simulation_input_data import simulation_input_data
```

## forest_ensys/crud/data_parc.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from sqlalchemy.orm import Session
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import DataParc
from forest_ensys.schemas import DataParcCreate, DataParcUpdate
from datetime import datetime


class CRUDDataParc(CRUDBase[DataParc, DataParcCreate, DataParcUpdate]):
    def create(self, db: Session, *, obj_in: DataParcCreate, user_id: int) -> DataParc:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj
    
    def delete(self, db: Session) -> Optional[DataParc]:
        return db.query(DataParc).delete()
    

data_parc = CRUDDataParc(DataParc)
```

## forest_ensys/crud/emissions.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session
from sqlalchemy import desc


from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Emissions
from forest_ensys.schemas import EmissionsCreate, EmissionsUpdate


class CRUDEmissions(CRUDBase[Emissions, EmissionsCreate, EmissionsUpdate]):
    def get_current_emissions(self, db: Session) -> Optional[Emissions]:
        latest = db.query(Emissions).order_by(desc(Emissions.timestamp)).first()
        return db.query(Emissions).filter(
            Emissions.timestamp == latest.timestamp,
            Emissions.zone_key == "DE",
            Emissions.emission_factor_type == "direct",
        )

    def get_specific_emissions(
        self, db: Session, *, zone_key: str, emission_type: str, production_mode: str
    ) -> Optional[Emissions]:
        return (
            db.query(Emissions)
            .filter(
                Emissions.zone_key == zone_key,
                Emissions.emission_factor_type == emission_type,
                Emissions.production_mode == production_mode,
            )
            .order_by(desc(Emissions.timestamp))
            .first()
        )

    def create(
        self, db: Session, obj_in=Emissions | EmissionsCreate | dict[str, any]
    ) -> Optional[Emissions]:
        new_dataset: Emissions = super().create_multi(db=db, obj_in=obj_in)
        return new_dataset

    # do a delete from emissions
    def delete(self, db: Session) -> Optional[Emissions]:
        return db.query(Emissions).delete()


emissions = CRUDEmissions(Emissions)
```

## forest_ensys/crud/footprint.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from sqlalchemy.orm import Session
from sqlalchemy import desc
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Footprint
from forest_ensys.schemas import FootprintCreate, FootprintUpdate
from datetime import datetime


class CRUDFootprint(CRUDBase[Footprint, FootprintCreate, FootprintUpdate]):
    def get_latest(self, db: Session) -> Optional[Footprint]:
        return db.query(Footprint).order_by(desc(Footprint.timestamp)).first()

    def create(self, db: Session, *, obj_in: FootprintCreate) -> Optional[Footprint]:
        new_dataset: Footprint = super().create(db, obj_in=obj_in)
        return new_dataset
    
    # def get_multi_by_date_range(self,db: Session, start_date: datetime, end_date: datetime):
    #     return db.query(Footprint).filter(Footprint.timestamp >= start_date, Footprint.timestamp <= end_date).all()
    def delete(self, db: Session) -> Optional[Footprint]:
        return db.query(Footprint).delete()


footprint = CRUDFootprint(Footprint)
```

## forest_ensys/crud/grid.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import desc, func
import pandas as pd
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Grid, Footprint
from forest_ensys.schemas import GridCreate, GridUpdate


class CRUDGrid(CRUDBase[Grid, GridCreate, GridUpdate]):
    def get_current_grid(self, db: Session) -> Optional[Grid]:
        latest = db.query(Grid).order_by(desc(Grid.timestamp)).first()
        return db.query(Grid).filter(Grid.timestamp == latest.timestamp)

    def get_latest_for_commodity(self, db: Session, commodity_id: int) -> pd.Timestamp:
        return (
            db.query(Grid)
            .filter(Grid.commodity_id == commodity_id)
            .order_by(desc(Grid.timestamp))
            .first()
        )

    def get_average_co2_by_commodity(self, db: Session) -> List:
        return (
            db.query(
                Grid.timestamp,
                func.sum(Grid.co2).label("total_co2"),
                func.sum(Grid.mwh).label("total_mwh"),
            )
            .outerjoin(Footprint, Grid.timestamp == Footprint.timestamp)
            .filter(Footprint.timestamp.is_(None))
            .group_by(Grid.timestamp)
            .all()
        )

    def create(self, db: Session, obj_in: Grid | dict[str, Any]) -> Optional[Grid]:
        new_dataset: Grid = super().create(db, obj_in=obj_in)
        return new_dataset
    
    def delete(self, db: Session) -> Optional[Grid]:
        return db.query(Grid).delete()


grid = CRUDGrid(Grid)
```

## forest_ensys/crud/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session
from sqlalchemy import func

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Model
from forest_ensys.schemas import ModelCreate, ModelUpdate


class CRUDModel(CRUDBase[Model, ModelCreate, ModelUpdate]):
    def create(self, db: Session, *, obj_in: ModelCreate, user_id: int) -> Model:
        """
        Creates a new Model object.
        """
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        obj_in_dict["model"] = obj_in_dict["model"]
        db_obj = super().create(db, obj_in=obj_in_dict)
        # db_obj.model = json.loads(db_obj.model)
        return db_obj

    def get_by_name(self, db: Session, *, name: str) -> Optional[Model]:
        # db_obj = db.query(Model).filter(Model.model["model"]["name"] == name).first()
        db_obj = (
            db.query(Model)
            .filter(func.json_extract(Model.model, "$.model.name") == name)
            .first()
        )
        return db_obj
        # if db_obj is not None:
        #     obj_out_dict = db_obj.model
        # else:
        #     obj_out_dict = None
        # return obj_out_dict


model = CRUDModel(Model)
```

## forest_ensys/crud/process_electricity.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import ProcessElectricity
from forest_ensys.schemas import ProcessElectricityCreate, ProcessElectricityUpdate


class CRUDProcessElectricity(
    CRUDBase[ProcessElectricity, ProcessElectricityCreate, ProcessElectricityUpdate]
):
    def get_from_start_date(
        self, db: Session, user_id: int, start_date: str
    ) -> Optional[ProcessElectricity]:
        return db.query(ProcessElectricity).filter(
            ProcessElectricity.ref_created_by == user_id,
            ProcessElectricity.timestamp >= start_date,
        )

    def create(
        self, db: Session, *, obj_in: ProcessElectricityCreate, user_id: int
    ) -> ProcessElectricity:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj


process_electricity = CRUDProcessElectricity(ProcessElectricity)
```

## forest_ensys/crud/process_heat.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import ProcessHeat
from forest_ensys.schemas import ProcessHeatCreate, ProcessHeatUpdate
from datetime import datetime


class CRUDProcessHeat(CRUDBase[ProcessHeat, ProcessHeatCreate, ProcessHeatUpdate]):
    def get_from_start_date(
        self, db: Session, user_id: int, start_date: str
    ) -> Optional[ProcessHeat]:
        return db.query(ProcessHeat).filter(
            ProcessHeat.ref_created_by == user_id, ProcessHeat.timestamp >= start_date
        )

    def create(
        self, db: Session, *, obj_in: ProcessHeatCreate, user_id: int
    ) -> ProcessHeat:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj
    


process_heat = CRUDProcessHeat(ProcessHeat)
```

## forest_ensys/crud/user.py

```python
from typing import Optional, Union, Dict, Any

from sqlalchemy.orm import Session

from forest_ensys.core import security
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import User
from forest_ensys.schemas import UserCreate, UserUpdate


# noinspection PyMethodMayBeStatic,PyArgumentList
class CRUDUser(CRUDBase[User, UserCreate, UserUpdate]):
    def get_by_username(self, db: Session, *, username: str) -> Optional[User]:
        return db.query(User).filter(User.username == username).first()

    def create(self, db: Session, *, obj_in: UserCreate) -> User:
        db_obj = User(
            username=obj_in.username,
            hashed_password=security.get_password_hash(obj_in.password),
        )
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj

    def update(
        self, db: Session, *, db_obj: User, obj_in: Union[UserUpdate, Dict[str, Any]]
    ) -> User:
        if isinstance(obj_in, dict):
            update_data = obj_in
        else:
            update_data = obj_in.dict(exclude_unset=True)
        if update_data["password"]:
            hashed_password = security.get_password_hash(update_data["password"])
            del update_data["password"]
            update_data["hashed_password"] = hashed_password
        return super().update(db, db_obj=db_obj, obj_in=update_data)

    def authenticate(self, db: Session, *, username: str, password: str):
        user_obj = self.get_by_username(db, username=username)
        if not user_obj:
            return None
        if not security.verify_password(
            plain_password=password, hashed_password=user_obj.hashed_password
        ):
            return None
        return user_obj


user = CRUDUser(User)
```

## forest_ensys/crud/weather.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session
from sqlalchemy import desc

from forest_ensys.core import crawlers
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Weather
from forest_ensys.schemas import WeatherCreate, WeatherUpdate


class CRUDWeather(CRUDBase(Weather, WeatherCreate, WeatherUpdate)):
    def get_current_weather(self, db: Session) -> Optional[Weather]:
        latest = db.query(Weather).order_by(desc(Weather.timestamp)).first()
        return db.query(Weather).filter(Weather.timestamp == latest.timestamp)

    def create(self, db: Session) -> Optional[Weather]:
        db.add_all(crawlers.crawl_weather_data())
        db.commit()
        return db.query(Weather).order_by(desc(Weather.timestamp)).first()


grid = CRUDWeather(Weather)
```

## forest_ensys/crud/flexible_power.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List
from sqlalchemy.orm import Session
from forest_ensys.model import FlexiblePower
from forest_ensys.schemas import FlexiblePowerCreate, FlexiblePowerUpdate
from forest_ensys.crud.base import CRUDBase


class CRUDFlexiblePower(
    CRUDBase[FlexiblePower, FlexiblePowerCreate, FlexiblePowerUpdate]
):
    def create(self, db: Session, obj_in: FlexiblePower | FlexiblePowerCreate):
        db_obj = FlexiblePower(**obj_in.dict())
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj

    # def create_multi(
    #     self, db: Session, *, obj_in: List[FlexiblePowerCreate]
    # ) -> List[FlexiblePower]:
    #     """
    #     Create multiple flexible power objects.
    #     """
    #     db_objs = [FlexiblePower(**obj.dict()) for obj in obj_in]
    #     db.add_all(db_objs)
    #     db.commit()
    #     return db_objs

    def get_multi_flexible_power(self, db: Session, skip: int = 0, limit: int = 100):
        return db.query(FlexiblePower).offset(skip).limit(limit).all()
    
    def delete(self, db: Session, user_id: int):
        return db.query(FlexiblePower).filter(FlexiblePower.ref_created_by == user_id).delete()
    
    def delete_by_user_id_and_optimization_case_name(self, db: Session, user_id, optimization_case_name: str):
        return db.query(FlexiblePower).filter(FlexiblePower.ref_created_by == user_id, FlexiblePower.optimization_case_name == optimization_case_name).delete()


flexible_power = CRUDFlexiblePower(FlexiblePower)
```

## forest_ensys/crud/prices.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import desc
from sqlalchemy import text
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Prices
from datetime import datetime
import pandas as pd


class CRUDPrices(CRUDBase[Prices, Any, Any]):
    def get_by_timestamp(self, db: Session, *, timestamp: Any) -> Optional[Prices]:
        return db.query(Prices).filter(Prices.timestamp == timestamp).first()

    def get_latest(self, db: Session) -> Optional[Prices]:
        return db.query(Prices).filter(Prices.source == "smard").order_by(desc(Prices.timestamp)).first()

    def get_by_timestamp_range(
        self, db: Session, *, start: Any, end: Any
    ) -> List[Prices]:
        return (
            db.query(Prices)
            .filter(Prices.timestamp >= start, Prices.timestamp <= end)
            .all()
        )

    def get_by_source(
        self, db: Session, *, source: str, limit: int = 100, skip: int = 0
    ) -> List[Prices]:
        return (
            db.query(Prices)
            .filter(Prices.source == source)
            .offset(skip)
            .limit(limit)
            .all()
        )

    def get_multi_by_date_range_and_source(
        self, db: Session, *, start_date: datetime, end_date: datetime, source: str
    ) -> Optional[pd.DataFrame]:
        query = """
            SELECT * 
            FROM prices
            WHERE timestamp BETWEEN :start_date AND :end_date AND source = :source
        """

        # Abfrage ausfhren
        result = pd.read_sql_query(
            sql=text(query),
            con=db.connection(),
            params={
                "start_date": start_date,
                "end_date": end_date,
                "source": source,
            },
        )

        return result if not result.empty else None

    def create(self, db: Session, obj_in: Prices | dict[str, Any]) -> Optional[Prices]:
        new_dataset: Prices = super().create(db, obj_in=obj_in)
        return new_dataset

    def delete(self, db: Session, source: str) -> Optional[Prices]:
        return db.query(Prices).filter(Prices.source == source).delete()


prices = CRUDPrices(Prices)
```

## forest_ensys/crud/simulation_input_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any

from sqlalchemy.orm import Session
from sqlalchemy.sql import text

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import SimulationInputData
from datetime import datetime
import pandas as pd

class CRUDSimulationInputData(CRUDBase[SimulationInputData, Any, Any]):
    def create(
        self, db: Session, *, obj_in: SimulationInputData, user_id: int
    ) -> SimulationInputData:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj
    def delete_by_user_and_name(self, db: Session, *, user_id: int, name: str) -> Optional[SimulationInputData]:
        return db.query(self.model).filter(self.model.ref_created_by == user_id, self.model.name == name).delete()
    def get_multi_by_date_range_and_name(
        self, db: Session, *, start_date: datetime, end_date: datetime,user_id: int, name: str
    ) -> Optional[pd.DataFrame]:
        query = """
            SELECT * 
            FROM simulation_input_data
            WHERE timestamp BETWEEN :start_date AND :end_date AND ref_created_by = :user_id AND name = :name
        """

        # Abfrage ausfhren
        result = pd.read_sql_query(
            sql=text(query),
            con=db.connection(),
            params={
                "start_date": start_date,
                "end_date": end_date,
                "name": name,
                "user_id": user_id
            },
        )

        return result if not result.empty else None
    
simulation_input_data = CRUDSimulationInputData(SimulationInputData)
```

## forest_ensys/crud/optimization_results.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import desc
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import OptimizationResult


class CRUDOptimizationResults(CRUDBase[OptimizationResult, Any, Any]):
    def create(
        self, db: Session, *, obj_in: OptimizationResult, user_id: int = None
    ) -> OptimizationResult:
        if user_id is not None:
            obj_in["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in)
        return db_obj
    
    def get(self, db: Session, *, user_id: int, optimization_case_name: str) -> Optional[OptimizationResult]:
        return db.query(self.model).filter(
            self.model.ref_created_by == user_id,
            self.model.name == optimization_case_name,
        ).first()
    
    def delete(self, db: Session, *, user_id: int) -> None:
        db.query(self.model).filter(self.model.ref_created_by == user_id).delete()
        db.commit()
        
    def delete_by_user_id_and_optimization_case_name(
        self, db: Session, *, user_id: int, optimization_case_name: str
    ) -> None:
        db.query(self.model).filter(
            self.model.ref_created_by == user_id,
            self.model.name == optimization_case_name,
        ).delete()
        db.commit()

optimization_results = CRUDOptimizationResults(OptimizationResult)
```

## forest_ensys/crud/base.py

```python
from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union
from fastapi.encoders import jsonable_encoder
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import text
from forest_ensys.database.base_class import Base
import pandas as pd
from datetime import datetime

ModelType = TypeVar("ModelType", bound=Base)
CreateSchemaType = TypeVar("CreateSchemaType", bound=BaseModel)
UpdateSchemaType = TypeVar("UpdateSchemaType", bound=BaseModel)


class CRUDBase(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):
    def __init__(self, model: Type[ModelType]):
        """
        CRUD object with default methods to Create, Read, Update, Delete (CRUD).
        **Parameters**
        * `model`: A SQLAlchemy model class
        * `schema`: A Pydantic model (schema) class
        """
        self.model = model

    def get(self, db: Session, id: Any) -> Optional[ModelType]:
        return db.query(self.model).filter(self.model.id == id).first()

    def get_multi(
        self, db: Session, *, skip: int = 0, limit: int = 100, user_id: int = None
    ) -> List[ModelType]:
        if user_id:
            return (
                db.query(self.model)
                .filter(self.model.ref_created_by == user_id)
                .offset(skip)
                .limit(limit)
                .all()
            )
        else:
            return db.query(self.model).offset(skip).limit(limit).all()
        
    def get_multi_by_date_range(
        self, db: Session, user_id: Optional[int] = None, start_date: datetime = None, end_date: datetime = None
    ) -> Optional[pd.DataFrame]:
        # Basisabfrage
        query = f"""
            SELECT * 
            FROM {self.model.__tablename__}
            WHERE timestamp BETWEEN :start_date AND :end_date
        """
        
        # Dynamische Bedingung hinzufgen, falls user_id gesetzt ist
        if user_id is not None:
            query += " AND ref_created_by = :user_id"
        
        # Abfrage ausfhren
        result = pd.read_sql_query(
            sql=text(query),
            con=db.connection(),
            params={
                "user_id": user_id,
                "start_date": start_date,
                "end_date": end_date,
            }
        )
        
        return result if not result.empty else None


    def create(
        self, db: Session, *, obj_in: Union[CreateSchemaType, ModelType, dict]
    ) -> ModelType:
        if isinstance(obj_in, self.model):
            db_obj = obj_in
        elif isinstance(obj_in, dict):
            # filter obj_in to only pass fields in model to model's constructor
            data = {
                k: v
                for k, v in obj_in.items()
                if k in self.model.__table__.columns.keys()
            }
            db_obj = self.model(**data)
        else:
            obj_in_data = jsonable_encoder(
                obj_in, include=self.model.__table__.columns.keys()
            )
            db_obj = self.model(**obj_in_data)  # type: ignore
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj

    def create_multi(
        self, db: Session, *, obj_in: List[Union[CreateSchemaType, ModelType, dict]], ref_created_by: Optional[int] = None
    ) -> List[ModelType]:
        
        def _bulk_set_attr(objs: list, attr: str, value: Any):
            for obj in objs:
                setattr(obj, attr, value)
        if isinstance(obj_in, list) and all(isinstance(obj, self.model) for obj in obj_in):
            db_obj = obj_in
        elif isinstance(obj_in, list) and all(isinstance(obj, dict) for obj in obj_in):
            db_obj = [self.model(**data) for data in obj_in]
        else:
            raise ValueError("Invalid input type")
        if ref_created_by and db_obj[0].ref_created_by is None:
            _bulk_set_attr(db_obj, 'ref_created_by', ref_created_by)

        db.add_all(db_obj)
        db.commit()
        return db_obj

    def update(
        self,
        db: Session,
        *,
        db_obj: ModelType,
        obj_in: Union[UpdateSchemaType, Dict[str, Any]],
    ) -> ModelType:
        obj_data = jsonable_encoder(db_obj)
        if isinstance(obj_in, dict):
            update_data = obj_in
        else:
            update_data = obj_in.dict(exclude_unset=True)
        for field in obj_data:
            if field in update_data:
                setattr(db_obj, field, update_data[field])
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj
    
    def delete(self, db: Session) -> Optional[ModelType]:
        return db.query(self.model).delete()

    def remove(self, db: Session, *, id: Any) -> ModelType:
        obj = db.query(self.model).get(id)
        db.delete(obj)
        db.commit()
        return obj
```

## forest_ensys/database/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains everything needed for the database.
"""
```

## forest_ensys/database/base_class.py

```python
import re
from typing import Any

from sqlalchemy.orm import as_declarative, declared_attr


@as_declarative()
class Base:
    """
    Base class for all database models
    """

    id: Any
    __name__: str
    __allow_unmapped__ = True

    # Generate __tablename__ automatically
    @declared_attr
    def __tablename__(self) -> str:
        return re.sub(r"(?<!^)(?=[A-Z])", "_", self.__name__).lower()
```

## forest_ensys/database/init_db.py

```python
import logging
from forest_ensys.database.base_class import Base
from forest_ensys.database.session import engine, SessionLocal
from sqlalchemy import text

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_connection():
    try:
        db = SessionLocal()
        db.execute(text("SELECT 1"))
    except Exception as e:
        logger.error(e)
        raise e


def create_all():
    Base.metadata.create_all(engine)
```

## forest_ensys/database/session.py

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

from forest_ensys.core import settings

engine = create_engine(
    settings.SQLALCHEMY_DATABASE_URI,
    execution_options={"isolation_level": "AUTOCOMMIT"},
    pool_pre_ping=True,
    poolclass=StaticPool,
)
SessionLocal = sessionmaker(
    autocommit=False, autoflush=False, expire_on_commit=False, bind=engine
)
```

## forest_ensys/model/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains all models in database
"""

from .user import User
from .grid import Grid
from .process_electricity import ProcessElectricity
from .process_heat import ProcessHeat
from .emissions import Emissions
from .model import Model
from .footprint import Footprint
from .data_parc import DataParc
from .flexible_power import FlexiblePower
from .optimization_results import OptimizationResult
from .prices import Prices
from .simulation_input_data import SimulationInputData
```

## forest_ensys/model/data_parc.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime, String

from forest_ensys.database.base_class import Base


class DataParc(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    signal_id = Column(String, primary_key=True, nullable=False)
    signal_name = Column(String, nullable=False)
    value = Column(Double, nullable=False)
    unit = Column(String, nullable=False)
```

## forest_ensys/model/emissions.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, String, Double, DateTime
from forest_ensys.database.base_class import Base


class Emissions(Base):
    """
    Database class for the emissions of a powerplant.
    """

    timestamp = Column(DateTime, primary_key=True, nullable=False)
    zone_key = Column(String, primary_key=True, nullable=False)
    emission_factor_type = Column(String, primary_key=True, nullable=False)
    production_mode = Column(String, primary_key=True, nullable=False)
    value = Column(Double, nullable=False)
    source = Column(String, nullable=False)
```

## forest_ensys/model/footprint.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Double, DateTime
from forest_ensys.database.base_class import Base


class Footprint(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    co2 = Column(Double, primary_key=True, nullable=False)
```

## forest_ensys/model/grid.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Integer, String, Double, DateTime
from forest_ensys.database.base_class import Base


class Grid(Base):
    """
    Grid class
    """

    timestamp = Column(DateTime, primary_key=True, nullable=False)
    commodity_id = Column(Integer, primary_key=True, nullable=False)
    commodity_name = Column(String, nullable=False)
    mwh = Column(Double, nullable=False)
    co2 = Column(Double, nullable=False)
```

## forest_ensys/model/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, ForeignKey
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base


class Model(Base):
    id = Column(Integer, primary_key=True, index=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    model = Column(JSONB, nullable=False)

    user = relationship("User", back_populates="model")
```

## forest_ensys/model/process_electricity.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base


class ProcessElectricity(Base):
    id = Column(Integer, primary_key=True, index=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    timestamp = Column(DateTime, nullable=False)
    power_demand = Column(Double, nullable=False)

    user = relationship("User", back_populates="process_electricity")
```

## forest_ensys/model/process_heat.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base


class ProcessHeat(Base):
    id = Column(Integer, primary_key=True, index=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    timestamp = Column(DateTime, nullable=False)
    heat_demand = Column(Double, nullable=False)
    # temperature = Column(Double, nullable=False)
    # pressure = Column(Double, nullable=False)
    # mass_flow = Column(Double, nullable=False)

    user = relationship("User", back_populates="process_heat")
```

## forest_ensys/model/user.py

```python
from sqlalchemy import Column, Integer, String
from sqlalchemy.orm import relationship
from forest_ensys.database.base_class import Base


class User(Base):
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    process_electricity = relationship("ProcessElectricity", back_populates="user")
    process_heat = relationship("ProcessHeat", back_populates="user")
    model = relationship("Model", back_populates="user")
```

## forest_ensys/model/weather.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Integer, String, Double, DateTime
from forest_ensys.database.base_class import Base


class Weather(Base):
    id = Column(Integer, primary_key=True, index=True)
    timestamp = Column(DateTime, nullable=False)
    nuts_id = Column(String, nullable=False)
    temperature = Column(Double, nullable=False)
    humidity = Column(Double, nullable=False)
    ghi = Column(Double, nullable=False)
```

## forest_ensys/model/prices.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Double, DateTime, String
from forest_ensys.database.base_class import Base

class Prices(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    source = Column(String, primary_key=True, nullable=False)
    price = Column(Double, nullable=False)
```

## forest_ensys/model/simulation_input_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime, String
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base

class SimulationInputData(Base):
    timestamp = Column(DateTime, nullable=False, primary_key=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False, primary_key=True)
    name = Column(String, nullable=False, primary_key=True)
    value = Column(Double, nullable=False)
```

## forest_ensys/model/optimization_results.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Float, String, Integer, ForeignKey, DateTime
from forest_ensys.database.base_class import Base


class OptimizationResult(Base):
    name = Column(String, nullable=False, primary_key=True)
    ref_created_by = Column(
        Integer, ForeignKey("user.id"), nullable=False, primary_key=True
    )
    time_from = Column(DateTime, nullable=False)
    time_to = Column(DateTime, nullable=False)
    network_fee_type = Column(String, nullable=False)
    network_fee = Column(Float, nullable=False)
    total_energy_demand = Column(Float, nullable=False)
    electricity_used = Column(Float, nullable=False)
    gas_usage = Column(Float, nullable=False)
    cost_savings = Column(Float, nullable=False)
    emissions_savings = Column(Float, nullable=False)
    cost_gas_only = Column(Float, nullable=False)
    cost_with_electric_heating = Column(Float, nullable=False)
    emissions_gas_only = Column(Float, nullable=False)
    emissions_with_electric_heating = Column(Float, nullable=False)
    full_load_hours = Column(Float, nullable=False)
    full_load_hours_after_optimization = Column(Float, nullable=False)
    mean_electricity_price_when_heating = Column(Float, nullable=False)
    electric_heating_in_low_price_windows_ratio = Column(Float, nullable=False)
```

## forest_ensys/model/flexible_power.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, DateTime, Float, String, Integer, ForeignKey
from forest_ensys.database.base_class import Base


class FlexiblePower(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    ref_created_by = Column(
        Integer, ForeignKey("user.id"), nullable=False, primary_key=True
    )
    optimization_case_name = Column(String, primary_key=True, nullable=False)
    electricity_used = Column(Float, nullable=False)
    low_price_window = Column(Integer, nullable=False)
```

## forest_ensys/schemas/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains every model that is returned from the Rest-API.
"""

from .user import User, UserCreate, UserInDB, UserUpdate
from .token import Token, TokenPayload
from .grid import Grid, GridCreate, GridUpdate
from .process_electricity import (
    ProcessElectricity,
    ProcessElectricityCreate,
    ProcessElectricityUpdate,
)
from .process_heat import ProcessHeat, ProcessHeatCreate, ProcessHeatUpdate
from .emissions import Emissions, EmissionsCreate, EmissionsUpdate
from .co2 import Co2
from .model import Model, ModelCreate, ModelUpdate
from .footprint import Footprint, FootprintCreate, FootprintUpdate
from .data_parc import DataParc, DataParcCreate, DataParcUpdate
from .optimization_results import OptimizationResult
from .prices import Prices
from .flexible_power import FlexiblePower, FlexiblePowerCreate, FlexiblePowerUpdate
```

## forest_ensys/schemas/co2.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field


class Co2(BaseModel):
    co2: float = Field(None, description="co2 in kg_co2/kwh", example="5000")
```

## forest_ensys/schemas/data_parc.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class DataParcBase(BaseModel):
    """
    Base class for Process  entries of data parc
    """

    timestamp: datetime = Field(None, description="timestamp for every entry")
    signal_id: str = Field(
        None,
        description="uuid of the signal",
        example="123e4567-e89b-12d3-a456-426655440000",
    )
    signal_name: str = Field(
        None, description="name of the signal", example="Dampfverbrauch"
    )
    value: float = Field(None, description="value of the signal", example=100.0)
    unit: str = Field(None, description="unit of the signal", example="kWh")


class DataParcCreate(DataParcBase):
    """
    Create a Process entry
    """

    pass


class DataParcUpdate(DataParcBase):
    """
    Update a Process entry
    """

    pass


class DataParcInDBBase(DataParcBase):
    """
    Process entry to return via API
    """

    ref_created_by: Optional[int] = Field(
        None, description="if of who send this  entry", example="1"
    )

    class Config:
        from_attributes = True


class DataParc(DataParcInDBBase):
    pass
```

## forest_ensys/schemas/emissions.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class EmissionsBase(BaseModel):
    """
    Shared attributes for the emissions table.
    """

    timestamp: Optional[datetime] = Field(
        default=None,
        description="Date and time when the emission factors were updated.",
        example="2020-01-01 00:00:00",
    )
    zone_key: Optional[str] = Field(
        default=None, description="Zone key of the country.", example="DE"
    )
    emission_factor_type: Optional[str] = Field(
        default=None,
        description="Emissions factor of the commodity.",
        example="direct or lifecycle",
    )
    production_mode: Optional[str] = Field(
        default=None, description="Energy commodity.", example="gas"
    )
    value: Optional[float] = Field(
        default=None, description="Emissions factor value.", example=820
    )
    source: Optional[str] = Field(
        default=None,
        description="Source of how the emissions factor is calculated.",
        example="ENTSO-E 2021",
    )


class EmissionsCreate(EmissionsBase):
    """
    Attributes to receive via API on creation of a Emissions object.
    """

    pass


class EmissionsUpdate(EmissionsBase):
    """
    Attributes to receive via API on update of a Emissions object.
    """

    pass


class EmissionsInDB(EmissionsBase):
    """
    Attributes to return via API for an Emissions object.
    """

    pass

    class Config:
        from_attributes = True


class Emissions(EmissionsInDB):
    """
    Attributes to return via API for an Emissions object.
    """

    pass
```

## forest_ensys/schemas/footprint.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field
from datetime import datetime


class Footprint(BaseModel):
    timestamp: datetime = Field(
        None, description="timestamp for every entry", example="yyyy-mm-dd hh:mm:ss"
    )
    co2: float = Field(None, description="co2 in g_co2/kwh", example="370")


class FootprintCreate(Footprint):
    pass


class FootprintUpdate(Footprint):
    pass


class FootprintInDBBase(Footprint):
    class Config:
        from_attributes = True


class Footprint(FootprintInDBBase):
    pass
```

## forest_ensys/schemas/grid.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field
from datetime import datetime


class GridBase(BaseModel):
    """
    Shared attributes for the grid table.
    """

    timestamp: datetime = Field(
        None, description="timestamp for every entry", example="yyyy-mm-dd hh:mm:ss"
    )
    mwh: float = Field(None, description="power value in mwh.", example="5000")
    commodity_id: int = Field(None, description="id of the energy type", example="4067")
    commodity_name: str = Field(
        None, description="name of the energy type", example="Wind Onshore"
    )
    co2: float = Field(None, description="co2 in kg_co2/kwh", example="5000")


class GridCreate(GridBase):
    """
    Attributes to receive via API on creation of a dataset.
    """

    pass


class GridUpdate(GridBase):
    """
    Attributes to receive via API on update of a dataset.
    """

    pass


class GridInDBBase(GridBase):
    class Config:
        from_attributes = True


class Grid(GridInDBBase):
    """
    Attributes to return via API for a dataset.
    """

    pass
```

## forest_ensys/schemas/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import yaml
from typing import Optional, Dict
from pydantic import BaseModel, Field

yaml_data = """
config:
  init:
    name: Wepa Mainz
    calliope_version: 0.7.0

  build:
    mode: plan # Choices: plan, operate
    ensure_feasibility: true # Switching on unmet demand

  solve:
    solver: cbc

parameters:
  objective_cost_weights:
    data: 1
    index: monetary
    dims: costs
  bigM: 1e6

data_sources:
  demand:
    source: demand_data
    rows: timesteps
    columns: [nodes, techs]
    add_dims:
      parameters: resource
    
nodes:
  X1:
    techs:
      supply_grid_power:
        cost_flow_cap.data: 100 # cost of transformers
      supply_gas:
      supply_grid_heat:
      demand_electricity:
      demand_heat:
    

techs:
  supply_grid_power:
    name: "National grid import"
    base_tech: supply
    inherit: interest_rate_setter
    carrier_out: electricity
    source_use_max: .inf
    flow_cap_max: 2000
    lifetime: 25
    cost_flow_cap:
      data: 15
      index: monetary
      dims: costs
    cost_flow_in:
      data: 0.1 # 10p/kWh electricity price #ppt
      index: monetary
      dims: costs

  supply_gas:
    name: "Natural gas import"
    base_tech: supply
    inherit: interest_rate_setter
    carrier_out: gas
    source_use_max: .inf
    flow_cap_max: 2000
    lifetime: 25
    cost_flow_cap:
      data: 1
      index: monetary
      dims: costs
    cost_flow_in:
      data: 0.025 # 2.5p/kWh gas price #ppt
      index: monetary
      dims: costs

  supply_grid_heat:
    name: "Heat from the grid"
    base_tech: supply
    carrier_out: heat
    flow_cap_max: 2000000000
    cost_flow_in:
      data: 0.05
      index: monetary
      dims: costs

  demand_electricity:
    name: "Electrical demand"
    base_tech: demand
    carrier_in: electricity

  demand_heat:
    name: "Heat demand"
    base_tech: demand
    carrier_in: heat
"""
parsed_yaml = yaml.safe_load(yaml_data)


class ModelBase(BaseModel):
    model: Optional[Dict] = Field(
        ..., description="JSON describing the technologies", example=parsed_yaml
    )


class ModelCreate(ModelBase):
    """A model representing a calliope model to be created."""

    pass


class ModelUpdate(ModelBase):
    """A model representing a calliope model to be updated."""

    pass


class ModelInDBBase(ModelBase):
    id: Optional[int] = Field(
        None, description="Unique identifier for each calliope model entry"
    )
    ref_created_by: Optional[int] = Field(
        None, description="Reference to the user who created the calliope model entry"
    )

    class Config:
        from_attributes = True


class Model(ModelInDBBase):
    """A model representing a calliope model in the database."""

    pass
```

## forest_ensys/schemas/process_electricity.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field
from datetime import datetime


class ProcessElectricityBase(BaseModel):
    timestamp: datetime = Field(
        ...,
        description="timestamp for every entry",
    )
    power_demand: float = Field(..., description="power value in watt.", example="5000")


class ProcessElectricityCreate(ProcessElectricityBase):
    pass


class ProcessElectricityUpdate(ProcessElectricityBase):
    pass


class ProcessElectricityInDBBase(ProcessElectricityBase):
    id: int = Field(
        ...,
        description="id for every entry",
    )
    ref_created_by: int = Field(
        ...,
        description="ref to user for every entry",
    )

    class Config:
        from_attributes = True


class ProcessElectricity(ProcessElectricityInDBBase):
    pass
```

## forest_ensys/schemas/process_heat.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class ProcessHeatBase(BaseModel):
    """
    Base class for ProcessHeat
    """

    timestamp: datetime = Field(None, description="timestamp for every entry")
    heat_demand: float = Field(None, description="power value in watt.", example=100.0)
    # temperature: float = Field(None,
    #                             description="needed temperature for the process in C",
    #                             example=20.0)
    # pressure: float = Field(None,
    #                         description="needed pressure for the process in Pascal",
    #                         example=300000.0)
    # mass_flow: float = Field(None,
    #                          description="needed mass flow for the process in kg/s",
    #                          example=100.0)


class ProcessHeatCreate(ProcessHeatBase):
    """
    Create a ProcessHeat entry
    """

    pass


class ProcessHeatUpdate(ProcessHeatBase):
    """
    Update a ProcessHeat entry
    """

    pass


class ProcessHeatInDBBase(ProcessHeatBase):
    """
    ProcessHeat entry to return via API
    """

    id: Optional[int] = Field(
        None, description="id of the ProcessHeat entry", example=1
    )

    ref_created_by: Optional[int] = Field(
        None, description="if of who send this heat entry", example="1"
    )

    class Config:
        from_attributes = True


class ProcessHeat(ProcessHeatInDBBase):
    pass
```

## forest_ensys/schemas/token.py

```python
from typing import Optional

from pydantic import BaseModel


class Token(BaseModel):
    access_token: str
    token_type: str


class TokenPayload(BaseModel):
    sub: Optional[int] = None
```

## forest_ensys/schemas/user.py

```python
from typing import Optional

from pydantic import BaseModel


# Shared attributes
class UserBase(BaseModel):
    username: Optional[str] = None


# Attributes to receive via API on creation
class UserCreate(UserBase):
    username: str
    password: str


# Attributes to receive via API on update
class UserUpdate(UserBase):
    password: Optional[str] = None


class UserInDBBase(UserBase):
    id: Optional[int] = None

    class Config:
        from_attributes = True


# Additional properties to return via API
class User(UserInDBBase):
    pass


# Additional properties stored in DB
class UserInDB(UserInDBBase):
    hashed_password: str
```

## forest_ensys/schemas/weather.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class WeatherBase(BaseModel):
    timestamp: datetime = Field(
        None, description="timestamp for every entry", example="yyyy-mm-dd hh:mm:ss"
    )
    nuts_id: str = Field(None, description="nuts id", example="DE")
    temperature: float = Field(
        None, description="temperature in celsius.", example=25.0
    )
    humidity: float = Field(None, description="humidity in percentage.", example=50.0)
    ghi: float = Field(
        None, description="global horizontal irradiation in w/m2.", example=100.0
    )


class WeatherCreate(WeatherBase):
    pass


class WeatherUpdate(WeatherBase):
    pass


class WeatherInDBBase(WeatherBase):
    id: Optional[int] = None

    class Config:
        from_attributes = True


class Weather(WeatherInDBBase):
    pass
```

## forest_ensys/schemas/flexible_power.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class FlexiblePowerBase(BaseModel):
    timestamp: Optional[datetime] = Field(
        default=None,
        description="Date and time when the emission factors were updated.",
        example="2020-01-01 00:00:00",
    )
    optimization_case_name: Optional[str] = Field(
        default=None,
        description="The name of the optimization case.",
        example="Optimization Case 1",
    )
    electricity_used: Optional[float] = Field(
        default=None, description="The flexible power used.", example=0.0
    )


class FlexiblePowerCreate(FlexiblePowerBase):
    pass


class FlexiblePowerUpdate(FlexiblePowerBase):
    pass


class FlexiblePowerInDB(FlexiblePowerBase):
    pass

    class Config:
        from_attributes = True


class FlexiblePower(FlexiblePowerInDB):
    pass
```

## forest_ensys/schemas/optimization_results.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class OptimizationResult(BaseModel):
    total_energy_demand: Optional[float] = Field(
        default=None, description="The total energy demand in kWh.", example=0.0
    )
    time_from: Optional[datetime] = Field(
        default=None,
        description="The start time of the optimization.",
        example="2020-01-01 00:00:00",
    )
    time_to: Optional[datetime] = Field(
        default=None,
        description="The end time of the optimization.",
        example="2020-01-01 00:00:00",
    )
    network_fee_type: Optional[str] = Field(
        default=None,
        description="The type of network fee.",
        example="static" or "dynamic",
    )
    network_fee: Optional[float] = Field(
        default=None, description="The network fee in Euro.", example=0.0
    )
    electricity_used: Optional[float] = Field(
        default=None, description="The flexible power used in kWh.", example=0.0
    )
    gas_usage: Optional[float] = Field(
        default=None, description="The gas usage in kWh.", example=0.0
    )
    cost_savings: Optional[float] = Field(
        default=None, description="The cost savings in Euro.", example=0.0
    )
    emissions_savings: Optional[float] = Field(
        default=None, description="The emissions savings in tonnes CO2.", example=0.0
    )
    cost_gas_only: Optional[float] = Field(
        default=None, description="The cost as is in Euro for gas usage.", example=0.0
    )
    cost_with_electric_heating: Optional[float] = Field(
        default=None, description="The cost with electric heating in Euro.", example=0.0
    )
    emissions_gas_only: Optional[float] = Field(
        default=None,
        description="The emissions as is in tonnes CO2 for gas usage.",
        example=0.0,
    )
    emissions_with_electric_heating: Optional[float] = Field(
        default=None,
        description="The emissions with electric heating in tonnes CO2.",
        example=0.0,
    )
    full_load_hours: Optional[float] = Field(
        default=None, description="The full load hours.", example=0.0
    )
    full_load_hours_after_optimization: Optional[float] = Field(
        default=None, description="The full load hours after optimization.", example=0.0
    )
    mean_electricity_price_when_heating: Optional[float] = Field(
        default=None, description="The mean electricity price when heating.", example=0.0
    )
    electric_heating_in_low_price_windows_ratio: Optional[float] = Field(
        default=None,
        description="The ratio of electric heating in low price windows.",
        example=0.0,
    )

    # flexible_power_used: float
    # gas_usage: float
    # cost_savings: float
    # emissions_savings: float
```

## forest_ensys/schemas/prices.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class Prices(BaseModel):
    timestamp: Optional[datetime] = Field(
        default=None,
        description="The timestamp of the price.",
        example="2024-01-01T00:00:00",
    )
    price: Optional[float] = Field(
        default=None, description="The price in Euro.", example=0.0
    )
    source: Optional[str] = Field(
        default=None,
        description="The source of the price.",
        example="electricity_smard",
    )
```

## forest_ensys/__init__.py

```python

```

## forest_ensys/__main__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import uvicorn


def main():
    uvicorn.run(
        app="forest_ensys.app:app",
        host="0.0.0.0",
        port=8081,
        log_level="info",
        reload=True,
    )


if __name__ == "__main__":
    main()
```

## forest_ensys/app.py

```python
from fastapi import FastAPI, status, Request
from fastapi.responses import JSONResponse

from forest_ensys.api import api_router
from forest_ensys.core import settings
from forest_ensys.database import init_db

# Create FastAPI app and add all endpoints
app = FastAPI(
    title=settings.SERVER_NAME,
    root_path="/api",
)
app.include_router(api_router)


# Initialize database and create models
@app.on_event("startup")
def init_database():
    init_db.check_connection()
    init_db.create_all()


@app.exception_handler(Exception)
async def exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={"detail": str(exc)},
    )
```

## forest_ensys/repomix-output.md

````markdown
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-09-09 15:21:23

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
.ruff_cache
  0.6.7
    14794575240109317716
  .gitignore
  CACHEDIR.TAG
api
  endpoints
    __init__.py
    authentication.py
    emissions_data.py
    grid_data.py
    model.py
    process_data.py
    process_electricity_data.py
    process_heat_data.py
    users.py
    footprint_data.py
    flexibility.py
    simulation_input_data.py
    price_data.py
    result_data.py
  __init__.py
  api.py
  deps.py
core
  __init__.py
  calliope_model.py
  config.py
  security.py
  crawlers.py
  heat_pump.py
  optimization.py
  aas_helper.py
  timeseries_helpers.py
crud
  __init__.py
  data_parc.py
  emissions.py
  footprint.py
  grid.py
  model.py
  process_electricity.py
  process_heat.py
  user.py
  weather.py
  flexible_power.py
  prices.py
  simulation_input_data.py
  optimization_results.py
  base.py
database
  __init__.py
  base_class.py
  init_db.py
  session.py
model
  __init__.py
  data_parc.py
  emissions.py
  footprint.py
  grid.py
  model.py
  process_electricity.py
  process_heat.py
  user.py
  weather.py
  prices.py
  simulation_input_data.py
  optimization_results.py
  flexible_power.py
schemas
  __init__.py
  co2.py
  data_parc.py
  emissions.py
  footprint.py
  grid.py
  model.py
  process_electricity.py
  process_heat.py
  token.py
  user.py
  weather.py
  flexible_power.py
  optimization_results.py
  prices.py
__init__.py
__main__.py
app.py
```

# Repository Files


## api/endpoints/__init__.py

```python

```

## api/endpoints/authentication.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from datetime import timedelta

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session

from forest_ensys import schemas, crud, model
from forest_ensys.api import deps
from forest_ensys.core import security, settings

router = APIRouter()


@router.post("/login", response_model=schemas.Token)
def login(
    db: Session = Depends(deps.get_db), form_data: OAuth2PasswordRequestForm = Depends()
) -> schemas.Token:
    """
    OAuth2 compatible token login, get an access token for future requests
    """
    user = crud.user.authenticate(
        db, username=form_data.username, password=form_data.password
    )
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password!",
        )
    access_token_expires = timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    token = security.create_access_token(user.id, expires_delta=access_token_expires)
    return schemas.Token(access_token=token, token_type="bearer")


@router.post(
    "/register",
    response_model=schemas.User,
    responses={409: {"description": "User with same name already exists."}},
)
def register(
    request: schemas.UserCreate, db: Session = Depends(deps.get_db)
) -> schemas.User:
    """
    Create a new user
    """
    user = crud.user.get_by_username(db, username=request.username)
    if user:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="User with that username already exists!",
        )

    user = crud.user.create(db, obj_in=request)

    return user


@router.get("/test-token", response_model=schemas.User)
def test_token(
    current_user: model.User = Depends(deps.get_current_user),
) -> schemas.User:
    """
    Test access token
    """
    return current_user
```

## api/endpoints/emissions_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text, Optional

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core import crawlers

router = APIRouter()


@router.get("/", response_model=List[schemas.Emissions])
def get_all_emissions_data(
    db: Session = Depends(deps.get_db),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Emissions]:
    """
    Retrieve all emissions data
    """
    emissions_data = crud.emissions.get_multi(db=db, skip=skip, limit=limit)
    return emissions_data


@router.get(
    "/get-specific-emissions-factor", response_model=Optional[schemas.Emissions]
)
def get_emissions_data(
    db: Session = Depends(deps.get_db),
    zone_key: str = "DE",
    emission_type: str = "direct",
    production_mode: str = "gas",
) -> schemas.Emissions:
    """
    Retrieve emissions data for a specific zone and emission type and production mode
    """
    emissions_data = crud.emissions.get_specific_emissions(
        db=db,
        zone_key=zone_key,
        emission_type=emission_type,
        production_mode=production_mode,
    )
    return emissions_data


@router.post(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Emissions data updated successfully"}
                }
            },
        },
        502: {
            "description": "Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Could not retrieve emissions data. Server probably offline"
                    }
                }
            },
        },
        409: {
            "description": "Conflict Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Emissions data already exists. Please delete first if you want to update."
                    }
                }
            },
        },
    },
)
def update_emissions_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Retrieve the most recent emissions data
    """
    try:
        timeseries_data = crawlers.crawl_emissions_data()
    except Exception:
        raise HTTPException(
            status_code=502,
            detail="Could not retrieve emissions data. Server probably offline",
        )
    emission_objects = []
    for index, row in timeseries_data.iterrows():
        db_obj = model.Emissions(
            timestamp=row["datetime"],
            zone_key=row["zone_key"],
            emission_factor_type=row["emission_factor_type"],
            production_mode=row["production_mode"],
            value=row["value"],
            source=row["source"],
        )
        emission_objects.append(db_obj)

    try:
        crud.emissions.create_multi(db=db, obj_in=emission_objects)
        raise HTTPException(
            status_code=200, detail="Emissions data updated successfully"
        )
    except Exception:
        raise HTTPException(
            status_code=409,
            detail="Emissions data already exists. Please delete first if you want to update.",
        )


@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Emissions data table deleted successfully"}
                }
            },
        }
    },
)
def delete_emissions_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all emissions data
    """
    crud.emissions.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Emissions data table deleted successfully"
    )


@router.post("/get_specific_emissions_factor", response_model=Optional[schemas.Emissions])
def get_specific_emissions_factor(
    db: Session = Depends(deps.get_db),
    zone_key: str = "DE",
    emission_type: str = "direct",
    production_mode: str = "gas",
) -> schemas.Emissions:
    """
    Retrieve emissions data for a specific zone and emission type and production mode
    """
    return crud.emissions.get_specific_emissions(
        db=db,
        zone_key=zone_key,
        emission_type=emission_type,
        production_mode=production_mode,
    )
```

## api/endpoints/grid_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.api.endpoints import footprint_data, emissions_data

from forest_ensys.core import crawlers
import pandas as pd
from datetime import timedelta

router = APIRouter()

keys = {
    # 411: 'Prognostizierter Stromverbrauch',
    # 410: 'Realisierter Stromverbrauch',
    4169: "Preis",
    4066: "Biomasse",
    1226: "Wasserkraft",
    1225: "Wind Offshore",
    4067: "Wind Onshore",
    4068: "Photovoltaik",
    1228: "Sonstige Erneuerbare",
    1223: "Braunkohle",
    4071: "Erdgas",
    4070: "Pumpspeicher",
    1227: "Sonstige Konventionelle",
    4069: "Steinkohle",
    # 5097: 'Prognostizierte Erzeugung PV und Wind Day-Ahead'
}

grid_to_factors = {
    "Biomasse": "biomass",
    "Wasserkraft": "hydro",
    "Wind Offshore": "wind",
    "Wind Onshore": "wind",
    "Photovoltaik": "solar",
    "Braunkohle": "coal",
    "Steinkohle": "coal",
    "Erdgas": "gas",
    "Sonstige Konventionelle": "gas",
    "Sonstige Erneuerbare": "solar",
    "Pumpspeicher": "hydro",
}

@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Grid data table deleted successfully"}
                }
            },
        }
    },
)
def delete_grid_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all grid data
    """
    crud.grid.delete(db=db)
    crud.prices.delete(db=db)
    crud.footprint.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Grid data table deleted successfully"
    )

@router.get("/", response_model=List[schemas.Grid])
def get_all_grid_data(
    db: Session = Depends(deps.get_db),
    current: model.Grid = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Grid]:
    """
    Retrieve all grid data
    """
    grid_data = crud.grid.get_multi(db=db, skip=skip, limit=limit)
    return grid_data

@router.get("/update", responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Grid data updated successfully"}
                }
            },
        },
        502: {
            "description": "Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Could not retrieve emissions data. Server probably offline"
                    }
                }
            }
        },
        404: {
            "description": "Not Found",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Website did not return any data."
                    }
                }
            }
        }   
    }
)
def update_recent_grid_data(db: Session = Depends(deps.get_db)):
    """
    Retrieve the most recent grid data
    """
    while True:
        grid_data = []
        prices = []
        start_date = "12-31-2023 22:00:00"
        try:
            latest_emissions_factors = get_latest_emissions_factors(db=db)
        except Exception:
            raise HTTPException(
                status_code=502,
                detail="Could not retrieve emissions data. Server probably offline",
            )
        for commodity_id, commodity_name in keys.items():
            if commodity_id != 4169:
                latest = crud.grid.get_latest_for_commodity(db=db, commodity_id=commodity_id)
            else:
                latest = crud.prices.get_latest(db=db)
            latest_in_db = None
            try:
                latest = pd.to_datetime(latest.timestamp)
                latest_in_db = latest.tz_localize("UTC")
                print(f"The latest date in the database is {latest}")
                if latest.weekday() != 6 or (latest.hour < 21 and latest.minute == 45):
                    last_sunday = latest - timedelta(days=(latest.weekday() + 1) % 7)
                    print(
                        f"the latest date in the database is not a sunday after 22:00, taking last week sunday 22:00 as start date to fill the missing data: {latest} -> {last_sunday}"
                    )
                    latest = last_sunday
                if latest.hour == 21 and latest.minute == 45:
                    print(
                        "the latest date in the database is a sunday 21:45, taking this sunday 22:00 as start date"
                    )
                    latest = latest.replace(hour=22, minute=0, second=0, microsecond=0)
                    latest2 = latest.replace(hour=23, minute=0, second=0, microsecond=0)
                else:
                    print(
                        "the latest date in the database is a sunday 22:45, taking this sunday 23:00 as start date"
                    )
                    latest = latest.replace(hour=23, minute=0, second=0, microsecond=0)
                    latest2 = latest.replace(hour=22, minute=00, second=0, microsecond=0)
            except Exception as e:
                print(f"Using the default start date for commodity {commodity_id, e}")
                latest = pd.to_datetime(start_date)
                latest2 = latest.replace(hour=23, minute=0, second=0, microsecond=0)

            start_date_unix = int(latest.timestamp() * 1000)
            second_start_date_unix = int(latest2.timestamp() * 1000)
            data_for_commodity = crawlers.get_data_per_commodity(
                commodity_id, commodity_name, start_date_unix, second_start_date_unix
            )
            if data_for_commodity.empty:
                raise HTTPException(
                    status_code=404, detail="Could not get data for commodity {commodity_id}"
                )
            # if data_for_commodity is None:
            #    return HTTPException(status_code=404, detail="Could not reach website for crawling data")
            # delete timezone duplicate
            # https://stackoverflow.com/a/34297689
            data_for_commodity = data_for_commodity[
                ~data_for_commodity.index.duplicated(keep="first")
            ]
            if latest_in_db is not None:
                data_for_commodity = data_for_commodity[
                    data_for_commodity["timestamp"] > latest_in_db
                ]
            if commodity_id == 4169:
                for index, row in data_for_commodity.iterrows():
                    db_obj = model.Prices(
                        timestamp=row["timestamp"],
                        price=row["mwh"],
                        source="smard"
                    )
                    prices.append(db_obj)
            else:
                for index, row in data_for_commodity.iterrows():
                    db_obj = model.Grid(
                        timestamp=row["timestamp"],
                        commodity_id=row["commodity_id"],
                        commodity_name=row["commodity_name"],
                        mwh=row["mwh"],
                        co2=row["mwh"] * latest_emissions_factors[commodity_name] * 1000,
                    )
                    grid_data.append(db_obj)
        crud.grid.create_multi(db=db, obj_in=grid_data)
        crud.prices.create_multi(db=db, obj_in=prices)
        footprint_data.update_footprint_data(db)
    raise HTTPException(
            status_code=200, detail="Grid data updated successfully"
        )

def get_latest_emissions_factors(db: Session) -> dict:
    emissions = {}
    if not crud.emissions.get_multi(db=db):
        print("Emissions data seems empty, trying to crawl")
        emissions_data.update_emissions_data()

    for commodity_id, commodity_name in keys.items():
        if commodity_id == 4169:
            continue
        print(
            f"getting latest emission factor for {commodity_name} and {grid_to_factors[commodity_name]}"
        )
        specific_emission = crud.emissions.get_specific_emissions(
            db=db,
            zone_key="DE",
            emission_type="direct",
            production_mode=grid_to_factors[commodity_name],
        )
        print(specific_emission)
        emissions[commodity_name] = specific_emission.value
    return emissions
```

## api/endpoints/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List
from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core.calliope_model import generate_calliope_model
import pandas as pd
import json

router = APIRouter()


@router.get("/", response_model=List[schemas.Model])
def get_all_model_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Model]:
    """
    Retrieve all contraints data
    """
    model_data = crud.model.get_multi(db=db, user_id=current.id, skip=skip, limit=limit)
    return model_data


@router.post("/", response_model=schemas.Model)
def add_model_data(
    request: schemas.ModelCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Create a new model
    """
    return crud.model.create(db=db, obj_in=request, user_id=current.id)


@router.get("/{model_id}/optimize")
def optimize_model_by_id(
    model_id: int,
    start_date: datetime = None,
    end_date: datetime = None,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> str:
    """
    Optimize the model by id
    """
    model = crud.model.get(db=db, id=model_id)
    electricity_data = crud.process_electricity.get_multi_by_date_range(
        db=db, user_id=current.id, start_date=start_date, end_date=end_date
    )
    heat_data = crud.process_heat.get_multi_by_date_range(
        db=db, user_id=current.id, start_date=start_date, end_date=end_date
    )
    if electricity_data is None or heat_data is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Electricity data or heat data not found for model {model_id}!",
        )   
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail=f"Model {model_id} not found!"
        )
    calliope_model = generate_calliope_model(
        model.model, electricity_data, heat_data
    )
    calliope_model.build()
    calliope_model.solve()
    # results to html
    calliope_model.to_csv("results.csv")
    return json.dumps(calliope_model.results["cost"][0].to_dict())


@router.get("/{model_name}/optimize_by_name")
def optimize_model_by_name(
    model_name: str,
    start_date: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> str:
    """
    Optimize the model by name
    """
    model = crud.model.get_by_name(db=db, name=model_name)
    electricity_data = crud.process_electricity.get_from_start_date(
        db=db, user_id=current.id, start_date=start_date
    )
    electricity_data_df = pd.read_sql(
        electricity_data.statement, electricity_data.session.connection()
    )
    heat_data = crud.process_heat.get_from_start_date(
        db=db, user_id=current.id, start_date=start_date
    )
    heat_data_df = pd.read_sql(heat_data.statement, heat_data.session.connection())
    if electricity_data_df.empty or heat_data_df.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Electricity data or heat data not found for model {model_name}!",
        )
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Model {model_name} not found!",
        )
    calliope_model = generate_calliope_model(
        model.model, electricity_data_df, heat_data_df
    )
    calliope_model.run()
    return json.dumps(calliope_model.results["cost"][0].to_dict(), default=str)


@router.delete("/{model_name}", response_model=schemas.Model)
def delete_model_by_name(
    model_name: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Delete a model by name
    """
    model = crud.model.get_by_name(db=db, name=model_name)
    if model is None:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Model {model_name} not found!",
        )
    return crud.model.remove(db=db, id=model.id)
```

## api/endpoints/process_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()

@router.post("/", response_model=schemas.DataParc)
def add_process_data(
    request: schemas.DataParcCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Create a new process data
    """
    return crud.data_parc.create(db=db, obj_in=request, user_id=current.id)

@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {"message": "Process data table deleted successfully"}
                }
            }
        },
        502: {
            "description": "Server Error",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Could not retrieve emissions data. Server probably offline"
                    }
                }
            }
        }
    },
)
def delete_process_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all process data
    """
    crud.data_parc.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Process data table deleted successfully"
    )
```

## api/endpoints/process_electricity_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()


@router.get("/", response_model=List[schemas.ProcessElectricity])
def get_all_process_electricity_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.ProcessElectricity]:
    """
    Retrieve all electricity data
    """
    process_electricity_data = crud.process_electricity.get_multi(
        db=db, user_id=current.id, skip=skip, limit=limit
    )
    return process_electricity_data


@router.post("/", response_model=schemas.ProcessElectricity)
def add_process_electricity_data(
    request: schemas.ProcessElectricityCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
):
    """
    Create a new process_electricity
    """
    return crud.process_electricity.create(db=db, obj_in=request, user_id=current.id)
```

## api/endpoints/process_heat_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()


@router.get("/", response_model=List[schemas.ProcessHeat])
def get_all_process_heat_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[model.ProcessHeat]:
    """
    Retrieve all heat data
    """
    process_heat_data = crud.process_heat.get_multi(
        db=db, user_id=current.id, skip=skip, limit=limit
    )
    return process_heat_data


@router.post("/", response_model=schemas.ProcessHeat)
def add_process_heat_data(
    request: schemas.ProcessHeatCreate,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> model.ProcessHeat:
    """
    Create a new process heat data
    """
    return crud.process_heat.create(db=db, obj_in=request, user_id=current.id)
```

## api/endpoints/users.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

from forest_ensys import crud, model, schemas
from forest_ensys.api import deps

router = APIRouter()


@router.get("/", response_model=List[schemas.User])
def get_all_users(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.User]:
    """
    Retrieve all users.
    """
    users = crud.user.get_multi(db=db, skip=skip, limit=limit)
    return users
```

## api/endpoints/footprint_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List, Text
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from datetime import datetime
from pyomo.environ import ConcreteModel, Var, Objective, SolverFactory, NonNegativeReals, minimize, Constraint

router = APIRouter()

grid_to_factors = {
    "Biomasse": "biomass",
    "Wasserkraft": "hydro",
    "Wind Offshore": "wind",
    "Wind Onshore": "wind",
    "Photovoltaik": "solar",
    "Braunkohle": "coal",
    "Steinkohle": "coal",
    "Erdgas": "gas",
    "Sonstige Konventionelle": "gas",
    "Sonstige Erneuerbare": "solar",
    "Pumpspeicher": "hydro",
}
@router.delete(
    "/",
    responses={
        200: {
            "description": "Successful Response",
            "content": {
                "application/json": {
                    "example": {
                        "message": "Footprint data table deleted successfully"
                    }
                }
            },
        }
    },
)
def delete_footprint_data(db: Session = Depends(deps.get_db)) -> Text:
    """
    Delete all footprint data
    """
    crud.footprint.delete(db=db)
    raise HTTPException(
        status_code=200, detail="Footprint data table deleted successfully"
    )

def update_footprint_data(db: Session = Depends(deps.get_db)):
    grid = crud.grid.get_average_co2_by_commodity(db=db)
    footprint_data = []
    for row in grid:
        # this means we have missing data for this 15 mins and the co2 is zero
        if row[1] == 0:
            continue
        footprint_data.append(
            model.Footprint(
                timestamp=row[0],
                co2=row[1] / (row[2] * 1000),
            )
        )
    crud.footprint.create_multi(db=db, obj_in=footprint_data)

@router.get("/", response_model=List[schemas.Footprint])
def get_all_footprint_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Footprint]:
    """
    Retrieve all footprint data
    """
    return crud.footprint.get_multi(db=db, skip=skip, limit=limit)

@router.get("/latest", response_model=schemas.Footprint)
def get_latest_footprint_data(db: Session = Depends(deps.get_db)) -> schemas.Footprint:
    """
    Retrieve the latest footprint data
    """
    return crud.footprint.get_latest(db=db)
```

## api/endpoints/flexibility.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Text
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core.timeseries_hepers import (
    check_granularity_and_merge,
    calculate_dynamic_network_fee,
)
from forest_ensys.core.optimization import optimize_dryers as optimize
from forest_ensys.core.aas_helper import get_data_from_aas
from datetime import datetime

import pandas as pd
import numpy as np

router = APIRouter()


@router.delete(
    "/",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "message": "Flexibility data table deleted successfully",
                    }
                }
            }
        }
    },
)
def delete_flexibility_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> Text:
    """
    Delete all flexibility data
    """
    crud.flexible_power.delete(db=db, user_id=current.id)
    crud.optimization_results.delete(db=db, user_id=current.id)
    raise HTTPException(
        status_code=200, detail="Flexibility data table deleted successfully"
    )


@router.delete(
    "/{optimization_case_name}",
    responses={200: {"description": "Optimization case deleted successfully"}},
)
def delete_optimization_case(
    optimization_case_name: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> Text:
    """
    Delete a specific optimization case
    """
    crud.flexible_power.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name=optimization_case_name
    )
    crud.optimization_results.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name=optimization_case_name
    )
    raise HTTPException(
        status_code=200, detail="Optimization case deleted successfully"
    )


@router.post(
    "/optimize_flexibility",
    response_model=schemas.OptimizationResult,
    responses={404: {"description": "No data found for the given date range"}},
)
def optimize_flexibility(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    start_date: datetime = "2024-10-01",
    end_date: datetime = "2024-10-31",
    flexible_power: float = 8000,  # kW
    electricity_network_fee: float = 10,  # cost per MWh
    gas_emissions_factor: float = 204,  # g/kWh
    cost_per_mwh_gas: float = 60,  # cost per MWh
    co2_price: float = 55,  # cost per ton of CO2
) -> schemas.OptimizationResult:
    """
    Binary decision problem to optimize the use of electric heating
    """
    crud.flexible_power.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    crud.optimization_results.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    # Retrieve data from database
    footprint_data = crud.footprint.get_multi_by_date_range(
        db=db, start_date=start_date, end_date=end_date
    )
    heat_demand = crud.data_parc.get_multi_by_date_range(
        db=db, user_id=current.id, start_date=start_date, end_date=end_date
    )
    price_data = crud.prices.get_multi_by_date_range_and_source(
        db=db,
        start_date=start_date,
        end_date=end_date,
        source="smard",
    )

    # Merge dataframes and check granularity
    merged_data = check_granularity_and_merge(
        footprint_data, heat_demand[["timestamp", "value"]], method="sum"
    )
    merged_data = check_granularity_and_merge(
        merged_data,
        price_data[["timestamp", "price"]].rename(
            columns={"price": "electricity_price"}
        ),
    )
    merged_data["electricity_price"] = (
        merged_data["electricity_price"] + electricity_network_fee
    )

    time_interval_hours = merged_data["timestamp"].diff().min().total_seconds() / 3600

    # Decision based on electrical power (kW) at each timestamp
    use_flexible_power = merged_data["co2"] < gas_emissions_factor
    total_energy_demand = merged_data["value"].sum()
    # Integrate flexible power used and gas usage over time (kWh)
    electricity_used = np.where(
        use_flexible_power, flexible_power * time_interval_hours, 0
    ).sum()
    gas_usage = total_energy_demand - electricity_used

    # Flexible power time series (kWh per interval)
    flexible_power_time_series = pd.DataFrame(
        {
            "timestamp": merged_data["timestamp"],
            "electricity_used": np.where(use_flexible_power, flexible_power, 0),
        }
    )

    emissions_gas_only = merged_data["value"].sum() * gas_emissions_factor * 1e-6

    emissions_savings = (
        np.where(
            use_flexible_power,
            (gas_emissions_factor - merged_data["co2"])
            * flexible_power
            * time_interval_hours,
            0,
        ).sum()
    ) * 1e-6  # convert to tonnes

    emissions_with_electric_heating = emissions_gas_only - emissions_savings

    cost_gas_only = (
        total_energy_demand * 1e-3 * cost_per_mwh_gas
        + emissions_gas_only * 1e-6 * co2_price
    )

    cost_savings = emissions_savings * co2_price + (
        np.where(
            use_flexible_power,
            (cost_per_mwh_gas - merged_data["electricity_price"])
            * flexible_power
            * time_interval_hours
            * 1e-3,
            0,
        ).sum()
    )

    cost_with_electric_heating = cost_gas_only - cost_savings

    flexible_power = pd.DataFrame(
        {
            "timestamp": flexible_power_time_series["timestamp"],
            "electricity_used": flexible_power_time_series["electricity_used"],
            "low_price_window": 0,
            "optimization_case_name": "binary_decision_problem",
            "ref_created_by": current.id,
        }
    )

    crud.flexible_power.create_multi(
        db=db, obj_in=flexible_power.to_dict(orient="records")
    )

    optimization_results = {
        "name": "binary_decision_problem",
        "ref_created_by": current.id,
        "time_from": start_date,
        "time_to": end_date,
        "network_fee_type": "static",
        "network_fee": 0.0,
        "total_energy_demand": round(total_energy_demand, 2),
        "electricity_used": round(electricity_used, 2),
        "gas_usage": round(gas_usage, 2),
        "cost_savings": round(cost_savings, 2),
        "emissions_savings": round(emissions_savings, 2),
        "cost_gas_only": round(cost_gas_only, 2),
        "cost_with_electric_heating": round(cost_with_electric_heating, 2),
        "emissions_gas_only": round(emissions_gas_only, 2),
        "emissions_with_electric_heating": round(emissions_with_electric_heating, 2),
        "full_load_hours": 0,
        "full_load_hours_after_optimization": 0,
        "mean_electricity_price_when_heating": (
            merged_data["electricity_price"][use_flexible_power].mean()
        ),
        "electric_heating_in_low_price_windows_ratio": 0,
    }

    return crud.optimization_results.create(db=db, obj_in=optimization_results)


@router.post(
    "/optimize_flexibility_aas",
    response_model=schemas.OptimizationResult,
    responses={404: {"description": "No data found for the given date range"}},
)
def optimize_flexibility_aas_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
) -> schemas.OptimizationResult:
    """
    Binary decision problem to optimize the use of electric heating
    """
    parameters = get_data_from_aas()
    crud.flexible_power.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    flexible_power = int(parameters["powerMax"])
    electricity_network_fee = int(parameters["electricityNetworkFee"])
    gas_emissions_factor = 204
    cost_per_mwh_gas = int(parameters["gasPrice"])
    co2_price = int(parameters["co2Price"])
    crud.optimization_results.delete_by_user_id_and_optimization_case_name(
        db=db, user_id=current.id, optimization_case_name="binary_decision_problem"
    )
    # Retrieve data from database
    footprint_data = crud.footprint.get_multi_by_date_range(
        db=db, start_date=parameters["from"], end_date=parameters["until"]
    )
    heat_demand = crud.data_parc.get_multi_by_date_range(
        db=db,
        user_id=current.id,
        start_date=parameters["from"],
        end_date=parameters["until"],
    )
    price_data = crud.prices.get_multi_by_date_range_and_source(
        db=db,
        start_date=parameters["from"],
        end_date=parameters["until"],
        source="smard",
    )

    # Merge dataframes and check granularity
    merged_data = check_granularity_and_merge(
        footprint_data, heat_demand[["timestamp", "value"]], method="sum"
    )
    merged_data = check_granularity_and_merge(
        merged_data,
        price_data[["timestamp", "price"]].rename(
            columns={"price": "electricity_price"}
        ),
    )
    merged_data["electricity_price"] = (
        merged_data["electricity_price"] + electricity_network_fee
    )

    time_interval_hours = merged_data["timestamp"].diff().min().total_seconds() / 3600

    # Decision based on electrical power (kW) at each timestamp
    use_flexible_power = merged_data["co2"] < gas_emissions_factor
    total_energy_demand = merged_data["value"].sum()
    # Integrate flexible power used and gas usage over time (kWh)
    electricity_used = np.where(
        use_flexible_power, flexible_power * time_interval_hours, 0
    ).sum()
    gas_usage = total_energy_demand - electricity_used

    # Flexible power time series (kWh per interval)
    flexible_power_time_series = pd.DataFrame(
        {
            "timestamp": merged_data["timestamp"],
            "electricity_used": np.where(use_flexible_power, flexible_power, 0),
        }
    )

    emissions_gas_only = merged_data["value"].sum() * gas_emissions_factor * 1e-6

    emissions_savings = (
        np.where(
            use_flexible_power,
            (gas_emissions_factor - merged_data["co2"])
            * flexible_power
            * time_interval_hours,
            0,
        ).sum()
    ) * 1e-6  # convert to tonnes

    emissions_with_electric_heating = emissions_gas_only - emissions_savings

    cost_gas_only = (
        total_energy_demand * 1e-3 * cost_per_mwh_gas
        + emissions_gas_only * 1e-6 * co2_price
    )

    cost_savings = emissions_savings * co2_price + (
        np.where(
            use_flexible_power,
            (cost_per_mwh_gas - merged_data["electricity_price"])
            * flexible_power
            * time_interval_hours
            * 1e-3,
            0,
        ).sum()
    )

    cost_with_electric_heating = cost_gas_only - cost_savings

    flexible_power = pd.DataFrame(
        {
            "timestamp": flexible_power_time_series["timestamp"],
            "electricity_used": flexible_power_time_series["electricity_used"],
            "low_price_window": 0,
            "optimization_case_name": "binary_decision_problem",
            "ref_created_by": current.id,
        }
    )

    crud.flexible_power.create_multi(
        db=db, obj_in=flexible_power.to_dict(orient="records")
    )

    optimization_results = {
        "name": "binary_decision_problem",
        "ref_created_by": current.id,
        "time_from": parameters["from"],
        "time_to": parameters["until"],
        "network_fee_type": "static",
        "network_fee": 0.0,
        "total_energy_demand": round(total_energy_demand, 2),
        "electricity_used": round(electricity_used, 2),
        "gas_usage": round(gas_usage, 2),
        "cost_savings": round(cost_savings, 2),
        "emissions_savings": round(emissions_savings, 2),
        "cost_gas_only": round(cost_gas_only, 2),
        "cost_with_electric_heating": round(cost_with_electric_heating, 2),
        "emissions_gas_only": round(emissions_gas_only, 2),
        "emissions_with_electric_heating": round(emissions_with_electric_heating, 2),
        "full_load_hours": 0,
        "full_load_hours_after_optimization": 0,
        "mean_electricity_price_when_heating": (
            merged_data["electricity_price"][use_flexible_power].mean()
        ),
        "electric_heating_in_low_price_windows_ratio": 0,
    }

    return crud.optimization_results.create(db=db, obj_in=optimization_results)


@router.post(
    "/optimize_dryers",
    response_model=schemas.OptimizationResult,
    responses={
        404: {"description": "No data found for the given date range"},
        400: {"description": "Invalid Network Type"},
        501: {"description": "Not implemented yet"},
        500: {"description": "Internal Server Error"},
        401: {"description": "Unauthorized"},
    },
)
def optimize_dryers(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    start_date: datetime = Query(
        "2024-01-01", description="Start date for the optimization period."
    ),
    end_date: datetime = Query(
        "2024-12-31", description="End date for the optimization period."
    ),
    optimization_case_name: str = Query(
        "optimization_case_1",
        description="Name of the optimization case. Used to store the results in the database.",
    ),
    electricity_price_data_source: str = Query(
        "smard",
        description="Data source for electricity prices. Smard are the market prices. You can upload PFCs or other forecasts under prices.",
    ),
    gas_price_data_source: str = Query(
        "gas_spot",
        description="Data source for gas prices. Constant uses a constant gas price for the whole period. You can upload PFCs or other forecasts under prices.",
    ),
    flexible_power: int = Query(
        8000, description="The power of the flexible load in kW."
    ),
    gas_emissions_factor: int = Query(
        204,
        description="The emissions factor for gas in g/kWh. It is around 200g/kWh for natural gas based on the Gross Calorific Value.",
    ),
    cost_per_mwh_gas: int = Query(
        60,
        description="The cost of gas in /MWh. 60/MWh was the average price for gas in Germany in 2024.",
    ),
    co2_price: int = Query(
        55,
        description="The price of CO2 in /Tonne CO2. 55/Tonne is the price for CO2 in Germany in 2025.",
    ),
    ramp_up_rate: int = Query(
        6000, description="The ramp up rate of the flexible load in kW/h."
    ),
    ramp_down_rate: int = Query(
        6000, description="The ramp down rate of the flexible load in kW/h."
    ),
    minimum_runtime: int = Query(
        2, description="The minimum runtime of the flexible load in quarter hours."
    ),
    network_fee: str = Query(
        "static",
        enum=["static", "dynamic"],
        description="Method for calculating network fees. Static adds a fixed value to the market electricity price, while dynamic calculates the network fee based on BK4-22-089",
    ),
    network_fee_value: float = Query(
        20.0,
        description="The network fee value in /MWh.",
    ),
    relative_network_fee_reduction: float = Query(
        0.8,
        description="The relative reduction of the network fee for flexible loads.",
    ),
    gas_network_fee: float = Query(
        4.0,
        description="The gas network fee in /MWh.",
    ),
    relative_network_fee_surcharge: float = Query(
        0.1,
        description="The relative surcharge of the network fee for inflexible loads.",
    ),
    window_size: int = Query(
        2,
        description="The window size for the dynamic network fee calculation in hours.",
    ),
) -> schemas.OptimizationResult:
    """
    Optimizes the use of electric heating using Pyomo.
    """
    if start_date > end_date:
        raise HTTPException(
            status_code=400, detail="Start date must be before end date"
        )
    if any(
        value < 0 or not isinstance(value, (int, float)) or value is None
        for value in [
            flexible_power,
            gas_emissions_factor,
            cost_per_mwh_gas,
            co2_price,
            ramp_up_rate,
            ramp_down_rate,
            minimum_runtime,
            network_fee_value,
        ]
    ):
        raise HTTPException(
            status_code=400, detail="All values must be positive numbers"
        )
    if (
        crud.optimization_results.get(
            db=db, user_id=current.id, optimization_case_name=optimization_case_name
        )
        is not None
    ):
        raise HTTPException(
            status_code=400,
            detail="Optimization case already exists. Please delete it first or change the name.",
        )
    footprint_data = crud.footprint.get_multi_by_date_range(
        db=db, start_date=start_date, end_date=end_date
    )
    if footprint_data is None:
        raise HTTPException(
            status_code=404, detail="No footprint data found for the given date range"
        )
    heat_demand = crud.simulation_input_data.get_multi_by_date_range_and_name(
        db=db,
        user_id=current.id,
        start_date=start_date,
        end_date=end_date,
        name="flexible_device_demand",
    )
    if heat_demand is None:
        raise HTTPException(
            status_code=404, detail="No heat demand data found for the given date range"
        )
    heat_demand.rename(columns={"value": heat_demand["name"][0]}, inplace=True)
    heat_demand.drop(columns=["name"], inplace=True)
    total_electricity_demand = (
        crud.simulation_input_data.get_multi_by_date_range_and_name(
            db=db,
            user_id=current.id,
            start_date=start_date,
            end_date=end_date,
            name="total_electricity_demand",
        )
    )
    if total_electricity_demand is None:
        raise HTTPException(
            status_code=404,
            detail="No total electricity demand data found for the given date range",
        )
    total_electricity_demand.rename(
        columns={"value": total_electricity_demand["name"][0]}, inplace=True
    )
    total_electricity_demand.drop(columns=["name"], inplace=True)
    electricity_price_data = crud.prices.get_multi_by_date_range_and_source(
        db=db,
        start_date=start_date,
        end_date=end_date,
        source=electricity_price_data_source,
    )
    if electricity_price_data is None:
        raise HTTPException(
            status_code=404,
            detail="No price data found for the given date range and source",
        )
    if gas_price_data_source != "constant":
        gas_price_data = crud.prices.get_multi_by_date_range_and_source(
            db=db,
            start_date=start_date,
            end_date=end_date,
            source=gas_price_data_source,
        )
        if gas_price_data is None:
            raise HTTPException(
                status_code=404,
                detail="No gas price data found for the given date range and source",
            )
        # as gas price data is only daily data, we interpolate it to 15 minutes
        gas_price_data = (
            gas_price_data.set_index("timestamp")
            .resample("15min")
            .ffill()
            .reset_index()
        )
    else:
        gas_price_data = pd.DataFrame(
            {
                "timestamp": pd.date_range(
                    start=start_date, end=end_date, freq="15min"
                ),
                "price": cost_per_mwh_gas,
                "source": "constant",
            }
        )

    # Merge dataframes and check granularity
    merged_data = check_granularity_and_merge(footprint_data, heat_demand, method="sum")
    merged_data = check_granularity_and_merge(
        merged_data, total_electricity_demand, method="sum"
    )
    merged_data = check_granularity_and_merge(
        merged_data,
        electricity_price_data[["timestamp", "price"]].rename(
            columns={"price": "electricity_price"}
        ),
    )  # TODO make this better
    merged_data = check_granularity_and_merge(
        merged_data,
        gas_price_data[["timestamp", "price"]].rename(columns={"price": "gas_price"}),
    )
    merged_data["gas_data_source"] = gas_price_data["source"]
    merged_data["electricity_data_source"] = electricity_price_data["source"]

    if merged_data.empty:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No data found for the given date range",
        )

    if merged_data["timestamp"][0] != pd.to_datetime(start_date).tz_localize(
        "UTC"
    ) or merged_data["timestamp"].iloc[-1] != pd.to_datetime(end_date).tz_localize(
        "UTC"
    ):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Data does not cover the entire date range",
        )
    # we can also make full load hours a parameter. Then we would not need the time series anymore.
    granularity_factor = 1 / (
        merged_data["timestamp"].diff().min().total_seconds() / 3600
    )
    full_load_hours = merged_data["total_electricity_demand"].sum() / (
        merged_data["total_electricity_demand"].max() * granularity_factor
    )

    if network_fee == "static":
        merged_data["electricity_price"] = merged_data[
            "electricity_price"
        ] + network_fee_value * (1 - relative_network_fee_reduction)
        merged_data["window_type"] = 0
    elif network_fee == "dynamic":
        merged_data = calculate_dynamic_network_fee(
            merged_data,
            network_fee_value,
            relative_network_fee_reduction,
            relative_network_fee_surcharge,
            window_size,
        )
    else:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid network fee type",
        )
    merged_data["gas_price"] = merged_data["gas_price"] + gas_network_fee

    # Parameters
    heat_demand_dict = merged_data[
        "flexible_device_demand"
    ].to_dict()  # Heat demand per interval (kW)
    co2_electricity_dict = merged_data[
        "co2"
    ].to_dict()  # Electricity CO2 intensity (g/kWh)
    price_electricity_dict = merged_data[
        "electricity_price"
    ].to_dict()  # Electricity price (/MWh)
    price_gas_dict = merged_data["gas_price"].to_dict()  # Gas price (/MWh)
    electricity_demand = merged_data["total_electricity_demand"].to_dict()
    window_type = merged_data["window_type"].to_dict()

    time_interval_hours = merged_data["timestamp"].diff().min().total_seconds() / 3600

    optimization_results = optimize(
        co2_data=co2_electricity_dict,
        heat_demand_data=heat_demand_dict,
        electricity_price_data=price_electricity_dict,
        electricity_demand=electricity_demand,
        window_type=window_type,
        electric_heating=flexible_power,
        gas_emissions_factor=gas_emissions_factor,
        gas_price_data=price_gas_dict,
        co2_price=co2_price,
        ramp_up_rate=ramp_up_rate,
        ramp_down_rate=ramp_down_rate,
        minimum_runtime=minimum_runtime,
        time_interval_hours=time_interval_hours,
    )

    merged_data["total_electricity_demand_with_flexible_power"] = np.where(
        merged_data["window_type"] != 1,
        merged_data["total_electricity_demand"]
        + (
            optimization_results["optimized_results_df"]["electric_power_used_kW"]
            * time_interval_hours
        ),
        merged_data["total_electricity_demand"],
    )
    full_load_hours_after_optimization = merged_data[
        "total_electricity_demand_with_flexible_power"
    ].sum() / (
        merged_data["total_electricity_demand_with_flexible_power"].max()
        * granularity_factor
    )
    print("Full Load Hours After Optimization: ", full_load_hours_after_optimization)

    electric_heating_in_low_price_windows = merged_data[
        merged_data["window_type"] == 1
    ]["total_electricity_demand_with_flexible_power"].sum()
    electric_heating_in_low_price_windows_ratio = (
        electric_heating_in_low_price_windows
        / (merged_data["total_electricity_demand_with_flexible_power"].sum())
    )
    mean_electricity_price_when_heating = np.nanmean(
        np.where(
            optimization_results["optimized_results_df"]["electric_power_used_kW"] > 0,
            merged_data["electricity_price"],
            np.nan,
        )
    )
    # Store optimized electric power usage in DB if needed:
    optimization_results["optimized_results_df"]["timestamp"] = merged_data["timestamp"]
    flexible_power = pd.DataFrame(
        {
            "timestamp": merged_data["timestamp"],
            "electricity_used": optimization_results["optimized_results_df"][
                "electric_power_used_kW"
            ],
            "low_price_window": merged_data["window_type"],
            "optimization_case_name": optimization_case_name,
            "ref_created_by": current.id,
        }
    )

    optimization_results["name"] = optimization_case_name
    optimization_results["time_from"] = start_date
    optimization_results["time_to"] = end_date
    optimization_results["network_fee_type"] = network_fee
    optimization_results["network_fee"] = network_fee_value
    optimization_results["full_load_hours"] = full_load_hours
    optimization_results["full_load_hours_after_optimization"] = (
        full_load_hours_after_optimization
    )
    optimization_results["mean_electricity_price_when_heating"] = round(
        mean_electricity_price_when_heating, 2
    )
    optimization_results["electric_heating_in_low_price_windows_ratio"] = round(
        electric_heating_in_low_price_windows_ratio, 2
    )
    try:
        crud.flexible_power.create_multi(
            db=db, obj_in=flexible_power.to_dict(orient="records")
        )
        return crud.optimization_results.create(
            db=db, obj_in=optimization_results, user_id=current.id
        )
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# @router.post("/optimize_flexible_power_size", response_model=schemas.OptimizationResult)
# def optimize_flexible_power_size(
#     db: Session = Depends(deps.get_db),
#     current: model.User = Depends(deps.get_current_user),
#     start_date: datetime = "2024-09-10",
#     end_date: datetime = "2024-09-28",
#     max_flexible_power: float = 100000,
#     flexible_power_cost: float = 0.00001,
#     gas_emissions_factor: float = 800,
#     gas_price: float = 0.1,
#     w_cost: float = 0.5,
#     w_emissions: float = 0.5,
# ) -> schemas.OptimizationResult:
#     # Validate weights
#     if not (0 <= w_cost <= 1) or not (0 <= w_emissions <= 1):
#         raise HTTPException(status_code=400, detail="Weights must be between 0 and 1.")

#     if abs(w_cost + w_emissions - 1) > 1e-6:
#         raise HTTPException(
#             status_code=400, detail="The sum of w_cost and w_emissions must equal 1."
#         )

#     # Retrieve data from database
#     footprint_data = crud.footprint.get_multi_by_date_range(
#         db=db, start_date=start_date, end_date=end_date
#     )
#     heat_demand = crud.data_parc.get_multi_by_date_range(
#         db=db, user_id=current.id, start_date=start_date, end_date=end_date
#     )
#     price_data = crud.prices.get_multi_by_date_range(
#         db=db, start_date=start_date, end_date=end_date
#     )

#     # Merge dataframes and check granularity
#     merged_data = check_granularity_and_merge(footprint_data, heat_demand)
#     merged_data = check_granularity_and_merge(merged_data, price_data)

#     # Extract relevant data from the dataframes
#     co2_emissions = merged_data["co2"].values.tolist()
#     heat_demand_values = merged_data["value"].values.tolist()
#     timestamps = merged_data["timestamp"].values

#     el_price = merged_data["price"].tolist()

#     # Create Pyomo model
#     model = ConcreteModel()

#     # Define sets (time steps)
#     model.T = RangeSet(0, len(timestamps) - 1)

#     # Parameters
#     model.co2_emissions = Param(
#         model.T, initialize={i: co2_emissions[i] for i in range(len(co2_emissions))}
#     )
#     model.heat_demand = Param(
#         model.T,
#         initialize={i: heat_demand_values[i] for i in range(len(heat_demand_values))},
#     )
#     model.el_price = Param(
#         model.T, initialize={i: el_price[i] for i in range(len(el_price))}
#     )

#     # Decision variable: Single flexible power variable for the entire time period
#     model.flexible_power = Var(within=NonNegativeReals, bounds=(0, max_flexible_power))
#     model.gas_power = Var(model.T, within=NonNegativeReals)

#     # Objective function with normalization
#     def objective_rule(model):
#         max_cost = max(el_price) * max_flexible_power * len(
#             heat_demand_values
#         ) + gas_price * sum(heat_demand_values)
#         max_emission = gas_emissions_factor * sum(heat_demand_values)

#         normalized_cost = sum(
#             (model.el_price[t] * model.flexible_power + gas_price * model.gas_power[t])
#             / max_cost
#             for t in model.T
#         )

#         normalized_emission = sum(
#             (
#                 gas_emissions_factor * model.gas_power[t]
#                 + model.co2_emissions[t] * model.flexible_power
#             )
#             / max_emission
#             for t in model.T
#         )

#         flexible_cost = flexible_power_cost * model.flexible_power

#         return (
#             w_cost * normalized_cost + w_emissions * normalized_emission + flexible_cost
#         )

#     model.objective = Objective(rule=objective_rule, sense=minimize)

#     # Constraints: Ensure heat demand is met at each time step
#     def heat_demand_rule(model, t):
#         return model.gas_power[t] + model.flexible_power >= model.heat_demand[t] * 0.99

#     model.heat_demand_constraint = Constraint(model.T, rule=heat_demand_rule)

#     # Solve the optimization problem
#     solver = SolverFactory("glpk")
#     results = solver.solve(model)

#     # Check solver status
#     if results.solver.termination_condition != TerminationCondition.optimal:
#         raise HTTPException(
#             status_code=500,
#             detail=f"Optimization failed with termination condition {results.solver.termination_condition}",
#         )

#     # Extract the optimal flexible power value
#     try:
#         optimal_flexible_power = value(model.flexible_power)

#         return schemas.OptimizationResult(
#             optimal_flexible_power=optimal_flexible_power,
#             detail="Optimization completed successfully.",
#         )

#     except Exception as e:
#         raise HTTPException(
#             status_code=500,
#             detail=f"Error extracting results from optimization: {str(e)}",
#         )
```

## api/endpoints/simulation_input_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from fastapi import APIRouter, Depends, File, UploadFile, HTTPException, status, Query
from fastapi.responses import JSONResponse
import pandas as pd
from sqlalchemy.orm import Session
import sqlalchemy

from forest_ensys import crud, model
from forest_ensys.api import deps
from forest_ensys.core.timeseries_hepers import ensure_consistent_granularity

router = APIRouter()


# we want a method that receives a excel file and uploads it to the database
@router.post(
    "/upload-heat-demand",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "message": "Data uploaded successfully",
                    }
                }
            }
        },
        400: {"description": "Invalid file format"},
        409: {"description": "Data already exists"},
        422: {"description": "Validation error"},
        500: {"description": "Internal server error"},
    },
)
async def upload_simulation_input_data(
    file: UploadFile = File(...),
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    name: str = Query(
        "flexibility_timeseries",
        enum=["flexible_device_demand", "total_electricity_demand"],
        description="The name of the simulation input data. 'flexible_device_demand' is the energy demand of the flexible device. 'total_electricity_demand' is the total electricity demand of the building.",
    ),
    delimiter: str = ";",
    skiprows: int = 3,
    DateTimeColumn: str = "DateTime",
    ValueColumn: str = "Value",
    unit: str = Query(
        "m/h",
        enum=["m/h", "kWh", "kW"],
        description="The unit of the heat demand. Can be 'm/h', 'kWh' or 'kW'",
    ),
    heating_value: float = 10.0,
    conversion_factor: float = 0.8,
):
    if unit.lower() not in ["m/h", "kw", "kwh", "m3/h"]:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Invalid unit. Use 'm/h', 'kWh' or 'kW'",
        )
    if (
        not file
        or not delimiter
        or skiprows is None
        or not DateTimeColumn
        or not ValueColumn
        or not unit
        or not heating_value
        or not conversion_factor
        or not name
    ):
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Missing required parameters",
        )

    try:
        df = pd.read_csv(
            file.file,
            skiprows=skiprows,
            delimiter=delimiter,
            usecols=[DateTimeColumn, ValueColumn],
        )
    except pd.errors.EmptyDataError:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "Empty/invalid CSV file")
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Processing failed: {str(e)}",
        )

    # crud.simulation_input_data.delete(db)

    df.rename(columns={DateTimeColumn: "timestamp", ValueColumn: "value"}, inplace=True)
    df, granularity = ensure_consistent_granularity(df)
    df["name"] = name
    if unit.lower() == "m/h" or unit.lower() == "m3/h":
        df["value"] = df["value"] * heating_value * conversion_factor
    elif unit.lower() == "kw":
        df["value"] = df["value"] * granularity
    crud.simulation_input_data.create_multi(db, obj_in=df.to_dict(orient="records"), ref_created_by=current.id)
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "success", "message": "Data uploaded successfully"},
    )
    
@router.delete(
    "/simulation_input_data",
    responses={
        200: {"description": "Data deleted successfully"},
        400: {"description": "Bad request"},
    },
)
def delete_simulation_input_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    name: str = Query(
        "flexibility_timeseries",
        enum=["flexible_device_demand", "total_electricity_demand"],
        description="The name of the simulation input data. 'flexible_device_demand' is the energy demand of the flexible device. 'total_electricity_demand' is the total electricity demand of the building.",
    ) 
):
    """
    Delete all simulation input data.
    """
    crud.simulation_input_data.delete_by_user_and_name(db, user_id=current.id, name=name)
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "success", "message": "Data deleted successfully"},
    )
```

## api/endpoints/price_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List

from fastapi import APIRouter, Depends, File, UploadFile, HTTPException, status, Query
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from forest_ensys import crud, model, schemas
from forest_ensys.api import deps
from forest_ensys.core.timeseries_hepers import ensure_consistent_granularity
import pandas as pd

router = APIRouter()


@router.get("/", response_model=List[schemas.Prices])
def get_all_price_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Prices]:
    """
    Retrieve all price data
    """
    price_data = crud.prices.get_multi(db=db, skip=skip, limit=limit)
    return price_data

@router.get("/{source}", response_model=List[schemas.Prices])
def get_price_data_by_source(
    source: str,
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    skip: int = 0,
    limit: int = 100,
) -> List[schemas.Prices]:
    """
    Retrieve price data by source
    """
    try:
        price_data = crud.prices.get_by_source(db=db, source=source, skip=skip, limit=limit)
        return price_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post(
    "/upload-price-data",
    responses={
        200: {
            "content": {
                "application/json": {
                    "example": {
                        "status": "success",
                        "message": "Data uploaded successfully",
                    }
                }
            }
        },
        400: {"description": "Invalid file format"},
        422: {"description": "Validation error"},
        500: {"description": "Internal server error"},
    },
)
async def upload_price_data(
    file: UploadFile = File(...),
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    delimiter: str = ";",
    skiprows: int = 3,
    DateTimeColumn: str = "timestamp",
    ValueColumn: str = "price",
    source: str = "greenPFC",
):
    if (
        not file
        or not delimiter
        or skiprows is None
        or not DateTimeColumn
        or not ValueColumn
        or not source
    ):
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
            detail="Missing required parameters",
        )
    # check if file is excel or csv
    if not file.filename.endswith((".csv", ".xls", ".xlsx")):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid file format. Only CSV and Excel files are allowed.",
        )
    try:
        if file.filename.endswith(".csv"):
            df = pd.read_csv(
                file.file,
                skiprows=skiprows,
                delimiter=delimiter,
                usecols=[DateTimeColumn, ValueColumn],
            )
        else:
            df = pd.read_excel(
                file.file, skiprows=skiprows, usecols=[DateTimeColumn, ValueColumn]
            )
    except pd.errors.EmptyDataError:
        raise HTTPException(status.HTTP_400_BAD_REQUEST, "Empty/invalid CSV file")
    except Exception as e:
        raise HTTPException(
            status.HTTP_500_INTERNAL_SERVER_ERROR, f"Error reading CSV file: {str(e)}"
        )
    df.rename(columns={DateTimeColumn: "timestamp", ValueColumn: "price"}, inplace=True)
    df, granularity = ensure_consistent_granularity(df, ignore_timezone=True)# TODO
    df["source"] = source
    crud.prices.create_multi(db, obj_in=df.to_dict(orient="records"))
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"status": "success", "message": "Data uploaded successfully"},
    )


@router.delete(
    "/prices",
    responses={
        200: {"description": "Data deleted successfully"},
        404: {"description": "Error deleting prices"},
    },
)
def delete_prices(
    db: Session = Depends(deps.get_db),
    source: str = Query(..., description="Source of the prices to delete"),
):
    try:
        crud.prices.delete(db, source=source)
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"status": "success", "message": "Data deleted successfully"},
        )
    except Exception as e:
        raise HTTPException(
            status.HTTP_404_NOT_FOUND, f"Error deleting prices: {str(e)}"
        )
```

## api/endpoints/result_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from fastapi import APIRouter, Depends, HTTPException, status, Query
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session

from forest_ensys import crud, model
from forest_ensys.api import deps

router = APIRouter()


@router.delete(
    "/",
    responses={
        200: {"description": "Data deleted successfully"},
        400: {"description": "Bad request"},
    },
)
def delete_result_data(
    db: Session = Depends(deps.get_db),
    current: model.User = Depends(deps.get_current_user),
    optimization_case_name: str = Query(
        ..., description="The name of the optimization case."
    ),
):
    try:
        crud.optimization_results.delete_by_user_id_and_optimization_case_name(
            db, user_id=current.id, optimization_case_name=optimization_case_name
        )
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"status": "success", "message": "Data deleted successfully"},
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"An error occurred: {str(e)}",
        )
```

## api/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains the routes exposed as REST-API.
"""

from .api import api_router
```

## api/api.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from fastapi import APIRouter

from .endpoints import (
    authentication,
    users,
    grid_data,
    process_electricity_data,
    process_heat_data,
    emissions_data,
    model,
    footprint_data,
    process_data,
    flexibility,
    simulation_input_data,
    price_data,
    result_data,
)

api_router = APIRouter()
api_router.include_router(
    authentication.router, prefix="/auth", tags=["Authentication"]
)
api_router.include_router(users.router, prefix="/users", tags=["Users"])
api_router.include_router(grid_data.router, prefix="/grid-data", tags=["Grid Data"])
api_router.include_router(
    emissions_data.router, prefix="/emissions-data", tags=["Emissions Data"]
)
api_router.include_router(
    footprint_data.router, prefix="/footprint-data", tags=["Footprint Data"]
)
api_router.include_router(
    flexibility.router, prefix="/flexibility", tags=["Flexibility"]
)
api_router.include_router(
    process_electricity_data.router,
    prefix="/electricity-data",
    tags=["Process Electricity Data"],
)
api_router.include_router(
    process_heat_data.router, prefix="/heat-data", tags=["Process Heat Data"]
)
api_router.include_router(model.router, prefix="/model", tags=["Model"])
api_router.include_router(process_data.router, prefix="/process-data", tags=["Process Data"])
api_router.include_router(simulation_input_data.router, prefix="/simulation-input-data", tags=["Simulation Input Data"])
api_router.include_router(price_data.router, prefix="/prices", tags=["Prices"])
api_router.include_router(result_data.router, prefix="/results", tags=["Results"])
```

## core/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
All functions for everything.
"""

from .config import settings
```

## core/calliope_model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import calliope
import pandas as pd
import yaml


def generate_calliope_model(
    model_dict: dict, electricity_data: pd.DataFrame, heat_data: pd.DataFrame
) -> calliope.Model:
    """
    Generates a calliope model from a dictionary.
    """
    model_def = calliope.AttrDict.from_yaml_string(str(model_dict))
    electricity_data.drop(columns=["id","ref_created_by"], inplace=True)    
    heat_data.drop(columns=["id","ref_created_by"], inplace=True)
    electricity_data.rename(columns={"timestamp": "timesteps"}, inplace=True)
    heat_data.rename(columns={"timestamp": "timesteps"}, inplace=True)
    electricity_data = electricity_data.set_index("timesteps")
    heat_data = heat_data.set_index("timesteps")
    location_names = list(model_dict.get("nodes").keys())
    # rename columns
    electricity_data.columns = pd.MultiIndex.from_tuples(
        [(node, "demand_electricity") for node in location_names]
    )
    heat_data.columns = pd.MultiIndex.from_tuples(
        [(node, "demand_heat") for node in location_names]
    )
    electricity_data.columns.names = ["nodes", "techs"]
    heat_data.columns.names = ["nodes", "techs"]
    df = pd.concat([electricity_data, heat_data], axis=1)
    df.index = df.index.strftime("%Y-%m-%d %H:%M:%S")
    return calliope.Model(
    model_def,
    data_source_dfs={"demand_data": df}
)
```

## core/security.py

```python
from datetime import datetime, timedelta
from typing import Any, Union

from jose import jwt
from passlib.context import CryptContext

from forest_ensys.core import settings

ALGORITHM = "HS256"

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")


def create_access_token(
    subject: Union[str, Any], expires_delta: timedelta = None
) -> str:
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(
            minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES
        )
    to_encode = {"exp": expire, "sub": str(subject)}
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt


def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)


def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)
```

## core/crawlers.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import requests
import pandas as pd
import json
from io import StringIO


def crawl_emissions_data() -> pd.DataFrame:
    """
    Crawl the emissions data from the Electricity Maps database.
    """
    spreadsheet_id = "1ukTAD_oQKZfq-FgLpbLo_bGOv-UPTaoM_WS316xlDcE"
    url = f"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export?format=csv"
    response = requests.get(url)
    try:
        response.raise_for_status()
    except Exception as e:
        print(f"Coult not crawl emissions data: {e}")
    df = pd.read_csv(StringIO(response.text))
    df["datetime"] = pd.to_datetime(df["datetime"])
    df = df.dropna()
    return df


def get_data_per_commodity(
    commodity_id, commodity_name, start_date_unix, second_start_date_unix=None
) -> pd.DataFrame:
    url = f"https://www.smard.de/app/chart_data/{commodity_id}/DE/{commodity_id}_DE_quarterhour_{start_date_unix}.json"
    print(url)
    response = requests.get(url)
    try:
        response.raise_for_status()
    except requests.exceptions.HTTPError as e:
        print(f"Could not get data for commodity: {commodity_id} {e} trying different start date")
        return get_data_per_commodity(commodity_id, commodity_name, second_start_date_unix)
    data = json.loads(response.text)
    timeseries = pd.DataFrame.from_dict(data["series"])
    if timeseries.empty:
        print(f"Received empty data for commodity: {commodity_id}")
    timeseries[0] = pd.to_datetime(timeseries[0], unit="ms", utc=True)
    timeseries.columns = ["timestamp", "mwh"]
    timeseries["commodity_id"] = commodity_id
    timeseries["commodity_name"] = commodity_name
    timeseries = timeseries.dropna(subset="mwh")

    return timeseries
```

## core/heat_pump.py

```python
import numpy as np
import pandas as pd
from tespy.networks import Network
from tespy.components import (Source, Sink, Condenser, Pump,
                            Compressor, HeatExchanger, Valve, CycleCloser)
from tespy.connections import Connection

def create_heatpump():
    """Properly constrained heat pump model"""
    nw = Network(
        T_unit='C',
        p_unit='bar',
        h_unit='kJ / kg',
        m_unit='kg / s',
        iterinfo=False
    )

    # Components with minimal required specs
    cc = CycleCloser('Cycle Closer')
    comp = Compressor('Compressor', eta_s=0.8)  # No pr here to avoid over-spec
    cond = Condenser('Condenser')
    valve = Valve('Expansion Valve')
    evap = HeatExchanger('Evaporator')
    pump = Pump('Pump', eta_s=0.75)
    
    # Sources/Sinks
    air_in = Source('Air Source')
    air_out = Sink('Air Sink')
    water_in = Source('Water Source')
    water_out = Sink('Water Sink')

    # Connections
    c_ref1 = Connection(cc, 'out1', comp, 'in1', label='Refrigerant 1')
    c_ref2 = Connection(comp, 'out1', cond, 'in1', label='Refrigerant 2')
    c_ref3 = Connection(cond, 'out1', valve, 'in1', label='Refrigerant 3')
    c_ref4 = Connection(valve, 'out1', evap, 'in1', label='Refrigerant 4')
    c_ref5 = Connection(evap, 'out1', cc, 'in1', label='Refrigerant 5')
    
    c_air1 = Connection(air_in, 'out1', evap, 'in2', label='Air 1')
    c_air2 = Connection(evap, 'out2', air_out, 'in1', label='Air 2')
    
    c_water1 = Connection(water_in, 'out1', pump, 'in1', label='Water 1')
    c_water2 = Connection(pump, 'out1', cond, 'in2', label='Water 2')
    c_water3 = Connection(cond, 'out2', water_out, 'in1', label='Water 3')
    
    nw.add_conns(c_ref1, c_ref2, c_ref3, c_ref4, c_ref5,
                c_air1, c_air2, c_water1, c_water2, c_water3)

    # Critical parameterization (exactly 13 specs total)
    # Refrigerant cycle (5 specs)
    c_ref1.set_attr(fluid={'R134a': 1}, T=5, x=1)  # Evaporator outlet (sat vapor)
    c_ref3.set_attr(T=40, x=0)  # Condenser outlet (sat liquid)
    
    # Air cycle (3 specs)
    c_air1.set_attr(fluid={'air': 1}, T=10, p=1)
    
    # Water cycle (5 specs)
    c_water1.set_attr(fluid={'water': 1}, T=40, p=1)
    c_water3.set_attr(T=45)  # Target temperature
    cond.set_attr(Q=-1e6)  # Heat demand (1 spec)
    
    return nw

def run_simulation():
    nw = create_heatpump()
    
    try:
        # Design calculation
        print("Running design calculation...")
        nw.solve('design')
        
        # Time series simulation
        hours = 24
        time_series = pd.DataFrame({
            'time': range(hours),
            'heat_demand': [1e6 * (0.5 + 0.5 * abs((i-12)/12)) for i in range(hours)],
            'air_temp': [10 + 5 * np.sin(2*np.pi*i/24) for i in range(hours)],
            'flow_temp': [45 + 5 * (i > 6 and i < 18) for i in range(hours)]
        })
        
        results = []
        for idx, timestep in time_series.iterrows():
            try:
                # Update only necessary parameters
                nw.get_comp('Condenser').set_attr(Q=-timestep['heat_demand'])
                nw.get_conn('Air 1').set_attr(T=timestep['air_temp'])
                nw.get_conn('Water 3').set_attr(T=timestep['flow_temp'])
                
                nw.solve('offdesign', design_path='design')
                
                results.append({
                    'time': timestep['time'],
                    'cop': abs(nw.get_comp('Condenser').Q.val) / nw.get_comp('Compressor').P.val,
                    'mass_flow': nw.get_conn('Water 3').m.val
                })
                
            except Exception as e:
                print(f"Hour {timestep['time']}: {str(e)}")
                results.append({'time': timestep['time'], 'cop': np.nan, 'mass_flow': np.nan})
        
        return pd.DataFrame(results)
    
    except Exception as e:
        print(f"Simulation failed: {str(e)}")
        return None

if __name__ == "__main__":
    results = run_simulation()
    if results is not None:
        print(results.head())
```

## core/optimization.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pyomo.environ import (
    ConcreteModel,
    Var,
    Objective,
    SolverFactory,
    NonNegativeReals,
    minimize,
    Constraint,
    RangeSet,
    Param,
    value,
    Binary,
)
import pandas as pd
import random


def optimize_dryers(
    co2_data: dict,
    heat_demand_data: dict,
    electricity_price_data: dict,
    electricity_demand: dict,
    window_type: dict,
    electric_heating: float,
    gas_emissions_factor: float,
    gas_price_data: dict,
    co2_price: float,
    ramp_up_rate: int = 1,
    ramp_down_rate: int = 1,
    minimum_runtime: int = 1,
    time_interval_hours: float = 1,
) -> dict:
    """
    Optimizes the use of flexible power for electric heating in a dryer system.
    Parameters:
    - co2_data: DataFrame containing the electricity footprint data.
    - heat_demand_data: DataFrame containing the heat demand data.
    - electricity_price_data: DataFrame containing the electricity price data.
    - electricity_demand: DataFrame containing the electricity demand data.
    - window_type: DataFrame containing the window type data.
    - electric_heating: Maximum flexible power available for electric heating (kW).
    - gas_emissions_factor: Emissions factor for gas heating (g/kWh).
    - gas_price_data: DataFrame containing the gas price data.
    - cost_per_mwh_gas: Cost of gas heating per MWh (/MWh).
    - co2_price: Price of CO2 emissions (/ton CO2).
    - ramp_up_rate: Maximum ramp-up rate for electric heating (kW/h).
    - ramp_down_rate: Maximum ramp-down rate for electric heating (kW/h).
    - minimum_runtime: Minimum runtime for electric heating (h).
    - time_interval_hours: Time interval for the optimization (h).
    Returns:
    - A dictionary containing the optimized results.
    """

    # check if heat_demand, co2_data and price_data have the same length
    if (
        len(heat_demand_data) != len(co2_data)
        or len(heat_demand_data) != len(electricity_price_data)
        or len(heat_demand_data) != len(electricity_demand)
    ):
        raise ValueError(
            "The length of heat_demand, co2_data and price_data must be the same."
        )

    # Initialize model
    model = ConcreteModel()

    num_periods = len(heat_demand_data)
    print(f"Number of periods: {num_periods}")
    print(f"Time interval: {time_interval_hours} hours")
    model.T = RangeSet(0, num_periods - 1, doc="Time periods (integer indices)")

    model.heat_demand = Param(model.T, initialize=heat_demand_data)
    model.co2_electricity = Param(model.T, initialize=co2_data)
    model.price_electricity = Param(model.T, initialize=electricity_price_data)
    model.price_gas = Param(model.T, initialize=gas_price_data)
    model.electricity_demand = Param(model.T, initialize=electricity_demand)
    model.window_type = Param(model.T, initialize=window_type)
    model.max_total_demand = Var(within=NonNegativeReals)

    model.flexible_power_max = electric_heating  # kW
    model.gas_emissions_factor = gas_emissions_factor  # g/kWh
    model.co2_price = co2_price  # /ton CO2
    model.ramp_up_rate = ramp_up_rate
    model.ramp_down_rate = ramp_down_rate

    # Decision variables: Electric heating power used at each interval (kW)
    model.electric_power_used = Var(model.T, domain=NonNegativeReals)

    # Binary variable to track if the heater is running
    model.heater_on = Var(model.T, domain=Binary)

    # Auxiliary binary variable to track when the heater starts
    model.heater_start = Var(model.T, domain=Binary)

    # Constraints

    def ramp_up_rule(m, t):
        if t > 0:
            return (
                m.electric_power_used[t] - m.electric_power_used[t - 1]
                <= m.ramp_up_rate * time_interval_hours
            )
        return Constraint.Skip

    def ramp_down_rule(m, t):
        if t > 0:
            return (
                m.electric_power_used[t - 1] - m.electric_power_used[t]
                <= m.ramp_down_rate * time_interval_hours
            )
        return Constraint.Skip

    model.ramp_up = Constraint(model.T, rule=ramp_up_rule)
    model.ramp_down = Constraint(model.T, rule=ramp_down_rule)

    @model.Constraint(model.T)
    def heat_balance(m, t):
        return m.electric_power_used[t] * time_interval_hours <= m.heat_demand[t]

    @model.Constraint(model.T)
    def minimum_runtime_constraint(m, t):
        required_intervals = int(minimum_runtime / time_interval_hours)
        if t <= num_periods - required_intervals:
            # If heater turns on at t, it must stay on for required_intervals
            return sum(
                m.heater_on[t + i] for i in range(required_intervals)
            ) >= required_intervals * (
                m.heater_on[t] - (m.heater_on[t - 1] if t > 0 else 0)
            )
        else:
            # Near end of schedule: if heater turns on, it must stay on
            return sum(m.heater_on[t + i] for i in range(num_periods - t)) >= (
                num_periods - t
            ) * (m.heater_on[t] - (m.heater_on[t - 1] if t > 0 else 0))

    def max_electric_heating_rule(m, t):
        return m.electric_power_used[t] <= m.flexible_power_max * m.heater_on[t]

    model.max_electric_heating_constraint = Constraint(
        model.T, rule=max_electric_heating_rule
    )

    def electric_power_zero_when_off_rule(m, t):
        return m.electric_power_used[t] >= 0 * m.heater_on[t]

    model.electric_power_zero_when_off_constraint = Constraint(
        model.T, rule=electric_power_zero_when_off_rule
    )

    def electric_power_minimum_when_on_rule(m, t):
        return m.electric_power_used[t] >= 100 * m.heater_on[t]

    model.electric_power_minimum_when_on_constraint = Constraint(
        model.T, rule=electric_power_minimum_when_on_rule
    )

    def effective_total_demand(m, t):
        if m.window_type[t] != 1:
            return m.electricity_demand[t] + (
                m.electric_power_used[t] * time_interval_hours
            )
        else:
            return m.electricity_demand[t]

    def max_total_demand_rule(m, t):
        return m.max_total_demand >= effective_total_demand(m, t)

    model.max_total_demand_constraint = Constraint(model.T, rule=max_total_demand_rule)

    def full_load_hours_constraint_rule(m):
        total_demand = sum(effective_total_demand(m, t) for t in m.T)
        return total_demand >= (0.8*time_interval_hours*num_periods) * m.max_total_demand * (1 / time_interval_hours) # 0.8 is the minimum full load hours which is 7000 for one year

    model.full_load_hours_constraint = Constraint(rule=full_load_hours_constraint_rule)

    def total_cost_rule(m):
        electricity_cost = sum(
            m.electric_power_used[t]
            * m.price_electricity[t]
            * time_interval_hours
            * 1e-3
            for t in m.T
        )

        gas_cost = sum(
            (m.heat_demand[t] - (m.electric_power_used[t] * time_interval_hours))
            * m.price_gas[t]
            * 1e-3
            for t in m.T
        )

        emissions_gas = sum(
            (m.heat_demand[t] - (m.electric_power_used[t] * time_interval_hours))
            * m.gas_emissions_factor
            for t in m.T
        )
        emissions_electricity = sum(
            m.electric_power_used[t] * m.co2_electricity[t] * time_interval_hours
            for t in m.T
        )
        co2_cost = (emissions_gas + emissions_electricity) * 1e-6 * m.co2_price

        return electricity_cost + gas_cost + co2_cost

    model.total_cost = Objective(rule=total_cost_rule, sense=minimize)

    solver = SolverFactory("gurobi")
    solver.options["MIPGap"] = 0.001
    solver.options["TimeLimit"] = 120
    solver.options["OutputFlag"] = 1
    solver.options["Threads"] = 4
    
    results = solver.solve(model)

    print(results.solver.termination_condition)
    print(value(model.total_cost))

    optimized_results_df = pd.DataFrame(
        {
            "heat_demand_kwh": heat_demand_data.values(),
            "electric_power_used_kW": [
                value(model.electric_power_used[t]) for t in model.T
            ],
            "co2_electricity": co2_data.values(),
            "gas_price": gas_price_data.values(),
        }
    )
    optimized_results_df["gas_power_used_kwh"] = optimized_results_df[
        "heat_demand_kwh"
    ] - (optimized_results_df["electric_power_used_kW"] * time_interval_hours)

    optimized_results_df["emissions_gas"] = (
        optimized_results_df["gas_power_used_kwh"] * gas_emissions_factor
    )

    optimized_results_df["emissions_electricity"] = (
        optimized_results_df["electric_power_used_kW"]
        * optimized_results_df["co2_electricity"]
        * time_interval_hours
    )

    optimized_results_df["heater_on"] = [value(model.heater_on[t]) for t in model.T]

    total_energy_demand_kWh = optimized_results_df["heat_demand_kwh"].sum()
    electricity_used_kWh = (
        optimized_results_df["electric_power_used_kW"].sum() * time_interval_hours
    )
    gas_usage_kWh = total_energy_demand_kWh - electricity_used_kWh

    emissions_gas_only_tonnes = total_energy_demand_kWh * gas_emissions_factor * 1e-6

    cost_optimized_euro = value(model.total_cost)

    emissions_optimized_tonnes = (
        optimized_results_df["emissions_gas"].sum()
        + optimized_results_df["emissions_electricity"].sum()
    ) * 1e-6  # Gramm zu Tonnen

    emissions_savings_tonnes = emissions_gas_only_tonnes - emissions_optimized_tonnes

    # Cost savings calculation
    cost_gas_only = (
        (optimized_results_df["heat_demand_kwh"] / 1000)
        * optimized_results_df["gas_price"]
    ).sum() + (emissions_gas_only_tonnes * co2_price)
    cost_savings_euro = cost_gas_only - cost_optimized_euro

    return {
        "optimized_results_df": round(optimized_results_df, 2),
        "total_energy_demand": round(total_energy_demand_kWh, 2),
        "electricity_used": round(electricity_used_kWh, 2),
        "gas_usage": round(gas_usage_kWh, 2),
        "cost_savings": round(cost_savings_euro, 2),
        "cost_gas_only": round(cost_gas_only, 2),
        "cost_with_electric_heating": round(cost_optimized_euro, 2),
        "emissions_savings": round(emissions_savings_tonnes, 2),
        "emissions_gas_only": round(emissions_gas_only_tonnes, 2),
        "emissions_with_electric_heating": round(emissions_optimized_tonnes, 2),
    }


def test_optimize_dryers():
    """
    Test the optimize_dryers function with synthetic data to ensure constraints work as expected.
    """

    # Synthetic data
    time_intervals = 12  # 24 hours (1-hour intervals)
    time_interval_hours = 0.25  # Time interval in hours, e.g., 0.25 for 15 minutes,
    co2_data = {
        t: random.randint(0, 500)
        for t in range(int(time_intervals / time_interval_hours))
    }
    heat_demand_data = {
        t: random.randint(1000, 5000)
        for t in range(int(time_intervals / time_interval_hours))
    }
    price_data = {
        t: random.randint(0, 100)
        for t in range(int(time_intervals / time_interval_hours))
    }

    electric_heating = 200  # Max capacity of electric heating (kW)
    gas_emissions_factor = 250  # Gas emissions factor (g/kWh)
    cost_per_mwh_gas = 45  # Cost of gas heating (/MWh)
    co2_price = 50  # Price of CO2 emissions (/ton)
    ramp_up_rate = 100  # Max ramp-up rate (kW/h)
    ramp_down_rate = 100  # Max ramp-down rate (kW/h)
    minimum_runtime = 3  # Minimum runtime in hours

    # Run the optimization
    results = optimize_dryers(
        co2_data=co2_data,
        heat_demand_data=heat_demand_data,
        electricity_price_data=price_data,
        electric_heating=electric_heating,
        gas_emissions_factor=gas_emissions_factor,
        gas_price_data=cost_per_mwh_gas,
        co2_price=co2_price,
        ramp_up_rate=ramp_up_rate,
        ramp_down_rate=ramp_down_rate,
        minimum_runtime=minimum_runtime,
        time_interval_hours=time_interval_hours,
    )

    optimized_results_df = results["optimized_results_df"]

    print("Optimization completed successfully.")

    # Validation checks
    violations = []

    # Check if electric power used exceeds max capacity or heat demand
    for t, row in optimized_results_df.iterrows():
        if row["electric_power_used_kW"] > electric_heating:
            violations.append(f"Time {t}: Electric power used exceeds max capacity.")
        if row["electric_power_used_kW"] > row["heat_demand_kwh"]:
            violations.append(f"Time {t}: Electric power used exceeds heat demand.")

    # Check if ramp-up and ramp-down rates are respected
    for t in range(1, len(optimized_results_df)):
        ramp_up_diff = (
            optimized_results_df.loc[t, "electric_power_used_kW"]
            - optimized_results_df.loc[t - 1, "electric_power_used_kW"]
        )
        if ramp_up_diff > ramp_up_rate:
            violations.append(f"Time {t}: Ramp-up rate exceeded ({ramp_up_diff} kW).")

        ramp_down_diff = (
            optimized_results_df.loc[t - 1, "electric_power_used_kW"]
            - optimized_results_df.loc[t, "electric_power_used_kW"]
        )
        if ramp_down_diff > ramp_down_rate:
            violations.append(
                f"Time {t}: Ramp-down rate exceeded ({ramp_down_diff} kW)."
            )

    # Check if minimum consecutive runtime is respected
    heater_on_series = optimized_results_df["heater_on"]

    consecutive_runtime_violations = []

    current_run_length = 0
    for t, heater_on in enumerate(heater_on_series):
        if heater_on == 1:
            current_run_length += time_interval_hours
        else:
            if current_run_length > 0 and current_run_length < minimum_runtime:
                consecutive_runtime_violations.append(
                    f"Heater turned off after only {current_run_length} hours at time {t}."
                )
            current_run_length = 0

    if current_run_length > 0 and current_run_length < minimum_runtime:
        consecutive_runtime_violations.append(
            f"Heater turned off after only {current_run_length} hours at end of schedule."
        )

    violations.extend(consecutive_runtime_violations)

    # add a violation if heater on is 1 and electric power used is 0
    for t, row in optimized_results_df.iterrows():
        if row["heater_on"] == 1 and row["electric_power_used_kW"] == 0:
            violations.append(f"Time {t}: Heater is on but electric power used is 0.")
        if row["heater_on"] == 0 and row["electric_power_used_kW"] > 0:
            violations.append(
                f"Time {t}: Heater is off but electric power used is greater than 0."
            )

    #    print(optimized_results_df["heater_start"])
    print(optimized_results_df["heater_on"])
    print(optimized_results_df["electric_power_used_kW"])

    # Report results
    if violations:
        print("The following constraints were violated:")
        for violation in violations:
            print(f"- {violation}")
        print("\nTest failed due to constraint violations.")
    else:
        print("All constraints are satisfied. Test passed.")


# Run the test
# test_optimize_dryers()
```

## core/aas_helper.py

```python
import requests
import json
from typing import Dict, Any, Optional, Generator, List

from aas_core3 import jsonization
from aas_core3.types import (
    SubmodelElementCollection,
    Property,
    Range,
    SubmodelElement,
)

needed_properties = [
    "powerMax",
    "regenerationDuration",
    "from",
    "until",
    "activationGradient",
    "deactivationGradient",
    "electricityNetworkFee",
    "co2Price",
    "gasPrice",
]

class ServerEasyv3:
    @staticmethod
    def submodels_server_url() -> str:
        return "https://forest.nowum.fh-aachen.de/aas-env/submodels"

    def send_request_helper(self, url: str) -> Optional[requests.Response]:
        try:
            response = requests.get(url, timeout=3)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            return None

    def get_submodel(self, submodel_id: str) -> Optional[Any]:
        url = f"{self.submodels_server_url()}/{submodel_id}"
        response = self.send_request_helper(url)
        if response is None:
            return None

        try:
            json_data = response.json()
            submodel = jsonization.submodel_from_jsonable(json_data)
            print("Deserialization successful:")
            print(submodel)
            return submodel
        except json.JSONDecodeError as e:
            print(f"JSON decode failed: {e}")
            return None
        except jsonization.DeserializationException as ex:
            print(f"Deserialization failed: {ex}")
            return None

def traverse_elements(element: SubmodelElement) -> Generator[Dict[str, Any], None, None]:
    if isinstance(element, SubmodelElementCollection):
        if hasattr(element, 'value'):
            for sub_element in element.value:
                yield from traverse_elements(sub_element)
    elif isinstance(element, (Property, Range)):
        if element.id_short in needed_properties:
            value = getattr(element, "value", None)
            min_value = getattr(element, "min", None)
            max_value = getattr(element, "max", None)
            kind_value = getattr(getattr(element, "kind", None), "value", None) if hasattr(element, "kind") else None

            qualifier_values = [q.value for q in element.qualifiers] if hasattr(element, "qualifiers") and element.qualifiers else []
            qualifier_types = [q.type for q in element.qualifiers] if hasattr(element, "qualifiers") and element.qualifiers else []
            if value is None and min_value is None and max_value is None:
                return
            yield {
                "idShort": element.id_short,
                "value": value if value is not None else max_value,
                "qualifier_values": qualifier_values,
                "qualifier_types": qualifier_types,
            }

def get_data_from_aas() -> Dict[str, Any]:
    server = ServerEasyv3()
    submodel_id = "aHR0cHM6Ly9hZG1pbi1zaGVsbC5pby9pZHRhL0VuZXJneUZsZXhpYmlsaXR5RGF0YU1vZGVsLzEvMC9FbmVyZ3lGbGV4aWJpbGl0eURhdGFNb2RlbA"
    submodel = server.get_submodel(submodel_id)

    return_dict = {}
    if submodel and hasattr(submodel, "submodel_elements") and submodel.submodel_elements:
        print("Submodel elements found.")
        for element in submodel.submodel_elements:
            for elem_data in traverse_elements(element):
                return_dict[elem_data["idShort"]] = elem_data["value"]
    return return_dict

if __name__ == "__main__":
    print(get_data_from_aas())
```

## core/timeseries_helpers.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import pandas as pd
from datetime import timedelta
import numpy as np


def ensure_consistent_granularity(
    df, method="mean", ignore_timezone=False
) -> (pd.DataFrame, float):
    """
    Ensures that the DataFrame has a consistent granularity.

    Parameters:
    ----------
    df : pd.DataFrame
        The DataFrame to check and resample.
    method : str, optional
        The method to use for resampling (default is "mean").

    Returns:
    -------
    a float representing the granularity of the dataframe
    """
    if ignore_timezone:
        try:
            df["timestamp"] = (
                df["timestamp"]
                .str.replace(r"([+-]\d{2}:?\d{0,2})$", "", regex=True)
                .str.strip()
            )
        except AttributeError:
            pass
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True)
    granularity = df["timestamp"].diff().min()
    df_resampled = (
        df.set_index("timestamp").resample(granularity).agg(method).reset_index()
    )
    return df_resampled, granularity.total_seconds() / 3600


def check_granularity_and_merge(df1, df2, method="mean"):
    """
    Checks the granularity of the two DataFrames and merges them.

    Parameters:
    ----------
    df1 : pd.DataFrame
        The first DataFrame.
    df2 : pd.DataFrame
        The second DataFrame.

    Returns:
    -------
    pd.DataFrame
        The merged DataFrame.
    """
    df1["timestamp"] = pd.to_datetime(df1["timestamp"], utc=True)
    df2["timestamp"] = pd.to_datetime(df2["timestamp"], utc=True)
    df1_granularity = df1["timestamp"].diff().min()
    df2_granularity = df2["timestamp"].diff().min()
    if df1_granularity < df2_granularity:
        df1_resampled = (
            df1.set_index("timestamp")
            .resample(df2_granularity)
            .agg(method)
            .reset_index()
        )
        df2_resampled = df2
    else:
        df2_resampled = (
            df2.set_index("timestamp")
            .resample(df1_granularity)
            .agg(method)
            .reset_index()
        )
        df1_resampled = df1
    return pd.merge(df1_resampled, df2_resampled, on="timestamp", how="inner")


def get_reference_day(date):
    weekday = date.weekday()
    if weekday < 5:  # Mon-Fri
        days_back = 1 if weekday > 0 else 3
        ref_date = date - timedelta(days=days_back)
    else:
        ref_date = date - timedelta(days=7)
    return ref_date.date()


def select_peaks_no_overlap(day_df, window_size, price_col, kind="min"):
    """
    Select up to two non-overlapping peak timestamps (min or max) between 6:00 and 21:59.
    Each window is [peak_time - 2h, peak_time + 2h), i.e. 16 quarters.
    Windows must not overlap.
    Returns a list of peak timestamps.
    """
    # Only consider 6:00 to 21:59 (so window fits in day)
    mask = (day_df["timestamp"].dt.hour >= 6) & (day_df["timestamp"].dt.hour <= 21)
    df = day_df[mask].copy()
    if df.empty:
        return []
    # Sort by price
    if kind == "min":
        sorted_df = df.sort_values(price_col, ascending=True)
    else:
        sorted_df = df.sort_values(price_col, ascending=False)
    used_intervals = set()
    peaks = []
    for _, row in sorted_df.iterrows():
        peak_time = row["timestamp"]
        # Build set of all 15-min timestamps in the 4h window
        window_start = peak_time - timedelta(hours=window_size, minutes=15)
        window_end = peak_time + timedelta(hours=window_size)
        window_intervals = set(pd.date_range(window_start, window_end, freq="15min"))
        # Check for overlap with already selected windows
        if not window_intervals & used_intervals:
            peaks.append(peak_time)
            used_intervals.update(window_intervals)
        if len(peaks) == 2:
            break
    return sorted(peaks)


def calculate_dynamic_network_fee(
    merged_data,
    network_fee_value,
    relative_network_fee_reduction,
    relative_network_fee_surcharge,
    window_size
):
    merged_data = merged_data.copy()
    merged_data["timestamp"] = pd.to_datetime(merged_data["timestamp"])
    merged_data = merged_data.sort_values("timestamp").reset_index(drop=True)
    merged_data["date"] = merged_data["timestamp"].dt.date

    # Step 1: For each day, find non-overlapping min/max peaks
    peak_info = {}
    for date, group in merged_data.groupby("date"):
        min_peaks = select_peaks_no_overlap(
            group, window_size, price_col="electricity_price", kind="min"
        )
        max_peaks = select_peaks_no_overlap(
            group, window_size, price_col="electricity_price", kind="max"
        )
        peak_info[date] = {"min_peaks": min_peaks, "max_peaks": max_peaks}

    # Step 2: For each day, apply the reference day's windows
    merged_data["in_low_window"] = False
    merged_data["in_high_window"] = False
    for date in merged_data["date"].unique():
        ref_date = get_reference_day(pd.Timestamp(date))
        if ref_date not in peak_info:
            continue
        min_peaks = peak_info[ref_date]["min_peaks"]
        max_peaks = peak_info[ref_date]["max_peaks"]
        # Low price windows
        for peak_time in min_peaks:
            window_start = peak_time - timedelta(hours=window_size, minutes=15)
            window_end = peak_time + timedelta(hours=window_size)
            mask = (
                (merged_data["date"] == date)
                & (merged_data["timestamp"].dt.time >= window_start.time())
                & (merged_data["timestamp"].dt.time < window_end.time())
            )
            merged_data.loc[mask, "in_low_window"] = True

        # High price windows
        for peak_time in max_peaks:
            window_start = peak_time - timedelta(hours=window_size, minutes=15)
            window_end = peak_time + timedelta(hours=window_size)
            mask = (
                (merged_data["date"] == date)
                & (merged_data["timestamp"].dt.time >= window_start.time())
                & (merged_data["timestamp"].dt.time < window_end.time())
            )
            merged_data.loc[mask, "in_high_window"] = True

    # Step 3: High price window takes precedence
    # merged_data['window_type'] = np.where(
    #     merged_data['in_high_window'], 'high',
    #     np.where(merged_data['in_low_window'], 'low', 'normal')
    # )
    # Step 3: Low price window takes precedence
    merged_data["window_type"] = np.where(
        merged_data["in_low_window"],
        1,
        np.where(merged_data["in_high_window"], 2, 0),
    )

    # Step 4: Calculate dynamic price
    merged_data["electricity_price"] = merged_data.apply(
        lambda row: row["electricity_price"]
        + (
            network_fee_value * (1 - relative_network_fee_reduction)
            if row["window_type"] == 1
            else network_fee_value * (1 + relative_network_fee_surcharge)
            if row["window_type"] == 2
            else network_fee_value
        ),
        axis=1,
    )

    return merged_data


# df = pd.DataFrame({
#     'timestamp': pd.date_range("2025-03-01 00:00", periods=4800, freq="15min"),
#     'electricity_price': np.random.uniform(50, 200, 4800)
# })

# result = calculate_dynamic_network_fee(
#     df,
#     network_fee_value=20,
#     relative_network_fee_reduction=0.8,  # 80% Reduktion im Niedrigpreisfenster
#     relative_network_fee_surcharge=0.5   # 50% Zuschlag im Hochpreisfenster
# )

# print(result[['timestamp', 'electricity_price', 'dynamic_price', 'window_type']])
# result.to_csv('dynamic_prices.csv', index=False)
```

## crud/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains the CRUD operations (CREATE, READ, UPDATE, DELETE) for each repository/table in database.
"""

from .user import user
from .grid import grid
from .process_electricity import process_electricity
from .process_heat import process_heat
from .emissions import emissions
from .model import model
from .footprint import footprint
from .data_parc import data_parc
from .flexible_power import flexible_power
from .optimization_results import optimization_results
from .prices import prices
from .simulation_input_data import simulation_input_data
```

## crud/data_parc.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from sqlalchemy.orm import Session
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import DataParc
from forest_ensys.schemas import DataParcCreate, DataParcUpdate
from datetime import datetime


class CRUDDataParc(CRUDBase[DataParc, DataParcCreate, DataParcUpdate]):
    def create(self, db: Session, *, obj_in: DataParcCreate, user_id: int) -> DataParc:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj
    
    def delete(self, db: Session) -> Optional[DataParc]:
        return db.query(DataParc).delete()
    

data_parc = CRUDDataParc(DataParc)
```

## crud/emissions.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session
from sqlalchemy import desc


from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Emissions
from forest_ensys.schemas import EmissionsCreate, EmissionsUpdate


class CRUDEmissions(CRUDBase[Emissions, EmissionsCreate, EmissionsUpdate]):
    def get_current_emissions(self, db: Session) -> Optional[Emissions]:
        latest = db.query(Emissions).order_by(desc(Emissions.timestamp)).first()
        return db.query(Emissions).filter(
            Emissions.timestamp == latest.timestamp,
            Emissions.zone_key == "DE",
            Emissions.emission_factor_type == "direct",
        )

    def get_specific_emissions(
        self, db: Session, *, zone_key: str, emission_type: str, production_mode: str
    ) -> Optional[Emissions]:
        return (
            db.query(Emissions)
            .filter(
                Emissions.zone_key == zone_key,
                Emissions.emission_factor_type == emission_type,
                Emissions.production_mode == production_mode,
            )
            .order_by(desc(Emissions.timestamp))
            .first()
        )

    def create(
        self, db: Session, obj_in=Emissions | EmissionsCreate | dict[str, any]
    ) -> Optional[Emissions]:
        new_dataset: Emissions = super().create_multi(db=db, obj_in=obj_in)
        return new_dataset

    # do a delete from emissions
    def delete(self, db: Session) -> Optional[Emissions]:
        return db.query(Emissions).delete()


emissions = CRUDEmissions(Emissions)
```

## crud/footprint.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from sqlalchemy.orm import Session
from sqlalchemy import desc
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Footprint
from forest_ensys.schemas import FootprintCreate, FootprintUpdate
from datetime import datetime


class CRUDFootprint(CRUDBase[Footprint, FootprintCreate, FootprintUpdate]):
    def get_latest(self, db: Session) -> Optional[Footprint]:
        return db.query(Footprint).order_by(desc(Footprint.timestamp)).first()

    def create(self, db: Session, *, obj_in: FootprintCreate) -> Optional[Footprint]:
        new_dataset: Footprint = super().create(db, obj_in=obj_in)
        return new_dataset
    
    # def get_multi_by_date_range(self,db: Session, start_date: datetime, end_date: datetime):
    #     return db.query(Footprint).filter(Footprint.timestamp >= start_date, Footprint.timestamp <= end_date).all()
    def delete(self, db: Session) -> Optional[Footprint]:
        return db.query(Footprint).delete()


footprint = CRUDFootprint(Footprint)
```

## crud/grid.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import desc, func
import pandas as pd
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Grid, Footprint
from forest_ensys.schemas import GridCreate, GridUpdate


class CRUDGrid(CRUDBase[Grid, GridCreate, GridUpdate]):
    def get_current_grid(self, db: Session) -> Optional[Grid]:
        latest = db.query(Grid).order_by(desc(Grid.timestamp)).first()
        return db.query(Grid).filter(Grid.timestamp == latest.timestamp)

    def get_latest_for_commodity(self, db: Session, commodity_id: int) -> pd.Timestamp:
        return (
            db.query(Grid)
            .filter(Grid.commodity_id == commodity_id)
            .order_by(desc(Grid.timestamp))
            .first()
        )

    def get_average_co2_by_commodity(self, db: Session) -> List:
        return (
            db.query(
                Grid.timestamp,
                func.sum(Grid.co2).label("total_co2"),
                func.sum(Grid.mwh).label("total_mwh"),
            )
            .outerjoin(Footprint, Grid.timestamp == Footprint.timestamp)
            .filter(Footprint.timestamp.is_(None))
            .group_by(Grid.timestamp)
            .all()
        )

    def create(self, db: Session, obj_in: Grid | dict[str, Any]) -> Optional[Grid]:
        new_dataset: Grid = super().create(db, obj_in=obj_in)
        return new_dataset
    
    def delete(self, db: Session) -> Optional[Grid]:
        return db.query(Grid).delete()


grid = CRUDGrid(Grid)
```

## crud/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session
from sqlalchemy import func

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Model
from forest_ensys.schemas import ModelCreate, ModelUpdate


class CRUDModel(CRUDBase[Model, ModelCreate, ModelUpdate]):
    def create(self, db: Session, *, obj_in: ModelCreate, user_id: int) -> Model:
        """
        Creates a new Model object.
        """
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        obj_in_dict["model"] = obj_in_dict["model"]
        db_obj = super().create(db, obj_in=obj_in_dict)
        # db_obj.model = json.loads(db_obj.model)
        return db_obj

    def get_by_name(self, db: Session, *, name: str) -> Optional[Model]:
        # db_obj = db.query(Model).filter(Model.model["model"]["name"] == name).first()
        db_obj = (
            db.query(Model)
            .filter(func.json_extract(Model.model, "$.model.name") == name)
            .first()
        )
        return db_obj
        # if db_obj is not None:
        #     obj_out_dict = db_obj.model
        # else:
        #     obj_out_dict = None
        # return obj_out_dict


model = CRUDModel(Model)
```

## crud/process_electricity.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import ProcessElectricity
from forest_ensys.schemas import ProcessElectricityCreate, ProcessElectricityUpdate


class CRUDProcessElectricity(
    CRUDBase[ProcessElectricity, ProcessElectricityCreate, ProcessElectricityUpdate]
):
    def get_from_start_date(
        self, db: Session, user_id: int, start_date: str
    ) -> Optional[ProcessElectricity]:
        return db.query(ProcessElectricity).filter(
            ProcessElectricity.ref_created_by == user_id,
            ProcessElectricity.timestamp >= start_date,
        )

    def create(
        self, db: Session, *, obj_in: ProcessElectricityCreate, user_id: int
    ) -> ProcessElectricity:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj


process_electricity = CRUDProcessElectricity(ProcessElectricity)
```

## crud/process_heat.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import ProcessHeat
from forest_ensys.schemas import ProcessHeatCreate, ProcessHeatUpdate
from datetime import datetime


class CRUDProcessHeat(CRUDBase[ProcessHeat, ProcessHeatCreate, ProcessHeatUpdate]):
    def get_from_start_date(
        self, db: Session, user_id: int, start_date: str
    ) -> Optional[ProcessHeat]:
        return db.query(ProcessHeat).filter(
            ProcessHeat.ref_created_by == user_id, ProcessHeat.timestamp >= start_date
        )

    def create(
        self, db: Session, *, obj_in: ProcessHeatCreate, user_id: int
    ) -> ProcessHeat:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj
    


process_heat = CRUDProcessHeat(ProcessHeat)
```

## crud/weather.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional

from sqlalchemy.orm import Session
from sqlalchemy import desc

from forest_ensys.core import crawlers
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Weather
from forest_ensys.schemas import WeatherCreate, WeatherUpdate


class CRUDWeather(CRUDBase(Weather, WeatherCreate, WeatherUpdate)):
    def get_current_weather(self, db: Session) -> Optional[Weather]:
        latest = db.query(Weather).order_by(desc(Weather.timestamp)).first()
        return db.query(Weather).filter(Weather.timestamp == latest.timestamp)

    def create(self, db: Session) -> Optional[Weather]:
        db.add_all(crawlers.crawl_weather_data())
        db.commit()
        return db.query(Weather).order_by(desc(Weather.timestamp)).first()


grid = CRUDWeather(Weather)
```

## crud/flexible_power.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import List
from sqlalchemy.orm import Session
from forest_ensys.model import FlexiblePower
from forest_ensys.schemas import FlexiblePowerCreate, FlexiblePowerUpdate
from forest_ensys.crud.base import CRUDBase


class CRUDFlexiblePower(
    CRUDBase[FlexiblePower, FlexiblePowerCreate, FlexiblePowerUpdate]
):
    def create(self, db: Session, obj_in: FlexiblePower | FlexiblePowerCreate):
        db_obj = FlexiblePower(**obj_in.dict())
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj

    # def create_multi(
    #     self, db: Session, *, obj_in: List[FlexiblePowerCreate]
    # ) -> List[FlexiblePower]:
    #     """
    #     Create multiple flexible power objects.
    #     """
    #     db_objs = [FlexiblePower(**obj.dict()) for obj in obj_in]
    #     db.add_all(db_objs)
    #     db.commit()
    #     return db_objs

    def get_multi_flexible_power(self, db: Session, skip: int = 0, limit: int = 100):
        return db.query(FlexiblePower).offset(skip).limit(limit).all()
    
    def delete(self, db: Session, user_id: int):
        return db.query(FlexiblePower).filter(FlexiblePower.ref_created_by == user_id).delete()
    
    def delete_by_user_id_and_optimization_case_name(self, db: Session, user_id, optimization_case_name: str):
        return db.query(FlexiblePower).filter(FlexiblePower.ref_created_by == user_id, FlexiblePower.optimization_case_name == optimization_case_name).delete()


flexible_power = CRUDFlexiblePower(FlexiblePower)
```

## crud/prices.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import desc
from sqlalchemy import text
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import Prices
from datetime import datetime
import pandas as pd


class CRUDPrices(CRUDBase[Prices, Any, Any]):
    def get_by_timestamp(self, db: Session, *, timestamp: Any) -> Optional[Prices]:
        return db.query(Prices).filter(Prices.timestamp == timestamp).first()

    def get_latest(self, db: Session) -> Optional[Prices]:
        return db.query(Prices).filter(Prices.source == "smard").order_by(desc(Prices.timestamp)).first()

    def get_by_timestamp_range(
        self, db: Session, *, start: Any, end: Any
    ) -> List[Prices]:
        return (
            db.query(Prices)
            .filter(Prices.timestamp >= start, Prices.timestamp <= end)
            .all()
        )

    def get_by_source(
        self, db: Session, *, source: str, limit: int = 100, skip: int = 0
    ) -> List[Prices]:
        return (
            db.query(Prices)
            .filter(Prices.source == source)
            .offset(skip)
            .limit(limit)
            .all()
        )

    def get_multi_by_date_range_and_source(
        self, db: Session, *, start_date: datetime, end_date: datetime, source: str
    ) -> Optional[pd.DataFrame]:
        query = """
            SELECT * 
            FROM prices
            WHERE timestamp BETWEEN :start_date AND :end_date AND source = :source
        """

        # Abfrage ausfhren
        result = pd.read_sql_query(
            sql=text(query),
            con=db.connection(),
            params={
                "start_date": start_date,
                "end_date": end_date,
                "source": source,
            },
        )

        return result if not result.empty else None

    def create(self, db: Session, obj_in: Prices | dict[str, Any]) -> Optional[Prices]:
        new_dataset: Prices = super().create(db, obj_in=obj_in)
        return new_dataset

    def delete(self, db: Session, source: str) -> Optional[Prices]:
        return db.query(Prices).filter(Prices.source == source).delete()


prices = CRUDPrices(Prices)
```

## crud/simulation_input_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any

from sqlalchemy.orm import Session
from sqlalchemy.sql import text

from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import SimulationInputData
from datetime import datetime
import pandas as pd

class CRUDSimulationInputData(CRUDBase[SimulationInputData, Any, Any]):
    def create(
        self, db: Session, *, obj_in: SimulationInputData, user_id: int
    ) -> SimulationInputData:
        obj_in_dict = obj_in.dict()
        obj_in_dict["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in_dict)
        return db_obj
    def delete_by_user_and_name(self, db: Session, *, user_id: int, name: str) -> Optional[SimulationInputData]:
        return db.query(self.model).filter(self.model.ref_created_by == user_id, self.model.name == name).delete()
    def get_multi_by_date_range_and_name(
        self, db: Session, *, start_date: datetime, end_date: datetime,user_id: int, name: str
    ) -> Optional[pd.DataFrame]:
        query = """
            SELECT * 
            FROM simulation_input_data
            WHERE timestamp BETWEEN :start_date AND :end_date AND ref_created_by = :user_id AND name = :name
        """

        # Abfrage ausfhren
        result = pd.read_sql_query(
            sql=text(query),
            con=db.connection(),
            params={
                "start_date": start_date,
                "end_date": end_date,
                "name": name,
                "user_id": user_id
            },
        )

        return result if not result.empty else None
    
simulation_input_data = CRUDSimulationInputData(SimulationInputData)
```

## crud/optimization_results.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional, Any, List
from sqlalchemy.orm import Session
from sqlalchemy import desc
from forest_ensys.crud.base import CRUDBase
from forest_ensys.model import OptimizationResult


class CRUDOptimizationResults(CRUDBase[OptimizationResult, Any, Any]):
    def create(
        self, db: Session, *, obj_in: OptimizationResult, user_id: int = None
    ) -> OptimizationResult:
        if user_id is not None:
            obj_in["ref_created_by"] = user_id
        db_obj = super().create(db, obj_in=obj_in)
        return db_obj
    
    def get(self, db: Session, *, user_id: int, optimization_case_name: str) -> Optional[OptimizationResult]:
        return db.query(self.model).filter(
            self.model.ref_created_by == user_id,
            self.model.name == optimization_case_name,
        ).first()
    
    def delete(self, db: Session, *, user_id: int) -> None:
        db.query(self.model).filter(self.model.ref_created_by == user_id).delete()
        db.commit()
        
    def delete_by_user_id_and_optimization_case_name(
        self, db: Session, *, user_id: int, optimization_case_name: str
    ) -> None:
        db.query(self.model).filter(
            self.model.ref_created_by == user_id,
            self.model.name == optimization_case_name,
        ).delete()
        db.commit()

optimization_results = CRUDOptimizationResults(OptimizationResult)
```

## crud/base.py

```python
from typing import Any, Dict, Generic, List, Optional, Type, TypeVar, Union
from fastapi.encoders import jsonable_encoder
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import text
from forest_ensys.database.base_class import Base
import pandas as pd
from datetime import datetime

ModelType = TypeVar("ModelType", bound=Base)
CreateSchemaType = TypeVar("CreateSchemaType", bound=BaseModel)
UpdateSchemaType = TypeVar("UpdateSchemaType", bound=BaseModel)


class CRUDBase(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):
    def __init__(self, model: Type[ModelType]):
        """
        CRUD object with default methods to Create, Read, Update, Delete (CRUD).
        **Parameters**
        * `model`: A SQLAlchemy model class
        * `schema`: A Pydantic model (schema) class
        """
        self.model = model

    def get(self, db: Session, id: Any) -> Optional[ModelType]:
        return db.query(self.model).filter(self.model.id == id).first()

    def get_multi(
        self, db: Session, *, skip: int = 0, limit: int = 100, user_id: int = None
    ) -> List[ModelType]:
        if user_id:
            return (
                db.query(self.model)
                .filter(self.model.ref_created_by == user_id)
                .offset(skip)
                .limit(limit)
                .all()
            )
        else:
            return db.query(self.model).offset(skip).limit(limit).all()
        
    def get_multi_by_date_range(
        self, db: Session, user_id: Optional[int] = None, start_date: datetime = None, end_date: datetime = None
    ) -> Optional[pd.DataFrame]:
        # Basisabfrage
        query = f"""
            SELECT * 
            FROM {self.model.__tablename__}
            WHERE timestamp BETWEEN :start_date AND :end_date
        """
        
        # Dynamische Bedingung hinzufgen, falls user_id gesetzt ist
        if user_id is not None:
            query += " AND ref_created_by = :user_id"
        
        # Abfrage ausfhren
        result = pd.read_sql_query(
            sql=text(query),
            con=db.connection(),
            params={
                "user_id": user_id,
                "start_date": start_date,
                "end_date": end_date,
            }
        )
        
        return result if not result.empty else None


    def create(
        self, db: Session, *, obj_in: Union[CreateSchemaType, ModelType, dict]
    ) -> ModelType:
        if isinstance(obj_in, self.model):
            db_obj = obj_in
        elif isinstance(obj_in, dict):
            # filter obj_in to only pass fields in model to model's constructor
            data = {
                k: v
                for k, v in obj_in.items()
                if k in self.model.__table__.columns.keys()
            }
            db_obj = self.model(**data)
        else:
            obj_in_data = jsonable_encoder(
                obj_in, include=self.model.__table__.columns.keys()
            )
            db_obj = self.model(**obj_in_data)  # type: ignore
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj

    def create_multi(
        self, db: Session, *, obj_in: List[Union[CreateSchemaType, ModelType, dict]], ref_created_by: Optional[int] = None
    ) -> List[ModelType]:
        
        def _bulk_set_attr(objs: list, attr: str, value: Any):
            for obj in objs:
                setattr(obj, attr, value)
        if isinstance(obj_in, list) and all(isinstance(obj, self.model) for obj in obj_in):
            db_obj = obj_in
        elif isinstance(obj_in, list) and all(isinstance(obj, dict) for obj in obj_in):
            db_obj = [self.model(**data) for data in obj_in]
        else:
            raise ValueError("Invalid input type")
        if ref_created_by and db_obj[0].ref_created_by is None:
            _bulk_set_attr(db_obj, 'ref_created_by', ref_created_by)

        db.add_all(db_obj)
        db.commit()
        return db_obj

    def update(
        self,
        db: Session,
        *,
        db_obj: ModelType,
        obj_in: Union[UpdateSchemaType, Dict[str, Any]],
    ) -> ModelType:
        obj_data = jsonable_encoder(db_obj)
        if isinstance(obj_in, dict):
            update_data = obj_in
        else:
            update_data = obj_in.dict(exclude_unset=True)
        for field in obj_data:
            if field in update_data:
                setattr(db_obj, field, update_data[field])
        db.add(db_obj)
        db.commit()
        db.refresh(db_obj)
        return db_obj
    
    def delete(self, db: Session) -> Optional[ModelType]:
        return db.query(self.model).delete()

    def remove(self, db: Session, *, id: Any) -> ModelType:
        obj = db.query(self.model).get(id)
        db.delete(obj)
        db.commit()
        return obj
```

## database/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains everything needed for the database.
"""
```

## database/base_class.py

```python
import re
from typing import Any

from sqlalchemy.orm import as_declarative, declared_attr


@as_declarative()
class Base:
    """
    Base class for all database models
    """

    id: Any
    __name__: str
    __allow_unmapped__ = True

    # Generate __tablename__ automatically
    @declared_attr
    def __tablename__(self) -> str:
        return re.sub(r"(?<!^)(?=[A-Z])", "_", self.__name__).lower()
```

## database/init_db.py

```python
import logging
from forest_ensys.database.base_class import Base
from forest_ensys.database.session import engine, SessionLocal
from sqlalchemy import text

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_connection():
    try:
        db = SessionLocal()
        db.execute(text("SELECT 1"))
    except Exception as e:
        logger.error(e)
        raise e


def create_all():
    Base.metadata.create_all(engine)
```

## database/session.py

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

from forest_ensys.core import settings

engine = create_engine(
    settings.SQLALCHEMY_DATABASE_URI,
    execution_options={"isolation_level": "AUTOCOMMIT"},
    pool_pre_ping=True,
    poolclass=StaticPool,
)
SessionLocal = sessionmaker(
    autocommit=False, autoflush=False, expire_on_commit=False, bind=engine
)
```

## model/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains all models in database
"""

from .user import User
from .grid import Grid
from .process_electricity import ProcessElectricity
from .process_heat import ProcessHeat
from .emissions import Emissions
from .model import Model
from .footprint import Footprint
from .data_parc import DataParc
from .flexible_power import FlexiblePower
from .optimization_results import OptimizationResult
from .prices import Prices
from .simulation_input_data import SimulationInputData
```

## model/data_parc.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime, String

from forest_ensys.database.base_class import Base


class DataParc(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    signal_id = Column(String, primary_key=True, nullable=False)
    signal_name = Column(String, nullable=False)
    value = Column(Double, nullable=False)
    unit = Column(String, nullable=False)
```

## model/emissions.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, String, Double, DateTime
from forest_ensys.database.base_class import Base


class Emissions(Base):
    """
    Database class for the emissions of a powerplant.
    """

    timestamp = Column(DateTime, primary_key=True, nullable=False)
    zone_key = Column(String, primary_key=True, nullable=False)
    emission_factor_type = Column(String, primary_key=True, nullable=False)
    production_mode = Column(String, primary_key=True, nullable=False)
    value = Column(Double, nullable=False)
    source = Column(String, nullable=False)
```

## model/footprint.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Double, DateTime
from forest_ensys.database.base_class import Base


class Footprint(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    co2 = Column(Double, primary_key=True, nullable=False)
```

## model/grid.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Integer, String, Double, DateTime
from forest_ensys.database.base_class import Base


class Grid(Base):
    """
    Grid class
    """

    timestamp = Column(DateTime, primary_key=True, nullable=False)
    commodity_id = Column(Integer, primary_key=True, nullable=False)
    commodity_name = Column(String, nullable=False)
    mwh = Column(Double, nullable=False)
    co2 = Column(Double, nullable=False)
```

## model/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, ForeignKey
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base


class Model(Base):
    id = Column(Integer, primary_key=True, index=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    model = Column(JSONB, nullable=False)

    user = relationship("User", back_populates="model")
```

## model/process_electricity.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base


class ProcessElectricity(Base):
    id = Column(Integer, primary_key=True, index=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    timestamp = Column(DateTime, nullable=False)
    power_demand = Column(Double, nullable=False)

    user = relationship("User", back_populates="process_electricity")
```

## model/process_heat.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base


class ProcessHeat(Base):
    id = Column(Integer, primary_key=True, index=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False)
    timestamp = Column(DateTime, nullable=False)
    heat_demand = Column(Double, nullable=False)
    # temperature = Column(Double, nullable=False)
    # pressure = Column(Double, nullable=False)
    # mass_flow = Column(Double, nullable=False)

    user = relationship("User", back_populates="process_heat")
```

## model/user.py

```python
from sqlalchemy import Column, Integer, String
from sqlalchemy.orm import relationship
from forest_ensys.database.base_class import Base


class User(Base):
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    process_electricity = relationship("ProcessElectricity", back_populates="user")
    process_heat = relationship("ProcessHeat", back_populates="user")
    model = relationship("Model", back_populates="user")
```

## model/weather.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Integer, String, Double, DateTime
from forest_ensys.database.base_class import Base


class Weather(Base):
    id = Column(Integer, primary_key=True, index=True)
    timestamp = Column(DateTime, nullable=False)
    nuts_id = Column(String, nullable=False)
    temperature = Column(Double, nullable=False)
    humidity = Column(Double, nullable=False)
    ghi = Column(Double, nullable=False)
```

## model/prices.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Double, DateTime, String
from forest_ensys.database.base_class import Base

class Prices(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    source = Column(String, primary_key=True, nullable=False)
    price = Column(Double, nullable=False)
```

## model/simulation_input_data.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later


from sqlalchemy import Column, Integer, Double, ForeignKey, DateTime, String
from sqlalchemy.orm import relationship

from forest_ensys.database.base_class import Base

class SimulationInputData(Base):
    timestamp = Column(DateTime, nullable=False, primary_key=True)
    ref_created_by = Column(Integer, ForeignKey("user.id"), nullable=False, primary_key=True)
    name = Column(String, nullable=False, primary_key=True)
    value = Column(Double, nullable=False)
```

## model/optimization_results.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, Float, String, Integer, ForeignKey, DateTime
from forest_ensys.database.base_class import Base


class OptimizationResult(Base):
    name = Column(String, nullable=False, primary_key=True)
    ref_created_by = Column(
        Integer, ForeignKey("user.id"), nullable=False, primary_key=True
    )
    time_from = Column(DateTime, nullable=False)
    time_to = Column(DateTime, nullable=False)
    network_fee_type = Column(String, nullable=False)
    network_fee = Column(Float, nullable=False)
    total_energy_demand = Column(Float, nullable=False)
    electricity_used = Column(Float, nullable=False)
    gas_usage = Column(Float, nullable=False)
    cost_savings = Column(Float, nullable=False)
    emissions_savings = Column(Float, nullable=False)
    cost_gas_only = Column(Float, nullable=False)
    cost_with_electric_heating = Column(Float, nullable=False)
    emissions_gas_only = Column(Float, nullable=False)
    emissions_with_electric_heating = Column(Float, nullable=False)
    full_load_hours = Column(Float, nullable=False)
    full_load_hours_after_optimization = Column(Float, nullable=False)
    mean_electricity_price_when_heating = Column(Float, nullable=False)
    electric_heating_in_low_price_windows_ratio = Column(Float, nullable=False)
```

## model/flexible_power.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from sqlalchemy import Column, DateTime, Float, String, Integer, ForeignKey
from forest_ensys.database.base_class import Base


class FlexiblePower(Base):
    timestamp = Column(DateTime, primary_key=True, nullable=False)
    ref_created_by = Column(
        Integer, ForeignKey("user.id"), nullable=False, primary_key=True
    )
    optimization_case_name = Column(String, primary_key=True, nullable=False)
    electricity_used = Column(Float, nullable=False)
    low_price_window = Column(Integer, nullable=False)
```

## schemas/__init__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

"""
This package contains every model that is returned from the Rest-API.
"""

from .user import User, UserCreate, UserInDB, UserUpdate
from .token import Token, TokenPayload
from .grid import Grid, GridCreate, GridUpdate
from .process_electricity import (
    ProcessElectricity,
    ProcessElectricityCreate,
    ProcessElectricityUpdate,
)
from .process_heat import ProcessHeat, ProcessHeatCreate, ProcessHeatUpdate
from .emissions import Emissions, EmissionsCreate, EmissionsUpdate
from .co2 import Co2
from .model import Model, ModelCreate, ModelUpdate
from .footprint import Footprint, FootprintCreate, FootprintUpdate
from .data_parc import DataParc, DataParcCreate, DataParcUpdate
from .optimization_results import OptimizationResult
from .prices import Prices
from .flexible_power import FlexiblePower, FlexiblePowerCreate, FlexiblePowerUpdate
```

## schemas/co2.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field


class Co2(BaseModel):
    co2: float = Field(None, description="co2 in kg_co2/kwh", example="5000")
```

## schemas/data_parc.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class DataParcBase(BaseModel):
    """
    Base class for Process  entries of data parc
    """

    timestamp: datetime = Field(None, description="timestamp for every entry")
    signal_id: str = Field(
        None,
        description="uuid of the signal",
        example="123e4567-e89b-12d3-a456-426655440000",
    )
    signal_name: str = Field(
        None, description="name of the signal", example="Dampfverbrauch"
    )
    value: float = Field(None, description="value of the signal", example=100.0)
    unit: str = Field(None, description="unit of the signal", example="kWh")


class DataParcCreate(DataParcBase):
    """
    Create a Process entry
    """

    pass


class DataParcUpdate(DataParcBase):
    """
    Update a Process entry
    """

    pass


class DataParcInDBBase(DataParcBase):
    """
    Process entry to return via API
    """

    ref_created_by: Optional[int] = Field(
        None, description="if of who send this  entry", example="1"
    )

    class Config:
        from_attributes = True


class DataParc(DataParcInDBBase):
    pass
```

## schemas/emissions.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class EmissionsBase(BaseModel):
    """
    Shared attributes for the emissions table.
    """

    timestamp: Optional[datetime] = Field(
        default=None,
        description="Date and time when the emission factors were updated.",
        example="2020-01-01 00:00:00",
    )
    zone_key: Optional[str] = Field(
        default=None, description="Zone key of the country.", example="DE"
    )
    emission_factor_type: Optional[str] = Field(
        default=None,
        description="Emissions factor of the commodity.",
        example="direct or lifecycle",
    )
    production_mode: Optional[str] = Field(
        default=None, description="Energy commodity.", example="gas"
    )
    value: Optional[float] = Field(
        default=None, description="Emissions factor value.", example=820
    )
    source: Optional[str] = Field(
        default=None,
        description="Source of how the emissions factor is calculated.",
        example="ENTSO-E 2021",
    )


class EmissionsCreate(EmissionsBase):
    """
    Attributes to receive via API on creation of a Emissions object.
    """

    pass


class EmissionsUpdate(EmissionsBase):
    """
    Attributes to receive via API on update of a Emissions object.
    """

    pass


class EmissionsInDB(EmissionsBase):
    """
    Attributes to return via API for an Emissions object.
    """

    pass

    class Config:
        from_attributes = True


class Emissions(EmissionsInDB):
    """
    Attributes to return via API for an Emissions object.
    """

    pass
```

## schemas/footprint.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field
from datetime import datetime


class Footprint(BaseModel):
    timestamp: datetime = Field(
        None, description="timestamp for every entry", example="yyyy-mm-dd hh:mm:ss"
    )
    co2: float = Field(None, description="co2 in g_co2/kwh", example="370")


class FootprintCreate(Footprint):
    pass


class FootprintUpdate(Footprint):
    pass


class FootprintInDBBase(Footprint):
    class Config:
        from_attributes = True


class Footprint(FootprintInDBBase):
    pass
```

## schemas/grid.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field
from datetime import datetime


class GridBase(BaseModel):
    """
    Shared attributes for the grid table.
    """

    timestamp: datetime = Field(
        None, description="timestamp for every entry", example="yyyy-mm-dd hh:mm:ss"
    )
    mwh: float = Field(None, description="power value in mwh.", example="5000")
    commodity_id: int = Field(None, description="id of the energy type", example="4067")
    commodity_name: str = Field(
        None, description="name of the energy type", example="Wind Onshore"
    )
    co2: float = Field(None, description="co2 in kg_co2/kwh", example="5000")


class GridCreate(GridBase):
    """
    Attributes to receive via API on creation of a dataset.
    """

    pass


class GridUpdate(GridBase):
    """
    Attributes to receive via API on update of a dataset.
    """

    pass


class GridInDBBase(GridBase):
    class Config:
        from_attributes = True


class Grid(GridInDBBase):
    """
    Attributes to return via API for a dataset.
    """

    pass
```

## schemas/model.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import yaml
from typing import Optional, Dict
from pydantic import BaseModel, Field

yaml_data = """
config:
  init:
    name: Wepa Mainz
    calliope_version: 0.7.0

  build:
    mode: plan # Choices: plan, operate
    ensure_feasibility: true # Switching on unmet demand

  solve:
    solver: cbc

parameters:
  objective_cost_weights:
    data: 1
    index: monetary
    dims: costs
  bigM: 1e6

data_sources:
  demand:
    source: demand_data
    rows: timesteps
    columns: [nodes, techs]
    add_dims:
      parameters: resource
    
nodes:
  X1:
    techs:
      supply_grid_power:
        cost_flow_cap.data: 100 # cost of transformers
      supply_gas:
      supply_grid_heat:
      demand_electricity:
      demand_heat:
    

techs:
  supply_grid_power:
    name: "National grid import"
    base_tech: supply
    inherit: interest_rate_setter
    carrier_out: electricity
    source_use_max: .inf
    flow_cap_max: 2000
    lifetime: 25
    cost_flow_cap:
      data: 15
      index: monetary
      dims: costs
    cost_flow_in:
      data: 0.1 # 10p/kWh electricity price #ppt
      index: monetary
      dims: costs

  supply_gas:
    name: "Natural gas import"
    base_tech: supply
    inherit: interest_rate_setter
    carrier_out: gas
    source_use_max: .inf
    flow_cap_max: 2000
    lifetime: 25
    cost_flow_cap:
      data: 1
      index: monetary
      dims: costs
    cost_flow_in:
      data: 0.025 # 2.5p/kWh gas price #ppt
      index: monetary
      dims: costs

  supply_grid_heat:
    name: "Heat from the grid"
    base_tech: supply
    carrier_out: heat
    flow_cap_max: 2000000000
    cost_flow_in:
      data: 0.05
      index: monetary
      dims: costs

  demand_electricity:
    name: "Electrical demand"
    base_tech: demand
    carrier_in: electricity

  demand_heat:
    name: "Heat demand"
    base_tech: demand
    carrier_in: heat
"""
parsed_yaml = yaml.safe_load(yaml_data)


class ModelBase(BaseModel):
    model: Optional[Dict] = Field(
        ..., description="JSON describing the technologies", example=parsed_yaml
    )


class ModelCreate(ModelBase):
    """A model representing a calliope model to be created."""

    pass


class ModelUpdate(ModelBase):
    """A model representing a calliope model to be updated."""

    pass


class ModelInDBBase(ModelBase):
    id: Optional[int] = Field(
        None, description="Unique identifier for each calliope model entry"
    )
    ref_created_by: Optional[int] = Field(
        None, description="Reference to the user who created the calliope model entry"
    )

    class Config:
        from_attributes = True


class Model(ModelInDBBase):
    """A model representing a calliope model in the database."""

    pass
```

## schemas/process_electricity.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from pydantic import BaseModel, Field
from datetime import datetime


class ProcessElectricityBase(BaseModel):
    timestamp: datetime = Field(
        ...,
        description="timestamp for every entry",
    )
    power_demand: float = Field(..., description="power value in watt.", example="5000")


class ProcessElectricityCreate(ProcessElectricityBase):
    pass


class ProcessElectricityUpdate(ProcessElectricityBase):
    pass


class ProcessElectricityInDBBase(ProcessElectricityBase):
    id: int = Field(
        ...,
        description="id for every entry",
    )
    ref_created_by: int = Field(
        ...,
        description="ref to user for every entry",
    )

    class Config:
        from_attributes = True


class ProcessElectricity(ProcessElectricityInDBBase):
    pass
```

## schemas/process_heat.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class ProcessHeatBase(BaseModel):
    """
    Base class for ProcessHeat
    """

    timestamp: datetime = Field(None, description="timestamp for every entry")
    heat_demand: float = Field(None, description="power value in watt.", example=100.0)
    # temperature: float = Field(None,
    #                             description="needed temperature for the process in C",
    #                             example=20.0)
    # pressure: float = Field(None,
    #                         description="needed pressure for the process in Pascal",
    #                         example=300000.0)
    # mass_flow: float = Field(None,
    #                          description="needed mass flow for the process in kg/s",
    #                          example=100.0)


class ProcessHeatCreate(ProcessHeatBase):
    """
    Create a ProcessHeat entry
    """

    pass


class ProcessHeatUpdate(ProcessHeatBase):
    """
    Update a ProcessHeat entry
    """

    pass


class ProcessHeatInDBBase(ProcessHeatBase):
    """
    ProcessHeat entry to return via API
    """

    id: Optional[int] = Field(
        None, description="id of the ProcessHeat entry", example=1
    )

    ref_created_by: Optional[int] = Field(
        None, description="if of who send this heat entry", example="1"
    )

    class Config:
        from_attributes = True


class ProcessHeat(ProcessHeatInDBBase):
    pass
```

## schemas/token.py

```python
from typing import Optional

from pydantic import BaseModel


class Token(BaseModel):
    access_token: str
    token_type: str


class TokenPayload(BaseModel):
    sub: Optional[int] = None
```

## schemas/user.py

```python
from typing import Optional

from pydantic import BaseModel


# Shared attributes
class UserBase(BaseModel):
    username: Optional[str] = None


# Attributes to receive via API on creation
class UserCreate(UserBase):
    username: str
    password: str


# Attributes to receive via API on update
class UserUpdate(UserBase):
    password: Optional[str] = None


class UserInDBBase(UserBase):
    id: Optional[int] = None

    class Config:
        from_attributes = True


# Additional properties to return via API
class User(UserInDBBase):
    pass


# Additional properties stored in DB
class UserInDB(UserInDBBase):
    hashed_password: str
```

## schemas/weather.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class WeatherBase(BaseModel):
    timestamp: datetime = Field(
        None, description="timestamp for every entry", example="yyyy-mm-dd hh:mm:ss"
    )
    nuts_id: str = Field(None, description="nuts id", example="DE")
    temperature: float = Field(
        None, description="temperature in celsius.", example=25.0
    )
    humidity: float = Field(None, description="humidity in percentage.", example=50.0)
    ghi: float = Field(
        None, description="global horizontal irradiation in w/m2.", example=100.0
    )


class WeatherCreate(WeatherBase):
    pass


class WeatherUpdate(WeatherBase):
    pass


class WeatherInDBBase(WeatherBase):
    id: Optional[int] = None

    class Config:
        from_attributes = True


class Weather(WeatherInDBBase):
    pass
```

## schemas/flexible_power.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class FlexiblePowerBase(BaseModel):
    timestamp: Optional[datetime] = Field(
        default=None,
        description="Date and time when the emission factors were updated.",
        example="2020-01-01 00:00:00",
    )
    optimization_case_name: Optional[str] = Field(
        default=None,
        description="The name of the optimization case.",
        example="Optimization Case 1",
    )
    electricity_used: Optional[float] = Field(
        default=None, description="The flexible power used.", example=0.0
    )


class FlexiblePowerCreate(FlexiblePowerBase):
    pass


class FlexiblePowerUpdate(FlexiblePowerBase):
    pass


class FlexiblePowerInDB(FlexiblePowerBase):
    pass

    class Config:
        from_attributes = True


class FlexiblePower(FlexiblePowerInDB):
    pass
```

## schemas/optimization_results.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class OptimizationResult(BaseModel):
    total_energy_demand: Optional[float] = Field(
        default=None, description="The total energy demand in kWh.", example=0.0
    )
    time_from: Optional[datetime] = Field(
        default=None,
        description="The start time of the optimization.",
        example="2020-01-01 00:00:00",
    )
    time_to: Optional[datetime] = Field(
        default=None,
        description="The end time of the optimization.",
        example="2020-01-01 00:00:00",
    )
    network_fee_type: Optional[str] = Field(
        default=None,
        description="The type of network fee.",
        example="static" or "dynamic",
    )
    network_fee: Optional[float] = Field(
        default=None, description="The network fee in Euro.", example=0.0
    )
    electricity_used: Optional[float] = Field(
        default=None, description="The flexible power used in kWh.", example=0.0
    )
    gas_usage: Optional[float] = Field(
        default=None, description="The gas usage in kWh.", example=0.0
    )
    cost_savings: Optional[float] = Field(
        default=None, description="The cost savings in Euro.", example=0.0
    )
    emissions_savings: Optional[float] = Field(
        default=None, description="The emissions savings in tonnes CO2.", example=0.0
    )
    cost_gas_only: Optional[float] = Field(
        default=None, description="The cost as is in Euro for gas usage.", example=0.0
    )
    cost_with_electric_heating: Optional[float] = Field(
        default=None, description="The cost with electric heating in Euro.", example=0.0
    )
    emissions_gas_only: Optional[float] = Field(
        default=None,
        description="The emissions as is in tonnes CO2 for gas usage.",
        example=0.0,
    )
    emissions_with_electric_heating: Optional[float] = Field(
        default=None,
        description="The emissions with electric heating in tonnes CO2.",
        example=0.0,
    )
    full_load_hours: Optional[float] = Field(
        default=None, description="The full load hours.", example=0.0
    )
    full_load_hours_after_optimization: Optional[float] = Field(
        default=None, description="The full load hours after optimization.", example=0.0
    )
    mean_electricity_price_when_heating: Optional[float] = Field(
        default=None, description="The mean electricity price when heating.", example=0.0
    )
    electric_heating_in_low_price_windows_ratio: Optional[float] = Field(
        default=None,
        description="The ratio of electric heating in low price windows.",
        example=0.0,
    )

    # flexible_power_used: float
    # gas_usage: float
    # cost_savings: float
    # emissions_savings: float
```

## schemas/prices.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

from typing import Optional
from pydantic import BaseModel, Field
from datetime import datetime


class Prices(BaseModel):
    timestamp: Optional[datetime] = Field(
        default=None,
        description="The timestamp of the price.",
        example="2024-01-01T00:00:00",
    )
    price: Optional[float] = Field(
        default=None, description="The price in Euro.", example=0.0
    )
    source: Optional[str] = Field(
        default=None,
        description="The source of the price.",
        example="electricity_smard",
    )
```

## __init__.py

```python

```

## __main__.py

```python
# SPDX-FileCopyrightText: 2024 Jonathan Sejdija
#
# SPDX-License-Identifier: AGPL-3.0-or-later

import uvicorn


def main():
    uvicorn.run(
        app="forest_ensys.app:app",
        host="0.0.0.0",
        port=8081,
        log_level="info",
        reload=True,
    )


if __name__ == "__main__":
    main()
```

## app.py

```python
from fastapi import FastAPI, status, Request
from fastapi.responses import JSONResponse

from forest_ensys.api import api_router
from forest_ensys.core import settings
from forest_ensys.database import init_db

# Create FastAPI app and add all endpoints
app = FastAPI(
    title=settings.SERVER_NAME,
    root_path="/api",
)
app.include_router(api_router)


# Initialize database and create models
@app.on_event("startup")
def init_database():
    init_db.check_connection()
    init_db.create_all()


@app.exception_handler(Exception)
async def exception_handler(request: Request, exc: Exception):
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={"detail": str(exc)},
    )
```

## Statistics

- Total Files: 74
- Total Characters: 160954
- Total Tokens: 0
````

## .gitignore

```text
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
generated/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/
docs/source/generated/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that dont work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# IDE
.idea/
.vscode/

# Database
**/local.db

# Temp folder
tmp/

model.yaml
model.json
timeseries_data/
data_sources/
model.yaml
datasource.yml
.aider*
*.ipynb
*.csv
*.xlsx
test.py
forest_ensys/core/heat_pump.py
```

## Dockerfile

```text
FROM python:3.11-slim
RUN mkdir /code
COPY requirements.txt /code
WORKDIR /code

RUN apt update && apt install -y glpk-utils

RUN pip install -r requirements.txt

COPY ./forest_ensys /code/forest_ensys

ENV PYTHONUNBUFFERED=TRUE
EXPOSE 9000
CMD ["uvicorn", "forest_ensys.app:app", "--host", "0.0.0.0", "--port", "9000", "--log-level", "debug"]
```

## README.md

```markdown
# forest_ensys

This project contains the AP 4 part of the FOREST project. If you have questions you might contact the developer: sejdija@fh-aachen.de
```

## requirements.txt

```text
https://github.com/calliope-project/calliope/archive/refs/tags/v0.7.0.dev4.zip
fastapi
sqlalchemy
gunicorn
uvicorn
pydantic
passlib
bcrypt
numpy
pandas
autodoc-pydantic
python-multipart
python-jose
pyaml
psycopg2-binary
gurobipy
openpyxl
aas-core3.0
```

## grafana/dashboard-provisioning.yml

```yaml
apiVersion: 1
providers:
  - name: 'Smard'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /var/lib/grafana/dashboards
```

## grafana/datasource-example.yml

```yaml
apiVersion: 1
datasources:
  - name: grafana-postgresql-datasource
    type: postgres
    url: IP:PORT
    user: SET THIS
    database: forest
    basicAuth: false
    isDefault: true
    secureJsonData:
      password: SET THIS
    jsonData:
      sslmode: disable
    editable: true
```

## grafana/dashboard_smard.json

```json
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 2,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "default": true,
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisGridShow": false,
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 36,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "normal"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "megwatt"
        },
        "overrides": [
          {
            "matcher": {
              "id": "byName",
              "options": "Co2 intensity"
            },
            "properties": [
              {
                "id": "unit",
                "value": "g/kwh"
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 11,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "hideTimeOverride": true,
      "id": 2,
      "interval": "15m",
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "desc"
        }
      },
      "pluginVersion": "8.5.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "time_series",
          "group": [],
          "metricColumn": "country",
          "rawQuery": true,
          "rawSql": "SELECT\n  $__timeGroupAlias(\"timestamp\",$__interval),\n  \"commodity_name\" AS metric,\n  avg(\"mwh\"*4) AS \"mwh\"\nFROM\n  grid\nWHERE\n  $__timeFilter(\"timestamp\")\nGROUP BY 1,2\nORDER BY 1,2",
          "refId": "A",
          "select": [
            [
              {
                "params": [
                  "biomass"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "fossil_hard_coal"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "geothermal"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "nuclear"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "\"fossil_brown_coal/lignite\""
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "\"fossil_coal-derived_gas\""
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "\"hydro_run-of-river_and_poundage\""
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "waste"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "solar"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "wind_offshore"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "wind_onshore"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "other_renewable"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "hydro_water_reservoir"
                ],
                "type": "column"
              }
            ],
            [
              {
                "params": [
                  "fossil_gas"
                ],
                "type": "column"
              }
            ]
          ],
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          },
          "table": "query_generation",
          "timeColumn": "index",
          "timeColumnType": "timestamp",
          "where": [
            {
              "name": "$__timeFilter",
              "params": [],
              "type": "macro"
            },
            {
              "datatype": "text",
              "name": "",
              "params": [
                "country",
                "=",
                "'$country'"
              ],
              "type": "expression"
            }
          ]
        },
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "hide": false,
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    co2 AS \"Co2 intensity\"\nFROM \n    footprint \nWHERE \n    $__timeFilter(\"timestamp\")\n",
          "refId": "B",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "timeShift": "24h",
      "title": "Stromerzeugung SMARD (average over ${__interval})",
      "type": "timeseries"
    },
    {
      "datasource": {
        "default": true,
        "type": "grafana-postgresql-datasource"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "watt"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 11
      },
      "hideTimeOverride": true,
      "id": 3,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    heat_demand AS \"Thermal Energy\"\nFROM \n    process_heat\nWHERE \n    $__timeFilter(\"timestamp\")\n",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        },
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "hide": false,
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    power_demand AS \"Electricity\"\nFROM \n    process_electricity \nWHERE \n    $__timeFilter(\"timestamp\")\n",
          "refId": "B",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "timeShift": "24h",
      "title": "Energy demand PM1",
      "type": "timeseries"
    },
    {
      "datasource": {
        "default": true,
        "type": "grafana-postgresql-datasource"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "watth"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 12,
        "y": 11
      },
      "id": 4,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "SELECT \n    SUM(power_demand * (5.0 / 3600.0)) AS \"Total Energy Consumed (Wh)\"\nFROM \n    process_electricity \n",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Consumed Electricity",
      "type": "stat"
    },
    {
      "datasource": {
        "default": true,
        "type": "grafana-postgresql-datasource"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "watth"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 6,
        "x": 18,
        "y": 11
      },
      "id": 5,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "SELECT \n    SUM(heat_demand * (5.0 / 3600.0)) AS \"Total Energy Consumed (Wh)\"\nFROM \n    process_heat \n",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Consumed Thermal Energy",
      "type": "stat"
    }
  ],
  "refresh": "5s",
  "schemaVersion": 39,
  "tags": [],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-6h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "smard",
  "uid": "d7e44e51-6f7f-4316-b9fb-1bb32c03fa18",
  "version": 1,
  "weekStart": ""
}
```

## grafana/dashboard_heating_optimization.json

```json
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 2,
  "id": null,
  "links": [],
  "panels": [
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "kwatth"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 3,
        "x": 0,
        "y": 0
      },
      "id": 4,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select total_energy_demand\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Hood Energy demand",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 5,
        "x": 3,
        "y": 0
      },
      "id": 8,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select full_load_hours\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Full Load Hours without Electric Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "none"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 5,
        "x": 8,
        "y": 0
      },
      "id": 9,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select full_load_hours_after_optimization\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Full Load Hours with Electric Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "kwatth"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 5,
        "x": 13,
        "y": 0
      },
      "id": 6,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select gas_usage\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Gas Used for Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "kwatth"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 6,
        "x": 18,
        "y": 0
      },
      "id": 5,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select electricity_used\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Electricity Used for Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "masst"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 8,
        "x": 0,
        "y": 3
      },
      "id": 10,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select emissions_savings\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Emissions Savings by Using Hybrid Electric Heating vs. just Gas Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "currencyEUR"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 8,
        "x": 8,
        "y": 3
      },
      "id": 7,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select cost_savings\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Cost Savings by Using Hybrid Electric Heating vs. just Gas Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "percentunit"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 4,
        "w": 8,
        "x": 16,
        "y": 3
      },
      "id": 15,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select electric_heating_in_low_price_windows_ratio\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Ratio Electric Heating in Low Price Windows",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyEUR"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 6,
        "x": 0,
        "y": 7
      },
      "id": 14,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select mean_electricity_price_when_heating\nFROM optimization_result WHERE name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Mean Electricity Price When Heating",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyEUR"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 6,
        "x": 6,
        "y": 7
      },
      "id": 11,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select avg(price)\nFROM prices WHERE source = 'gas_spot'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Mean Gas Price",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "currencyEUR"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 3,
        "w": 5,
        "x": 12,
        "y": 7
      },
      "id": 12,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "Select avg(price)\nFROM prices WHERE source = 'smard'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Mean Electricity Price",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "kwatt"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 10
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    (electricity_used) AS \"Electric Heaters\"\nFROM \n    flexible_power\nWHERE \n    $__timeFilter(\"timestamp\")\n    AND optimization_case_name = '$Scenario'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Usage of Electric Heaters",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "kwatt"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 18
      },
      "id": 13,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.2.0",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    (value*4) AS \"Heat Demand\"\nFROM \n    simulation_input_data \nWHERE \n    $__timeFilter(\"timestamp\")\n    AND name = 'flexible_device_demand'",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Heat Demand of the Hood",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "grafana-postgresql-datasource"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "massg"
        },
        "overrides": [
          {
            "matcher": {
              "id": "byName",
              "options": "Co2 intensity"
            },
            "properties": [
              {
                "id": "unit",
                "value": "g/kWh"
              }
            ]
          },
          {
            "matcher": {
              "id": "byName",
              "options": "Price"
            },
            "properties": [
              {
                "id": "unit",
                "value": "currencyEUR"
              }
            ]
          },
          {
            "matcher": {
              "id": "byName",
              "options": "Low Price Window"
            },
            "properties": [
              {
                "id": "custom.drawStyle",
                "value": "line"
              },
              {
                "id": "unit",
                "value": "bool_on_off"
              },
              {
                "id": "custom.fillOpacity",
                "value": 25
              }
            ]
          }
        ]
      },
      "gridPos": {
        "h": 9,
        "w": 24,
        "x": 0,
        "y": 26
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.3.1",
      "targets": [
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "hide": false,
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    co2 AS \"Grid Co2 intensity\"\nFROM \n    footprint \nWHERE \n    $__timeFilter(\"timestamp\")\n",
          "refId": "B",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        },
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "hide": false,
          "rawQuery": true,
          "rawSql": "SELECT \n    $__timeGroupAlias(timestamp, $__interval),\n    (price) AS \"Price\"\nFROM \n    prices \nWHERE \n    $__timeFilter(\"timestamp\") AND source = 'smard'\n",
          "refId": "D",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        },
        {
          "datasource": {
            "type": "grafana-postgresql-datasource"
          },
          "editorMode": "code",
          "format": "table",
          "hide": false,
          "rawQuery": true,
          "rawSql": "SELECT\n  date_trunc('minute', timestamp) AS time,\n  COALESCE(\n    MAX(\n      CASE WHEN low_price_window = 1 THEN 1 ELSE 0 END\n    ), 0\n  ) AS \"Low Price Window\"\nFROM\n  flexible_power\nWHERE\n  optimization_case_name = '$Scenario'\nGROUP BY\n  time\nORDER BY\n  time\n",
          "refId": "A",
          "sql": {
            "columns": [
              {
                "parameters": [],
                "type": "function"
              }
            ],
            "groupBy": [
              {
                "property": {
                  "type": "string"
                },
                "type": "groupBy"
              }
            ],
            "limit": 50
          }
        }
      ],
      "title": "Co2 vs Price",
      "type": "timeseries"
    }
  ],
  "refresh": "",
  "schemaVersion": 39,
  "tags": [],
  "templating": {
    "list": [
      {
        "current": {},
        "datasource": {
          "type": "grafana-postgresql-datasource"
        },
        "definition": "SELECT name FROM public.optimization_result\n",
        "hide": 0,
        "includeAll": false,
        "multi": false,
        "name": "Scenario",
        "options": [],
        "query": "SELECT name FROM public.optimization_result\n",
        "refresh": 1,
        "regex": "",
        "skipUrlSync": false,
        "sort": 0,
        "type": "query"
      }
    ]
  },
  "time": {
    "from": "2023-12-31T23:00:00.000Z",
    "to": "2024-12-31T22:59:59.000Z"
  },
  "timepicker": {},
  "timezone": "browser",
  "title": "Heating Optimization",
  "uid": "eejv93w05ihvkb",
  "version": 20,
  "weekStart": ""
}
```

## compose.yml

```yaml
services:
  postgres:
    image: postgres:latest
    container_name: postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres-storage:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin4
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
      PGADMIN_LISTEN_PORT: 80
      PGADMIN_CONFIG_UPGRADE_CHECK_ENABLED: "False"
      PGADMIN_SERVER_JSON_FILE: /pgadmin4/servers.json
    volumes:
      - pgadmin-storage:/var/lib/pgadmin
    ports:
      - "8080:80"
    depends_on:
      - postgres

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: always
    environment:
      GF_SERVER_ROOT_URL: ${GF_SERVER_ROOT_URL}
      GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD}
      GF_SERVER_HTTP_PORT: 3000
      GF_SECURITY_ADMIN_USER: ${GF_SECURITY_ADMIN_USER}
      GF_SECURITY_ALLOW_EMBEDDING: true
      GF_AUTH_ANONYMOUS_ENABLED: true
      GF_USERS_DEFAULT_THEME: light
      GF_RENDERING_SERVER_URL: ${GF_RENDERING_SERVER_URL}
      GF_RENDERING_CALLBACK_URL: ${GF_RENDERING_CALLBACK_URL}
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
      - ./grafana/dashboard-provisioning.yml:/etc/grafana/provisioning/dashboards/dashboard-provisioning.yml
      - ./grafana/dashboard_smard.json:/var/lib/grafana/dashboards/dashboard_smard.json
      - ./grafana/dashboard_heating_optimization.json:/var/lib/grafana/dashboards/dashboard_heating_optimization.json
    ports:
      - "3000:3000"
    depends_on:
      - postgres
  renderer:
    image: grafana/grafana-image-renderer:latest
    container_name: renderer
    restart: always
    ports:
      - "8081:8081"
  forest-ensys:
    image: forest-ensys:latest
    build: .
    container_name: forest-ensys
    restart: always
    environment:
      - POSTGRES_SERVER=${POSTGRES_SERVER}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - SQLALCHEMY_DATABASE_URI=${SQLALCHEMY_DATABASE_URI}
      - GRB_LICENSE_FILE=/opt/gurobi/gurobi.lic
    ports:
      - "9012:9000"
    volumes:
      - ./gurobi.lic:/opt/gurobi/gurobi.lic:ro
volumes:
  postgres-storage:
  pgadmin-storage:
  grafana-storage:
```

## Statistics

- Total Files: 87
- Total Characters: 409310
- Total Tokens: 0
